{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) Networks\n",
                "\n",
                "## Introduction\n",
                "\n",
                "Recurrent Neural Networks (RNNs) are a class of neural networks designed to process sequential data by leveraging temporal dependencies. They have found widespread applications in language modeling, speech recognition, time series prediction, and more. However, traditional RNNs suffer from the vanishing gradient problem, which hampers their ability to learn long-term dependencies.\n",
                "\n",
                "Long Short-Term Memory (LSTM) networks, introduced by Hochreiter and Schmidhuber in 1997 [[1]](#ref1), address this limitation by incorporating gating mechanisms that regulate the flow of information. In this tutorial, we'll explore the architecture of RNNs and LSTMs, delve into the mathematics behind them, and implement LSTMs for language modeling tasks.\n",
                "\n",
                "![RNN vs. LSTM](https://miro.medium.com/max/1400/1*6tOKpaF16sAViP2BWe3Y_Q.png)\n",
                "\n",
                "*Image Source: [Medium](https://medium.com/)*\n",
                "\n",
                "## Table of Contents\n",
                "\n",
                "1. [Understanding Recurrent Neural Networks](#1)\n",
                "   - [Mathematical Formulation](#1.1)\n",
                "   - [Limitations of Standard RNNs](#1.2)\n",
                "2. [The Vanishing Gradient Problem](#2)\n",
                "3. [Long Short-Term Memory Networks](#3)\n",
                "   - [LSTM Architecture](#3.1)\n",
                "   - [Mathematical Equations](#3.2)\n",
                "4. [Implementing LSTMs for Language Modeling](#4)\n",
                "   - [Dataset Preparation](#4.1)\n",
                "   - [Building the LSTM Model](#4.2)\n",
                "   - [Training the Model](#4.3)\n",
                "   - [Generating Text](#4.4)\n",
                "5. [Advanced Developments in RNNs and LSTMs](#5)\n",
                "   - [Gated Recurrent Units (GRUs)](#5.1)\n",
                "   - [Bidirectional RNNs](#5.2)\n",
                "   - [Attention Mechanisms](#5.3)\n",
                "   - [Transformer Models](#5.4)\n",
                "6. [Conclusion](#6)\n",
                "7. [References](#7)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id=\"1\"></a>\n",
                "## 1. Understanding Recurrent Neural Networks\n",
                "\n",
                "Traditional neural networks assume all inputs (and outputs) are independent of each other. However, in many tasks, such as predicting the next word in a sentence, previous inputs are crucial. RNNs address this by maintaining a hidden state that captures information about previous inputs.\n",
                "\n",
                "<a id=\"1.1\"></a>\n",
                "### Mathematical Formulation\n",
                "\n",
                "At each time step $( t )$, the RNN updates its hidden state $( h_t )$ and outputs $( y_t )$:\n",
                "\n",
                "$[\n",
                "\\begin{align*}\n",
                "h_t &= \\sigma_h(W_h x_t + U_h h_{t-1} + b_h) \\\\\n",
                "y_t &= \\sigma_y(W_y h_t + b_y)\n",
                "\\end{align*}\n",
                "]$\n",
                "\n",
                "- $( x_t )$: Input at time $( t )$\n",
                "- $( h_{t-1} )$: Hidden state from the previous time step\n",
                "- $( W_h, U_h )$: Weight matrices\n",
                "- $( b_h, b_y )$: Bias vectors\n",
                "- $( \\sigma_h, \\sigma_y )$: Activation functions (e.g., tanh, softmax)\n",
                "\n",
                "<a id=\"1.2\"></a>\n",
                "### Limitations of Standard RNNs\n",
                "\n",
                "- **Vanishing/Exploding Gradients**: Gradients can diminish or explode during backpropagation through time (BPTT), making it difficult to learn long-term dependencies.\n",
                "- **Short-Term Memory**: Standard RNNs struggle with remembering information from far in the past.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id=\"2\"></a>\n",
                "## 2. The Vanishing Gradient Problem\n",
                "\n",
                "The vanishing gradient problem arises when gradients shrink exponentially as they are propagated backward through time. This is particularly problematic with deep networks or long sequences.\n",
                "\n",
                "**Mathematical Insight:**\n",
                "\n",
                "During BPTT, gradients are calculated as:\n",
                "\n",
                "$[\n",
                "\\frac{\\partial L}{\\partial W} = \\sum_{t} \\frac{\\partial L}{\\partial h_t} \\frac{\\partial h_t}{\\partial W}\n",
                "]$\n",
                "\n",
                "Due to the recursive nature:\n",
                "\n",
                "$[\n",
                "\\frac{\\partial h_t}{\\partial h_{t-1}} = U_h^\\top \\sigma_h'(W_h x_t + U_h h_{t-1} + b_h)\n",
                "]$\n",
                "\n",
                "Multiplying many small gradients (from $( \\sigma_h' )$) leads to vanishing gradients.\n",
                "\n",
                "**Solutions:**\n",
                "\n",
                "- **Gradient Clipping**: Limits the gradients to prevent them from exploding.\n",
                "- **Advanced Architectures**: LSTMs and GRUs are designed to mitigate this problem.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id=\"3\"></a>\n",
                "## 3. Long Short-Term Memory Networks\n",
                "\n",
                "LSTMs introduce memory cells and gating mechanisms to control the flow of information.\n",
                "\n",
                "<a id=\"3.1\"></a>\n",
                "### LSTM Architecture\n",
                "\n",
                "An LSTM cell contains:\n",
                "\n",
                "- **Cell State ($( C_t )$)**: Carries information across time steps.\n",
                "- **Gates**: Regulate the information flow.\n",
                "  - **Forget Gate ($( f_t )$)**\n",
                "  - **Input Gate ($( i_t )$)**\n",
                "  - **Output Gate ($( o_t )$)**\n",
                "\n",
                "![LSTM Cell](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
                "\n",
                "*Image Source: [Colah's Blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)*\n",
                "\n",
                "<a id=\"3.2\"></a>\n",
                "### Mathematical Equations\n",
                "\n",
                "$[\n",
                "\\begin{align*}\n",
                "f_t &= \\sigma(W_f x_t + U_f h_{t-1} + b_f) \\\\\n",
                "i_t &= \\sigma(W_i x_t + U_i h_{t-1} + b_i) \\\\\n",
                "\\tilde{C}_t &= \\tanh(W_C x_t + U_C h_{t-1} + b_C) \\\\\n",
                "C_t &= f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t \\\\\n",
                "o_t &= \\sigma(W_o x_t + U_o h_{t-1} + b_o) \\\\\n",
                "h_t &= o_t \\odot \\tanh(C_t)\n",
                "\\end{align*}\n",
                "]$\n",
                "\n",
                "- $( \\sigma )$: Sigmoid function\n",
                "- $( \\odot )$: Element-wise multiplication\n",
                "- $( \\tilde{C}_t )$: Candidate cell state\n",
                "\n",
                "**Explanation:**\n",
                "\n",
                "- **Forget Gate ($( f_t )$)**: Decides what information to discard from the cell state.\n",
                "- **Input Gate ($( i_t )$)**: Decides which new information to add.\n",
                "- **Cell State Update ($( C_t )$)**: Combines the old cell state and new candidate values.\n",
                "- **Output Gate ($( o_t )$)**: Decides what to output based on the cell state.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id=\"4\"></a>\n",
                "## 4. Implementing LSTMs for Language Modeling\n",
                "\n",
                "We'll build an LSTM language model using TensorFlow and Keras to predict the next word in a sequence.\n",
                "\n",
                "<a id=\"4.1\"></a>\n",
                "### Dataset Preparation\n",
                "\n",
                "We'll use a subset of the Shakespeare dataset.\n",
                "\n",
                "```python\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.preprocessing.text import Tokenizer\n",
                "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
                "import numpy as np\n",
                "\n",
                "# Sample text data\n",
                "text = \"\"\"To be, or not to be, that is the question:\n",
                "Whether 'tis nobler in the mind to suffer\n",
                "The slings and arrows of outrageous fortune,\n",
                "Or to take arms against a sea of troubles...\"\"\"\n",
                "\n",
                "# Tokenize the text\n",
                "tokenizer = Tokenizer()\n",
                "tokenizer.fit_on_texts([text])\n",
                "total_words = len(tokenizer.word_index) + 1\n",
                "\n",
                "# Create sequences of words\n",
                "input_sequences = []\n",
                "for line in text.split('\\n'):\n",
                "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
                "    for i in range(1, len(token_list)):\n",
                "        n_gram_sequence = token_list[:i+1]\n",
                "        input_sequences.append(n_gram_sequence)\n",
                "\n",
                "# Pad sequences\n",
                "max_seq_len = max([len(seq) for seq in input_sequences])\n",
                "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre'))\n",
                "\n",
                "# Create predictors and label\n",
                "X = input_sequences[:, :-1]\n",
                "y = input_sequences[:, -1]\n",
                "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
                "```\n",
                "\n",
                "<a id=\"4.2\"></a>\n",
                "### Building the LSTM Model\n",
                "\n",
                "```python\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
                "\n",
                "model = Sequential()\n",
                "model.add(Embedding(input_dim=total_words, output_dim=64, input_length=max_seq_len - 1))\n",
                "model.add(LSTM(150))\n",
                "model.add(Dense(total_words, activation='softmax'))\n",
                "\n",
                "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
                "model.summary()\n",
                "```\n",
                "\n",
                "<a id=\"4.3\"></a>\n",
                "### Training the Model\n",
                "\n",
                "```python\n",
                "history = model.fit(X, y, epochs=200, verbose=1)\n",
                "```\n",
                "\n",
                "<a id=\"4.4\"></a>\n",
                "### Generating Text\n",
                "\n",
                "```python\n",
                "seed_text = \"To be or not\"\n",
                "next_words = 10\n",
                "\n",
                "for _ in range(next_words):\n",
                "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
                "    token_list = pad_sequences([token_list], maxlen=max_seq_len - 1, padding='pre')\n",
                "    predicted = model.predict(token_list, verbose=0)\n",
                "    predicted_word_index = np.argmax(predicted, axis=1)[0]\n",
                "    output_word = tokenizer.index_word[predicted_word_index]\n",
                "    seed_text += \" \" + output_word\n",
                "\n",
                "print(seed_text)\n",
                "```\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id=\"5\"></a>\n",
                "## 5. Advanced Developments in RNNs and LSTMs\n",
                "\n",
                "<a id=\"5.1\"></a>\n",
                "### 5.1 Gated Recurrent Units (GRUs)\n",
                "\n",
                "**GRUs**, introduced by Cho et al. in 2014 [[2]](#ref2), simplify the LSTM architecture by combining the forget and input gates into a single update gate.\n",
                "\n",
                "**Equations:**\n",
                "\n",
                "$$[\n",
                "\\begin{align*}\n",
                "z_t &= \\sigma(W_z x_t + U_z h_{t-1} + b_z) \\\\\n",
                "r_t &= \\sigma(W_r x_t + U_r h_{t-1} + b_r) \\\\\n",
                "\\tilde{h}_t &= \\tanh(W_h x_t + U_h (r_t \\odot h_{t-1}) + b_h) \\\\\n",
                "h_t &= (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t\n",
                "\\end{align*}\n",
                "]$$\n",
                "\n",
                "- $( z_t )$: Update gate\n",
                "- $( r_t )$: Reset gate\n",
                "\n",
                "**Advantages:**\n",
                "\n",
                "- Fewer parameters than LSTM\n",
                "- Comparable performance\n",
                "\n",
                "<a id=\"5.2\"></a>\n",
                "### 5.2 Bidirectional RNNs\n",
                "\n",
                "**Bidirectional RNNs** process the sequence in both forward and backward directions, capturing past and future contexts.\n",
                "\n",
                "**Implementation:**\n",
                "\n",
                "```python\n",
                "from tensorflow.keras.layers import Bidirectional\n",
                "\n",
                "model = Sequential()\n",
                "model.add(Embedding(input_dim=total_words, output_dim=64, input_length=max_seq_len - 1))\n",
                "model.add(Bidirectional(LSTM(150)))\n",
                "model.add(Dense(total_words, activation='softmax'))\n",
                "```\n",
                "\n",
                "<a id=\"5.3\"></a>\n",
                "### 5.3 Attention Mechanisms\n",
                "\n",
                "**Attention**, introduced by Bahdanau et al. in 2015 [[3]](#ref3), allows the model to focus on specific parts of the input sequence when generating each part of the output.\n",
                "\n",
                "**Key Concepts:**\n",
                "\n",
                "- **Alignment Scores**: Determine the importance of each hidden state.\n",
                "- **Context Vector**: Weighted sum of hidden states.\n",
                "\n",
                "**Applications:**\n",
                "\n",
                "- Machine Translation\n",
                "- Text Summarization\n",
                "\n",
                "<a id=\"5.4\"></a>\n",
                "### 5.4 Transformer Models\n",
                "\n",
                "**Transformers**, introduced by Vaswani et al. in 2017 [[4]](#ref4), rely entirely on attention mechanisms, dispensing with recurrence entirely.\n",
                "\n",
                "**Advantages:**\n",
                "\n",
                "- Parallelizable\n",
                "- Handles long-range dependencies efficiently\n",
                "\n",
                "**Impact:**\n",
                "\n",
                "- Enabled models like BERT and GPT series\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id=\"6\"></a>\n",
                "## 6. Conclusion\n",
                "\n",
                "RNNs and LSTMs have significantly advanced the ability of neural networks to handle sequential data. While RNNs capture temporal dependencies, LSTMs mitigate the vanishing gradient problem, enabling learning over longer sequences. Further innovations like GRUs, attention mechanisms, and transformers continue to push the boundaries of what's possible in sequence modeling.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id=\"7\"></a>\n",
                "## 7. References\n",
                "\n",
                "1. <a id=\"ref1\"></a>Hochreiter, S., & Schmidhuber, J. (1997). *Long Short-Term Memory*. Neural Computation, 9(8), 1735–1780.\n",
                "2. <a id=\"ref2\"></a>Cho, K., et al. (2014). *Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation*. [arXiv:1406.1078](https://arxiv.org/abs/1406.1078)\n",
                "3. <a id=\"ref3\"></a>Bahdanau, D., Cho, K., & Bengio, Y. (2015). *Neural Machine Translation by Jointly Learning to Align and Translate*. [arXiv:1409.0473](https://arxiv.org/abs/1409.0473)\n",
                "4. <a id=\"ref4\"></a>Vaswani, A., et al. (2017). *Attention Is All You Need*. [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)\n",
                "\n",
                "---\n",
                "\n",
                "This notebook provides an in-depth exploration of RNNs and LSTMs. You can run the code cells to see how LSTMs are implemented and experiment with the models.\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "mimetype": "text/x-python",
            "name": "python",
            "version": "3.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
