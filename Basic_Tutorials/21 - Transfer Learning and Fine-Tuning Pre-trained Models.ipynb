{
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.x",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Transfer Learning and Fine-Tuning Pre-trained Models\n\n## Introduction\n\nTransfer learning is a powerful technique in deep learning where a model trained on a large dataset is repurposed for a different but related task. This approach leverages the learned features of pre-trained models, saving time and computational resources while often achieving better performance. Fine-tuning involves making slight adjustments to the pre-trained model's weights to adapt it to the new task.\n\nIn this tutorial, we will explore how to leverage pre-trained models like VGG, ResNet, and BERT for new tasks. We'll delve into the underlying mathematics, provide code examples, and explain the processes involved. We'll also reference key papers and discuss some of the latest developments in this field."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Table of Contents\n\n1. [Understanding Transfer Learning](#1)\n   - [Why Transfer Learning?](#1.1)\n   - [How Does It Work?](#1.2)\n2. [Pre-trained Models in Computer Vision](#2)\n   - [VGG Networks](#2.1)\n   - [ResNet (Residual Networks)](#2.2)\n3. [Fine-Tuning Pre-trained Models](#3)\n   - [Feature Extraction vs. Fine-Tuning](#3.1)\n   - [Implementation with Keras](#3.2)\n4. [Transfer Learning in Natural Language Processing](#4)\n   - [BERT (Bidirectional Encoder Representations from Transformers)](#4.1)\n   - [Fine-Tuning BERT for Text Classification](#4.2)\n5. [Latest Developments in Transfer Learning](#5)\n   - [Vision Transformers (ViT)](#5.1)\n   - [GPT Models](#5.2)\n6. [Conclusion](#6)\n7. [References](#7)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"1\"></a>\n## 1. Understanding Transfer Learning\n\n<a id=\"1.1\"></a>\n### Why Transfer Learning?\n\nTraining deep neural networks from scratch requires large amounts of data and computational resources. Transfer learning addresses this by:\n\n- **Reducing Training Time**: By starting with a pre-trained model, we can significantly reduce the time needed to train a model for a new task.\n- **Improving Performance**: Pre-trained models have learned rich feature representations that can improve performance on related tasks.\n- **Handling Limited Data**: Transfer learning is particularly useful when the new task has limited labeled data.\n\n<a id=\"1.2\"></a>\n### How Does It Work?\n\nThe idea is to leverage the knowledge learned in one domain (source task) and apply it to another domain (target task). This is typically done by:\n\n1. **Using Pre-trained Models**: Models trained on large datasets like ImageNet for vision tasks or large corpora for NLP tasks.\n2. **Feature Extraction**: Utilizing the pre-trained model's layers as feature extractors.\n3. **Fine-Tuning**: Adjusting the weights of the pre-trained model during training on the new task."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"2\"></a>\n## 2. Pre-trained Models in Computer Vision\n\nSeveral architectures have been pre-trained on large datasets and are available for transfer learning. Two popular ones are VGG and ResNet."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"2.1\"></a>\n### VGG Networks\n\nThe VGG network, introduced by Simonyan and Zisserman in 2014 [[1]](#ref1), is known for its simplicity and depth.\n\n- **Architecture**: Consists of 16 or 19 convolutional layers.\n- **Key Features**:\n  - Uses small convolutional filters (3x3).\n  - Depth increases by stacking convolutional layers.\n- **Applications**: Good for feature extraction due to its uniform architecture.\n\n**Mathematical Representation**:\n\nEach convolutional layer applies a filter $( W )$ to the input $( x )$:\n\n$[\n    y = sigma(W * x + b)\n]$\n\n- $( sigma )$: Activation function (e.g., ReLU).\n- $( * )$: Convolution operation."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"2.2\"></a>\n### ResNet (Residual Networks)\n\nResNet, introduced by He et al. in 2015 [[2]](#ref2), addresses the vanishing gradient problem by using residual connections.\n\n- **Architecture**: Includes residual blocks where the input is added to the output of convolutional layers.\n- **Key Features**:\n  - Enables training of very deep networks (up to 152 layers).\n  - Residual connections facilitate gradient flow.\n\n**Mathematical Representation**:\n\nA residual block computes:\n\n$[\n    y = F(x, W_i) + x\n]$\n\n- $( F(x, W_i) )$: Function representing convolutional layers.\n- $( x )$: Input to the residual block.\n- The addition of $( x )$ allows gradients to flow directly through the network."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"3\"></a>\n## 3. Fine-Tuning Pre-trained Models\n\n<a id=\"3.1\"></a>\n### Feature Extraction vs. Fine-Tuning\n\n- **Feature Extraction**:\n  - Freeze the convolutional base of the pre-trained model.\n  - Use the outputs of the convolutional base as features for the new task.\n- **Fine-Tuning**:\n  - Unfreeze some top layers of the pre-trained model.\n  - Retrain these layers along with the added classifier.\n\n<a id=\"3.2\"></a>\n### Implementation with Keras\n\nWe'll demonstrate how to use a pre-trained ResNet50 model for a new image classification task."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Import necessary libraries\nimport tensorflow as tf\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import SGD\n\n# Load the ResNet50 model with pre-trained weights, excluding the top classifier layers\nbase_model = ResNet50(weights='imagenet', include_top=False)\n\n# Freeze the base model layers\nbase_model.trainable = False\n\n# Add custom top layers\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\n# Let's say we have 10 classes\npredictions = Dense(10, activation='softmax')(x)\n\n# Create the full model\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Summary of the model\nmodel.summary()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Explanation**:\n\n- **Weights**: We use `weights='imagenet'` to load pre-trained weights.\n- **include_top=False**: Excludes the fully connected layers at the top.\n- **GlobalAveragePooling2D**: Reduces the spatial dimensions of the feature maps.\n- **Dense Layer**: Adds a new classifier for our specific task."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Fine-Tuning the Model**\n\nAfter training the top layers, we can unfreeze some of the layers in the base model for fine-tuning."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Unfreeze some layers for fine-tuning\nfor layer in base_model.layers[-30:]:  # Unfreeze the last 30 layers\n    layer.trainable = True\n\n# Recompile the model with a low learning rate\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Continue training\n# model.fit(...)  # Continue training with your data",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Explanation**:\n\n- **Unfreeze Layers**: We unfreeze the last 30 layers for fine-tuning.\n- **Low Learning Rate**: Using a low learning rate to prevent large updates to the pre-trained weights."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"4\"></a>\n## 4. Transfer Learning in Natural Language Processing\n\nTransfer learning has also made significant strides in NLP, especially with models like BERT."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"4.1\"></a>\n### BERT (Bidirectional Encoder Representations from Transformers)\n\nIntroduced by Devlin et al. in 2018 [[3]](#ref3), BERT is a transformer-based model pre-trained on a large corpus of unlabeled text.\n\n- **Key Features**:\n  - **Bidirectional**: Considers context from both left and right.\n  - **Transformer Architecture**: Uses self-attention mechanisms.\n- **Pre-training Tasks**:\n  - **Masked Language Modeling (MLM)**: Predicting masked words.\n  - **Next Sentence Prediction (NSP)**: Predicting sentence relationships.\n\n**Mathematical Representation**:\n\n- **Self-Attention Mechanism**:\n\n  Given queries $( Q )$, keys $( K )$, and values $( V )$:\n\n  $[\n  \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n  ]$\n\n  - $( d_k )$: Dimension of the key vectors.\n\n- **Transformer Encoder Layer**:\n\n  $[\n  \\begin{align*}\n  & \\text{Input Embedding} + \\text{Position Embedding} \\rightarrow X \\\\\n  & \\text{Self-Attention}(X) \\rightarrow \\text{Add & Norm} \\rightarrow \\text{Feed Forward} \\rightarrow \\text{Add & Norm}\n  \\end{align*}\n  ]$"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"4.2\"></a>\n### Fine-Tuning BERT for Text Classification\n\nWe'll demonstrate how to fine-tune BERT for a text classification task using the `transformers` library."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Install the transformers library (if not already installed)\n# !pip install transformers\n\nfrom transformers import BertTokenizer, TFBertForSequenceClassification\nimport tensorflow as tf\n\n# Load the tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n\n# Prepare the data (example with dummy data)\ntexts = [\"I love this product!\", \"This is the worst experience.\"]\nlabels = [1, 0]  # 1: positive, 0: negative\n\n# Tokenize the inputs\nencodings = tokenizer(texts, truncation=True, padding=True, max_length=128)\n\n# Convert to TensorFlow dataset\ndataset = tf.data.Dataset.from_tensor_slices((dict(encodings), labels))\ndataset = dataset.batch(2)\n\n# Compile the model\noptimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\nmodel.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])\n\n# Train the model\n# model.fit(dataset, epochs=2)\n\n# Note: For actual training, you need a larger dataset",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Explanation**:\n\n- **BertTokenizer**: Tokenizes the input text according to BERT's requirements.\n- **TFBertForSequenceClassification**: A BERT model tailored for sequence classification tasks.\n- **compute_loss**: Uses the built-in loss computation suitable for the task."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"5\"></a>\n## 5. Latest Developments in Transfer Learning\n\nTransfer learning continues to evolve with new architectures and models being introduced."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"5.1\"></a>\n### Vision Transformers (ViT)\n\n**Vision Transformers (ViT)** [[4]](#ref4) apply transformer architecture to image recognition tasks.\n\n- **Key Features**:\n  - Treats images as sequences of patches.\n  - Applies standard transformer encoder layers.\n- **Advantages**:\n  - Achieves state-of-the-art results with sufficient data.\n\n**Mathematical Representation**:\n\n- **Patch Embeddings**:\n\n  Images are split into patches and flattened:\n\n  $[\n  x_p = \\text{Flatten}(\\text{Patch}(x))\n  ]$\n\n- **Position Embeddings**: Added to patch embeddings to retain positional information."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"5.2\"></a>\n### GPT Models\n\n**Generative Pre-trained Transformer (GPT)** models [[5]](#ref5) have advanced NLP tasks significantly.\n\n- **Key Features**:\n  - Uses transformer decoder architecture.\n  - Trained on large amounts of text data.\n- **Applications**:\n  - Language generation, translation, summarization.\n- **Latest Models**:\n  - GPT-3, GPT-4 with billions of parameters, showing impressive language understanding and generation capabilities."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"6\"></a>\n## 6. Conclusion\n\nTransfer learning and fine-tuning pre-trained models have become indispensable techniques in deep learning. By leveraging models like VGG, ResNet, and BERT, we can achieve high performance on new tasks with less data and computational resources. As the field continues to evolve, staying updated with the latest models and techniques is crucial for leveraging transfer learning effectively."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"7\"></a>\n## 7. References\n\n1. <a id=\"ref1\"></a>Simonyan, K., & Zisserman, A. (2014). *Very Deep Convolutional Networks for Large-Scale Image Recognition*. [arXiv:1409.1556](https://arxiv.org/abs/1409.1556)\n2. <a id=\"ref2\"></a>He, K., Zhang, X., Ren, S., & Sun, J. (2015). *Deep Residual Learning for Image Recognition*. [arXiv:1512.03385](https://arxiv.org/abs/1512.03385)\n3. <a id=\"ref3\"></a>Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*. [arXiv:1810.04805](https://arxiv.org/abs/1810.04805)\n4. <a id=\"ref4\"></a>Dosovitskiy, A., et al. (2020). *An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale*. [arXiv:2010.11929](https://arxiv.org/abs/2010.11929)\n5. <a id=\"ref5\"></a>Brown, T. B., et al. (2020). *Language Models are Few-Shot Learners*. [arXiv:2005.14165](https://arxiv.org/abs/2005.14165)\n\n---\n\nThis notebook provides an in-depth exploration of transfer learning and fine-tuning pre-trained models. You can run the code cells to see how these models are implemented and experiment with different architectures and datasets."
  }
 ]
}
