{
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.x",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Attention Mechanisms and Transformer Models\n\n## Introduction\n\nAttention mechanisms have revolutionized the field of deep learning, particularly in natural language processing (NLP). They enable models to focus on specific parts of the input when generating each part of the output, effectively capturing long-range dependencies and improving performance on tasks like machine translation, text summarization, and question answering.\n\nTransformer models, introduced by Vaswani et al. in 2017 [[1]](#ref1), leverage attention mechanisms entirely, dispensing with recurrence and convolutions. This innovation has led to significant advancements in NLP and beyond.\n\nIn this tutorial, we'll delve into attention mechanisms, explore the Transformer architecture, implement a Transformer model using TensorFlow and Keras, and discuss the latest developments in this rapidly evolving field."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Table of Contents\n\n1. [Understanding Attention Mechanisms](#1)\n   - [Motivation](#1.1)\n   - [Mathematical Formulation](#1.2)\n2. [The Transformer Model](#2)\n   - [Architecture Overview](#2.1)\n   - [Multi-Head Attention](#2.2)\n   - [Positional Encoding](#2.3)\n3. [Implementing a Transformer Model](#3)\n   - [Dataset Preparation](#3.1)\n   - [Building the Transformer](#3.2)\n   - [Training the Model](#3.3)\n   - [Evaluating Performance](#3.4)\n4. [Latest Developments in Transformers](#4)\n   - [BERT (Bidirectional Encoder Representations from Transformers)](#4.1)\n   - [GPT Series](#4.2)\n   - [T5 (Text-to-Text Transfer Transformer)](#4.3)\n   - [Vision Transformers (ViT)](#4.4)\n5. [Conclusion](#5)\n6. [References](#6)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"1\"></a>\n## 1. Understanding Attention Mechanisms\n\n<a id=\"1.1\"></a>\n### Motivation\n\nIn traditional sequence-to-sequence (seq2seq) models, the encoder compresses the input sequence into a fixed-length context vector, which the decoder then uses to generate the output sequence. This approach can struggle with long input sequences because the fixed-length vector cannot capture all the necessary information.\n\nAttention mechanisms address this limitation by allowing the decoder to selectively focus on different parts of the input sequence at each decoding step."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"1.2\"></a>\n### Mathematical Formulation\n\nThe attention mechanism computes a weighted sum of the encoder's hidden states, where the weights (attention scores) reflect the relevance of each hidden state to the current decoding step.\n\nGiven:\n\n- Encoder hidden states: $( \\mathbf{H} = [\\mathbf{h}_1, \\mathbf{h}_2, \\dots, \\mathbf{h}_n] )$\n- Decoder hidden state at time $( t )$: $( \\mathbf{s}_t )$\n\nThe attention weights $( \\alpha_{t,i} )$ are computed using an alignment model $( a )$:\n\n$[\ne_{t,i} = a(\\mathbf{s}_{t-1}, \\mathbf{h}_i)\n]$\n\n$[\n\\alpha_{t,i} = \\frac{\\exp(e_{t,i})}{\\sum_{k=1}^{n} \\exp(e_{t,k})}\n]$\n\nThe context vector $( \\mathbf{c}_t )$ is the weighted sum:\n\n$[\n\\mathbf{c}_t = \\sum_{i=1}^{n} \\alpha_{t,i} \\mathbf{h}_i\n]$\n\nThe decoder then uses $( \\mathbf{c}_t )$ to generate the output at time $( t )$.\n\n**Alignment Model Options:**\n\n- **Dot Product:** $( e_{t,i} = \\mathbf{s}_{t-1}^\\top \\mathbf{h}_i )$\n- **Additive (Bahdanau Attention):** $( e_{t,i} = \\mathbf{v}^\\top \\tanh(\\mathbf{W}_1 \\mathbf{s}_{t-1} + \\mathbf{W}_2 \\mathbf{h}_i) )$\n\n**Reference:**\n\n- Bahdanau, D., Cho, K., & Bengio, Y. (2014). *Neural Machine Translation by Jointly Learning to Align and Translate*. [arXiv:1409.0473](https://arxiv.org/abs/1409.0473)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"2\"></a>\n## 2. The Transformer Model\n\n<a id=\"2.1\"></a>\n### Architecture Overview\n\nThe Transformer architecture removes recurrence entirely and relies solely on attention mechanisms to capture dependencies between input and output.\n\n**Key Components:**\n\n- **Encoder:** Processes the input sequence and generates representations.\n- **Decoder:** Generates the output sequence, using the encoder's representations and previous outputs.\n\n**Advantages:**\n\n- **Parallelization:** Without recurrence, computations can be parallelized over sequence positions.\n- **Long-Range Dependencies:** Attention allows direct connections between any two positions in the sequence."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"2.2\"></a>\n### Multi-Head Attention\n\nMulti-head attention allows the model to focus on different positions and aspects of the input.\n\n**Scaled Dot-Product Attention:**\n\nGiven queries $( \\mathbf{Q} )$, keys $( \\mathbf{K} )$, and values $( \\mathbf{V} )$:\n\n$[\n\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{d_k}}\\right)\\mathbf{V}\n]$\n\n- $( d_k )$: Dimension of the key vectors (used for scaling).\n\n**Multi-Head Attention:**\n\n$[\n\\text{MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) \\mathbf{W}^O\n]$\n\nWhere each head is:\n\n$[\n\\text{head}_i = \\text{Attention}(\\mathbf{Q}\\mathbf{W}_i^Q, \\mathbf{K}\\mathbf{W}_i^K, \\mathbf{V}\\mathbf{W}_i^V)\n]$\n\n- $( \\mathbf{W}_i^Q, \\mathbf{W}_i^K, \\mathbf{W}_i^V )$: Projection matrices for queries, keys, and values.\n- $( h )$: Number of heads."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"2.3\"></a>\n### Positional Encoding\n\nSince the Transformer has no recurrence or convolution, positional encoding is added to the input embeddings to provide the model with information about the position of each token in the sequence.\n\n**Sinusoidal Positional Encoding:**\n\n$[\n\\text{PE}_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n]$\n\n$[\n\\text{PE}_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n]$\n\n- $( pos )$: Position in the sequence.\n- $( i )$: Dimension index.\n- $( d_{\\text{model}} )$: Dimension of the model.\n\n**Reference:**\n\n- Vaswani, A., et al. (2017). *Attention Is All You Need*. [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "# Positional Encoding\nimport numpy as np\nimport tensorflow as tf\n\ndef get_angles(pos, i, d_model):\n    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n    return pos * angle_rates\n\ndef positional_encoding(position, d_model):\n    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n                            np.arange(d_model)[np.newaxis, :],\n                            d_model)\n\n    # Apply sin to even indices in the array; 2i\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n\n    # Apply cos to odd indices in the array; 2i+1\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n\n    pos_encoding = angle_rads[np.newaxis, ...]\n\n    return tf.cast(pos_encoding, dtype=tf.float32)\n\n# Example usage\npos_encoding = positional_encoding(50, 512)\nprint(pos_encoding.shape)  # (1, 50, 512)",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "# Scaled Dot-Product Attention\ndef scaled_dot_product_attention(q, k, v, mask):\n    \"\"\"Calculate the attention weights.\"\"\"\n    matmul_qk = tf.matmul(q, k, transpose_b=True)\n\n    # Scale matmul_qk\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n    # Add the mask to the scaled tensor\n    if mask is not None:\n        scaled_attention_logits += (mask * -1e9)\n\n    # Softmax on the last axis\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n\n    output = tf.matmul(attention_weights, v)\n\n    return output, attention_weights",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "# Multi-Head Attention\nfrom tensorflow.keras.layers import Layer\n\nclass MultiHeadAttention(Layer):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        assert d_model % num_heads == 0\n\n        self.num_heads = num_heads\n        self.d_model = d_model\n\n        self.depth = d_model // num_heads\n\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n\n        self.dense = tf.keras.layers.Dense(d_model)\n\n    def split_heads(self, x, batch_size):\n        \"\"\"Split the last dimension into (num_heads, depth).\"\"\"\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, v, k, q, mask):\n        batch_size = tf.shape(q)[0]\n\n        q = self.wq(q)\n        k = self.wk(k)\n        v = self.wv(v)\n\n        q = self.split_heads(q, batch_size)\n        k = self.split_heads(k, batch_size)\n        v = self.split_heads(v, batch_size)\n\n        # Scaled Dot-Product Attention\n        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n\n        # Concatenate heads\n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n\n        output = self.dense(concat_attention)\n\n        return output, attention_weights",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"3\"></a>\n## 3. Implementing a Transformer Model\n\nWe'll implement a Transformer model for machine translation using TensorFlow and Keras.\n\n<a id=\"3.1\"></a>\n### Dataset Preparation\n\nWe'll use a simple English-to-Portuguese translation dataset."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "import tensorflow_datasets as tfds\n\n# Load the dataset\nexamples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True, as_supervised=True)\ntrain_examples, val_examples = examples['train'], examples['validation']\n\n# Tokenization\nimport tensorflow_text  # Required to run tokenizer ops\n\nmodel_name = 'ted_hrlr_translate_pt_en_converter'\ntokenizers = tf.saved_model.load(model_name)\n\ndef tokenize_pairs(pt, en):\n    pt = tokenizers.pt.tokenize(pt)\n    pt = pt.to_tensor()\n\n    en = tokenizers.en.tokenize(en)\n    en = en.to_tensor()\n    return pt, en\n\nBUFFER_SIZE = 20000\nBATCH_SIZE = 64\n\ntrain_dataset = train_examples.map(tokenize_pairs)\ntrain_dataset = train_dataset.cache()\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\ntrain_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\nval_dataset = val_examples.map(tokenize_pairs)\nval_dataset = val_dataset.padded_batch(BATCH_SIZE)",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"3.2\"></a>\n### Building the Transformer\n\nNow we'll build the Transformer model by combining the components we've implemented."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "# Encoder Layer\nclass EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(EncoderLayer, self).__init__()\n\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = tf.keras.Sequential([\n            tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n            tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n        ])\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, training, mask):\n        attn_output, _ = self.mha(x, x, x, mask)  # Self-attention\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(x + attn_output)  # Residual connection\n\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layernorm2(out1 + ffn_output)  # Residual connection\n\n        return out2",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "# Decoder Layer\nclass DecoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(DecoderLayer, self).__init__()\n\n        self.mha1 = MultiHeadAttention(d_model, num_heads)\n        self.mha2 = MultiHeadAttention(d_model, num_heads)\n\n        self.ffn = tf.keras.Sequential([\n            tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n            tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n        ])\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n        self.dropout3 = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # Masked MHA\n        attn1 = self.dropout1(attn1, training=training)\n        out1 = self.layernorm1(attn1 + x)\n\n        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)  # MHA with encoder output\n        attn2 = self.dropout2(attn2, training=training)\n        out2 = self.layernorm2(attn2 + out1)\n\n        ffn_output = self.ffn(out2)\n        ffn_output = self.dropout3(ffn_output, training=training)\n        out3 = self.layernorm3(ffn_output + out2)\n\n        return out3, attn_weights_block1, attn_weights_block2",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "# Complete Transformer Model\nclass Transformer(tf.keras.Model):\n    def __init__(self, num_layers, d_model, num_heads, dff,\n                 input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n        super(Transformer, self).__init__()\n\n        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n                               input_vocab_size, pe_input, rate)\n\n        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n                               target_vocab_size, pe_target, rate)\n\n        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n\n    def call(self, inp, tar, training, enc_padding_mask,\n             look_ahead_mask, dec_padding_mask):\n        enc_output = self.encoder(inp, training, enc_padding_mask)\n\n        dec_output, attention_weights = self.decoder(\n            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n\n        final_output = self.final_layer(dec_output)\n\n        return final_output, attention_weights",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"3.3\"></a>\n### Training the Model\n\nWe'll compile and train the Transformer model."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "# Hyperparameters\nnum_layers = 4\nd_model = 128\ndff = 512\nnum_heads = 8\ninput_vocab_size = tokenizer_pt.vocab_size + 2\ntarget_vocab_size = tokenizer_en.vocab_size + 2\ndropout_rate = 0.1\n\n# Instantiate the Transformer\ntransformer = Transformer(num_layers, d_model, num_heads, dff,\n                          input_vocab_size, target_vocab_size,\n                          pe_input=input_vocab_size,\n                          pe_target=target_vocab_size,\n                          rate=dropout_rate)\n\n# Optimizer\nclass CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=4000):\n        super(CustomSchedule, self).__init__()\n\n        self.d_model = d_model\n        self.d_model = tf.cast(self.d_model, tf.float32)\n\n        self.warmup_steps = warmup_steps\n\n    def __call__(self, step):\n        arg1 = tf.math.rsqrt(step)\n        arg2 = step * (self.warmup_steps ** -1.5)\n\n        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n\nlearning_rate = CustomSchedule(d_model)\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n                                     epsilon=1e-9)\n\n# Loss and Metrics\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = tf.keras.losses.SparseCategoricalCrossentropy(\n        from_logits=True, reduction='none')(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"3.4\"></a>\n### Evaluating Performance\n\nAfter training, we can evaluate the model's performance on the validation set."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "# Evaluation Function\ndef evaluate(inp_sentence):\n    start_token = [tokenizer_pt.vocab_size]\n    end_token = [tokenizer_pt.vocab_size + 1]\n\n    # Tokenize input sentence\n    inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\n    encoder_input = tf.expand_dims(inp_sentence, 0)\n\n    decoder_input = [tokenizer_en.vocab_size]\n    output = tf.expand_dims(decoder_input, 0)\n\n    for i in range(40):\n        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n            encoder_input, output)\n\n        # Predictions\n        predictions, attention_weights = transformer(encoder_input, \n                                                     output,\n                                                     False,\n                                                     enc_padding_mask,\n                                                     combined_mask,\n                                                     dec_padding_mask)\n\n        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n\n        if predicted_id == tokenizer_en.vocab_size + 1:\n            return tf.squeeze(output, axis=0), attention_weights\n\n        output = tf.concat([output, predicted_id], axis=-1)\n\n    return tf.squeeze(output, axis=0), attention_weights\n\n# Function to translate\ndef translate(sentence):\n    result, attention_weights = evaluate(sentence)\n\n    predicted_sentence = tokenizer_en.decode([i for i in result \n                                              if i < tokenizer_en.vocab_size])\n\n    print('Input: {}'.format(sentence))\n    print('Predicted translation: {}'.format(predicted_sentence))\n\n# Example\ntranslate(\"este Ã© um problema que temos que resolver.\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"4\"></a>\n## 4. Latest Developments in Transformers\n\n<a id=\"4.1\"></a>\n### BERT (Bidirectional Encoder Representations from Transformers)\n\n- Introduced by Devlin et al. in 2018 [[2]](#ref2)\n- Uses the encoder part of the Transformer\n- Pre-trained on large text corpora with masked language modeling and next sentence prediction\n- Achieves state-of-the-art results on various NLP tasks"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"4.2\"></a>\n### GPT Series\n\n- Generative Pre-trained Transformer models by OpenAI\n- GPT-1 (2018), GPT-2 (2019), GPT-3 (2020), GPT-4 (2023)\n- Uses the decoder part of the Transformer\n- Pre-trained on large text corpora\n- Capable of generating coherent and contextually relevant text"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"4.3\"></a>\n### T5 (Text-to-Text Transfer Transformer)\n\n- Introduced by Raffel et al. in 2019 [[3]](#ref3)\n- Unified framework that converts all NLP tasks into a text-to-text format\n- Achieves strong performance across a variety of tasks"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"4.4\"></a>\n### Vision Transformers (ViT)\n\n- Introduced by Dosovitskiy et al. in 2020 [[4]](#ref4)\n- Applies Transformer architecture to image recognition tasks\n- Splits images into patches and treats them as tokens"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"5\"></a>\n## 5. Conclusion\n\nAttention mechanisms and Transformer models have fundamentally changed the landscape of deep learning, particularly in NLP. By effectively modeling long-range dependencies and enabling parallel computation, Transformers have achieved state-of-the-art results across numerous tasks. Ongoing research continues to push the boundaries, extending Transformers to new domains and improving their efficiency."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"6\"></a>\n## 6. References\n\n1. <a id=\"ref1\"></a>Vaswani, A., et al. (2017). *Attention Is All You Need*. [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)\n2. <a id=\"ref2\"></a>Devlin, J., et al. (2018). *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*. [arXiv:1810.04805](https://arxiv.org/abs/1810.04805)\n3. <a id=\"ref3\"></a>Raffel, C., et al. (2019). *Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer*. [arXiv:1910.10683](https://arxiv.org/abs/1910.10683)\n4. <a id=\"ref4\"></a>Dosovitskiy, A., et al. (2020). *An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale*. [arXiv:2010.11929](https://arxiv.org/abs/2010.11929)\n\n---\n\nThis notebook provides an in-depth exploration of attention mechanisms and Transformer models. You can run the code cells to see how Transformers are implemented and experiment with the models."
  }
 ]
}
