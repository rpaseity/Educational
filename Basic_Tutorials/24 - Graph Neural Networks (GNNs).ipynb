{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Networks (GNNs)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Graph Neural Networks (GNNs) are a class of neural networks that operate on graph-structured data. They have gained significant attention due to their ability to model relationships and interactions in data represented as graphs. Applications of GNNs span social network analysis, recommendation systems, biological networks, and more.\n",
    "\n",
    "In this tutorial, we'll explore how to apply neural networks to graph-structured data. We'll delve into the mathematical foundations of GNNs, implement GNNs for tasks like node classification and link prediction, and discuss the latest developments in the field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Understanding Graph Neural Networks](#1)\n",
    "   - [What are Graphs?](#1.1)\n",
    "   - [Neural Networks on Graphs](#1.2)\n",
    "2. [Mathematical Foundations](#2)\n",
    "   - [Graph Convolutional Networks (GCNs)](#2.1)\n",
    "   - [Message Passing Neural Networks](#2.2)\n",
    "3. [Implementing GNNs](#3)\n",
    "   - [Node Classification with GCNs](#3.1)\n",
    "   - [Link Prediction with Graph Autoencoders](#3.2)\n",
    "4. [Latest Developments in GNNs](#4)\n",
    "   - [Graph Attention Networks (GAT)](#4.1)\n",
    "   - [Graph Isomorphism Networks (GIN)](#4.2)\n",
    "5. [Conclusion](#5)\n",
    "6. [References](#6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# 1. Understanding Graph Neural Networks\n",
    "\n",
    "<a id=\"1.1\"></a>\n",
    "## 1.1 What are Graphs?\n",
    "\n",
    "A graph is a data structure consisting of nodes (or vertices) and edges connecting pairs of nodes. Graphs are used to represent relational data and can model complex interactions in data such as social networks, molecules, transportation networks, and more.\n",
    "\n",
    "**Formally**, a graph \\( G \\) is defined as \\( G = (V, E) \\), where:\n",
    "\n",
    "- \\( V \\) is the set of nodes.\n",
    "- \\( E \\) is the set of edges connecting nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.2\"></a>\n",
    "## 1.2 Neural Networks on Graphs\n",
    "\n",
    "Traditional neural networks (e.g., CNNs, RNNs) are designed for data with grid-like structures (e.g., images, sequences). GNNs extend neural networks to operate on graph-structured data by aggregating and transforming information from a node's neighbors.\n",
    "\n",
    "**Key Idea**: Each node aggregates information from its neighbors to update its representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "# 2. Mathematical Foundations\n",
    "\n",
    "<a id=\"2.1\"></a>\n",
    "## 2.1 Graph Convolutional Networks (GCNs)\n",
    "\n",
    "Graph Convolutional Networks [[1]](#ref1) generalize convolutional neural networks to graphs. The convolution operation in GCNs involves aggregating feature information from a node's local neighborhood.\n",
    "\n",
    "### GCN Layer Definition\n",
    "\n",
    "The forward propagation rule for a single GCN layer is:\n",
    "\n",
    "$[\n",
    "H^{(l+1)} = \\sigma\\left( \\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}} H^{(l)} W^{(l)} \\right)\n",
    "]$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $( H^{(l)} )$: Node representations at layer $( l )$.\n",
    "- $( H^{(0)} = X )$: Input feature matrix.\n",
    "- $( \\tilde{A} = A + I )$: Adjacency matrix with added self-connections.\n",
    "- $( \\tilde{D} )$: Degree matrix of $( \\tilde{A} )$.\n",
    "- $( W^{(l)} )$: Weight matrix at layer $( l )$.\n",
    "- $( \\sigma )$: Activation function (e.g., ReLU).\n",
    "\n",
    "### Intuition\n",
    "\n",
    "- **Normalization**: The term \\( \\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}} \\) normalizes the adjacency matrix to prevent numerical instabilities and maintain the scale of features.\n",
    "- **Aggregation**: Each node aggregates features from its neighbors (including itself) to update its representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.2\"></a>\n",
    "## 2.2 Message Passing Neural Networks\n",
    "\n",
    "Message Passing Neural Networks (MPNNs) [[2]](#ref2) provide a general framework for GNNs.\n",
    "\n",
    "### Message Passing Framework\n",
    "\n",
    "1. **Message Function**: Compute messages from neighbors.\n",
    "\n",
    "   $[\n",
    "   m_{v}^{(l+1)} = \\sum_{u \\in \\mathcal{N}(v)} M^{(l)}(h_{v}^{(l)}, h_{u}^{(l)}, e_{uv})\n",
    "   ]$\n",
    "\n",
    "2. **Update Function**: Update node representations.\n",
    "\n",
    "   $[\n",
    "   h_{v}^{(l+1)} = U^{(l)}(h_{v}^{(l)}, m_{v}^{(l+1)})\n",
    "   ]$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $( h_{v}^{(l)} )$: Hidden state of node $( v )$ at layer $( l )$.\n",
    "- $( e_{uv} )$: Edge features between nodes $( u )$ and $( v )$.\n",
    "- $( \\mathcal{N}(v) )$: Neighbors of node $( v )$.\n",
    "- $( M^{(l)} )$, $( U^{(l)} )$: Learnable functions (e.g., neural networks).\n",
    "\n",
    "### Intuition\n",
    "\n",
    "- Nodes exchange messages with their neighbors and update their states based on the aggregated messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "# 3. Implementing GNNs\n",
    "\n",
    "We'll implement a GCN for node classification on the Cora dataset, a citation network where nodes represent papers, and edges represent citations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.1\"></a>\n",
    "## 3.1 Node Classification with GCNs\n",
    "\n",
    "### Setup\n",
    "\n",
    "We'll use the PyTorch Geometric library, which provides utilities for GNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch Geometric\n",
    "# Please run this cell to install the necessary packages.\n",
    "# For Google Colab users, uncomment the following lines.\n",
    "\n",
    "# !pip install torch\n",
    "# !pip install torch_geometric\n",
    "# !pip install torch_sparse\n",
    "# !pip install torch_scatter\n",
    "# !pip install torch_cluster\n",
    "# !pip install torch_spline_conv\n",
    "# !pip install torch-geometric\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Dataset\n",
    "\n",
    "We'll load the Cora dataset using PyTorch Geometric's built-in loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "print(f'Dataset: {dataset}')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_node_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Cora has only one graph\n",
    "print(data)\n",
    "\n",
    "# Data attributes\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = GCN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **GCNConv** layers perform the graph convolution operations.\n",
    "- We use ReLU activation and dropout for regularization.\n",
    "- The output layer uses a log-softmax activation for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Train Mask**: Specifies which nodes to use for training.\n",
    "- **Negative Log-Likelihood Loss**: Used for multi-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "_, pred = model(data).max(dim=1)\n",
    "correct = int(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "accuracy = correct / int(data.test_mask.sum())\n",
    "print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results:**\n",
    "\n",
    "- The GCN achieves a test accuracy of around 80% on the Cora dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.2\"></a>\n",
    "## 3.2 Link Prediction with Graph Autoencoders\n",
    "\n",
    "Link prediction involves predicting the existence of an edge between two nodes. We'll implement a simple Graph Autoencoder (GAE) for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import VGAE\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "\n",
    "# Prepare data for link prediction\n",
    "data = train_test_split_edges(data)\n",
    "\n",
    "class VariationalGraphAutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(VariationalGraphAutoEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 2 * out_channels)\n",
    "        self.conv_mu = GCNConv(2 * out_channels, out_channels)\n",
    "        self.conv_logstd = GCNConv(2 * out_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        return self.conv_mu(x, edge_index), self.conv_logstd(x, edge_index)\n",
    "\n",
    "    def reparameterize(self, mu, logstd):\n",
    "        if self.training:\n",
    "            std = torch.exp(logstd)\n",
    "            return mu + std * torch.randn_like(std)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z, pos_edge_index):\n",
    "        return torch.sigmoid((z[pos_edge_index[0]] * z[pos_edge_index[1]]).sum(dim=1))\n",
    "\n",
    "    def forward(self, data):\n",
    "        mu, logstd = self.encode(data.x, data.train_pos_edge_index)\n",
    "        z = self.reparameterize(mu, logstd)\n",
    "        return self.decode(z, data.train_pos_edge_index), mu, logstd\n",
    "\n",
    "model = VariationalGraphAutoEncoder(dataset.num_node_features, 16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **VGAE**: Variational Graph Autoencoder extends the GAE with probabilistic latent variables.\n",
    "- **Reparameterization Trick**: Allows backpropagation through stochastic variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the VGAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    pos_pred, mu, logstd = model(data)\n",
    "    # Reconstruction loss\n",
    "    pos_loss = -torch.log(pos_pred + 1e-15).mean()\n",
    "    # KL divergence\n",
    "    kl_loss = -0.5 / data.num_nodes * torch.mean(\n",
    "        torch.sum(1 + 2 * logstd - mu**2 - torch.exp(2 * logstd), dim=1))\n",
    "    loss = pos_loss + kl_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "for epoch in range(100):\n",
    "    loss = train()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the VGAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z = model.reparameterize(*model.encode(data.x, data.train_pos_edge_index))\n",
    "\n",
    "    # Positive edges\n",
    "    pos_edge_index = data.test_pos_edge_index\n",
    "    pos_pred = (z[pos_edge_index[0]] * z[pos_edge_index[1]]).sum(dim=1)\n",
    "    pos_pred = torch.sigmoid(pos_pred)\n",
    "\n",
    "    # Negative edges\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=data.train_pos_edge_index,\n",
    "        num_nodes=data.num_nodes,\n",
    "        num_neg_samples=pos_edge_index.size(1),\n",
    "    )\n",
    "    neg_pred = (z[neg_edge_index[0]] * z[neg_edge_index[1]]).sum(dim=1)\n",
    "    neg_pred = torch.sigmoid(neg_pred)\n",
    "\n",
    "    # Compute AUC\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    preds = torch.cat([pos_pred, neg_pred]).cpu().numpy()\n",
    "    labels = torch.cat([torch.ones(pos_pred.size(0)), torch.zeros(neg_pred.size(0))]).cpu().numpy()\n",
    "    auc = roc_auc_score(labels, preds)\n",
    "    print(f'Test AUC: {auc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results:**\n",
    "\n",
    "- The VGAE achieves a test AUC indicating its ability to predict links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "# 4. Latest Developments in GNNs\n",
    "\n",
    "<a id=\"4.1\"></a>\n",
    "## 4.1 Graph Attention Networks (GAT)\n",
    "\n",
    "Graph Attention Networks [[3]](#ref3) introduce attention mechanisms to GNNs, allowing nodes to weigh the importance of their neighbors.\n",
    "\n",
    "### GAT Layer Definition\n",
    "\n",
    "The attention coefficient between nodes $( i )$ and $( j )$:\n",
    "\n",
    "$[\n",
    "\\alpha_{ij} = \\frac{\\exp\\left( \\text{LeakyReLU}(a^{\\top}[W h_i \\parallel W h_j]) \\right)}{\\sum_{k \\in \\mathcal{N}(i)} \\exp\\left( \\text{LeakyReLU}(a^{\\top}[W h_i \\parallel W h_k]) \\right)}]$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $( h_i )$: Feature vector of node $( i )$.\n",
    "- $( W )$: Weight matrix.\n",
    "- $( a )$: Learnable attention vector.\n",
    "- $( \\parallel )$: Concatenation operator.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "- Nodes attend over their neighbors, assigning higher weights to more important ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.2\"></a>\n",
    "## 4.2 Graph Isomorphism Networks (GIN)\n",
    "\n",
    "Graph Isomorphism Networks [[4]](#ref4) are powerful GNNs capable of distinguishing different graph structures.\n",
    "\n",
    "### GIN Layer Definition\n",
    "\n",
    "$[\n",
    "h_v^{(k)} = \\text{MLP}^{(k)} \\left( (1 + \\epsilon^{(k)}) \\cdot h_v^{(k-1)} + \\sum_{u \\in \\mathcal{N}(v)} h_u^{(k-1)} \\right)\n",
    "]$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $( \\text{MLP}^{(k)} )$: Multi-layer perceptron at layer $( k )$.\n",
    "- $( \\epsilon^{(k)} )$: Learnable or fixed scalar.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "- GINs are designed to be as powerful as the Weisfeiler-Lehman graph isomorphism test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "# 5. Conclusion\n",
    "\n",
    "Graph Neural Networks extend deep learning to graph-structured data, enabling powerful modeling of relational data. We explored GCNs for node classification, GAEs for link prediction, and discussed advanced architectures like GAT and GIN. The field is rapidly evolving, with ongoing research pushing the boundaries of what's possible with GNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "# 6. References\n",
    "\n",
    "1. <a id=\"ref1\"></a>Kipf, T. N., & Welling, M. (2016). *Semi-Supervised Classification with Graph Convolutional Networks*. [arXiv:1609.02907](https://arxiv.org/abs/1609.02907)\n",
    "2. <a id=\"ref2\"></a>Gilmer, J., et al. (2017). *Neural Message Passing for Quantum Chemistry*. [arXiv:1704.01212](https://arxiv.org/abs/1704.01212)\n",
    "3. <a id=\"ref3\"></a>Veličković, P., et al. (2017). *Graph Attention Networks*. [arXiv:1710.10903](https://arxiv.org/abs/1710.10903)\n",
    "4. <a id=\"ref4\"></a>Xu, K., et al. (2018). *How Powerful are Graph Neural Networks?* [arXiv:1810.00826](https://arxiv.org/abs/1810.00826)\n",
    "\n",
    "---\n",
    "\n",
    "This notebook provides an in-depth exploration of Graph Neural Networks. You can run the code cells to see how GNNs are implemented and experiment with different architectures and datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
