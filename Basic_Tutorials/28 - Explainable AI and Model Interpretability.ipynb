{
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.x",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Explainable AI and Model Interpretability\n\n## Introduction\n\nAs neural networks become increasingly complex and ubiquitous in critical applications, understanding and interpreting their decisions is essential. Explainable AI (XAI) aims to make the decision-making processes of machine learning models transparent and interpretable. This tutorial explores techniques to interpret and explain neural network decisions, focusing on methods like LIME and SHAP values.\n\nWe'll delve into the underlying mathematics, provide example code, and explain the processes involved. We'll reference key papers and discuss some of the latest developments in this field. Visual aids will be included to enhance understanding."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Table of Contents\n\n1. [Understanding Explainable AI](#1)\n   - [Importance of Model Interpretability](#1.1)\n   - [Challenges in Interpreting Neural Networks](#1.2)\n2. [Model-Agnostic Methods](#2)\n   - [LIME (Local Interpretable Model-agnostic Explanations)](#2.1)\n     - [Underlying Mathematics](#2.1.1)\n     - [Example Code](#2.1.2)\n   - [SHAP (SHapley Additive exPlanations)](#2.2)\n     - [Underlying Mathematics](#2.2.1)\n     - [Example Code](#2.2.2)\n3. [Visualization Techniques](#3)\n   - [Saliency Maps](#3.1)\n   - [Grad-CAM (Gradient-weighted Class Activation Mapping)](#3.2)\n4. [Case Study: Interpreting a Neural Network for Classification](#4)\n   - [Dataset Preparation](#4.1)\n   - [Training the Model](#4.2)\n   - [Applying LIME](#4.3)\n   - [Applying SHAP](#4.4)\n   - [Visualizing Results](#4.5)\n5. [Latest Developments in Explainable AI](#5)\n   - [Integrated Gradients](#5.1)\n   - [DeepLIFT](#5.2)\n6. [Conclusion](#6)\n7. [References](#7)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"1\"></a>\n# 1. Understanding Explainable AI\n\n<a id=\"1.1\"></a>\n## 1.1 Importance of Model Interpretability\n\nModel interpretability is crucial for several reasons:\n\n- **Trust**: Users need to trust AI systems, especially in high-stakes applications like healthcare and finance.\n- **Compliance**: Regulations like GDPR require explanations for automated decisions.\n- **Debugging**: Interpretability helps identify model biases and errors.\n- **Insight**: Provides understanding of the underlying patterns in data."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"1.2\"></a>\n## 1.2 Challenges in Interpreting Neural Networks\n\n- **Complexity**: Deep neural networks have millions of parameters, making them black boxes.\n- **Nonlinearity**: Nonlinear activations and complex architectures hinder straightforward interpretation.\n- **Feature Interactions**: Features may interact in complex ways that are not easily disentangled."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"2\"></a>\n# 2. Model-Agnostic Methods\n\nModel-agnostic methods provide explanations that are applicable to any machine learning model. They treat the model as a black box and analyze input-output behavior."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"2.1\"></a>\n## 2.1 LIME (Local Interpretable Model-agnostic Explanations)\n\nLIME [[1]](#ref1) explains the predictions of any classifier by approximating it locally with an interpretable model."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"2.1.1\"></a>\n### Underlying Mathematics\n\nLIME generates explanations by:\n\n- **Perturbing the input**: Creating synthetic data around the instance to be explained.\n- **Weighting samples**: Assigning weights to the synthetic samples based on their proximity to the original instance.\n- **Fitting an interpretable model**: Training a simple model (e.g., linear regression) on the weighted samples.\n\nThe explanation model is obtained by minimizing the following loss function:\n\n $[\n xi(x) = \\arg\\min_{g \\in G} \\mathcal{L}(f, g, \\pi_x) + \\Omega(g)\n]$\n\nWhere:\n\n- $( x )$: Original instance.\n- $( f )$: Black-box model.\n- $( G )$: Class of interpretable models.\n- $( \\mathcal{L} )$: Loss function (e.g., mean squared error).\n- $( \\pi_x )$: Local weighting around $( x )$.\n- $( \\Omega(g) )$: Complexity penalty for the interpretable model."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"2.1.2\"></a>\n### Example Code\n\nWe'll demonstrate LIME using a simple text classification example."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install LIME\n# !pip install lime\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom lime.lime_text import LimeTextExplainer\n\n# Load dataset\ncategories = ['alt.atheism', 'soc.religion.christian']\nnewsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\nnewsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n\n# Create a pipeline\npipeline = make_pipeline(TfidfVectorizer(), LogisticRegression())\n\n# Train the model\npipeline.fit(newsgroups_train.data, newsgroups_train.target)\n\n# Choose an instance to explain\nidx = 83\ntext_instance = newsgroups_test.data[idx]\n\n# Predict the class\npred = pipeline.predict([text_instance])[0]\nprint(f'Predicted class: {newsgroups_test.target_names[pred]}')\n\n# Initialize LIME explainer\nexplainer = LimeTextExplainer(class_names=newsgroups_test.target_names)\n\n# Generate explanation\nexp = explainer.explain_instance(text_instance, pipeline.predict_proba, num_features=6)\n\n# Display explanation\nexp.show_in_notebook(text=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Explanation:**\n\n- **Pipeline**: Combines TF-IDF vectorization and logistic regression.\n- **LimeTextExplainer**: Used for text data.\n- **explain_instance**: Generates the explanation for the selected instance."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"2.2\"></a>\n## 2.2 SHAP (SHapley Additive exPlanations)\n\nSHAP [[2]](#ref2) explains predictions by computing the contribution of each feature to the prediction, based on concepts from cooperative game theory."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"2.2.1\"></a>\n### Underlying Mathematics\n\nSHAP values are based on Shapley values from game theory, which represent the average marginal contribution of a feature value across all possible coalitions.\n\nThe SHAP value for feature $( i )$ is:\n\n$[\n\\phi_i = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!(n - |S| - 1)!}{n!} [f_{S \\cup \\{i\\}}(x_{S \\cup \\{i\\}}) - f_S(x_S)]\n]$\n\nWhere:\n\n- $( N )$: Set of all features.\n- $( S )$: Subset of features excluding $( i )$.\n- $( f_S )$: Model trained on features in $( S )$.\n- $( x_S )$: Values of features in $( S )$.\n\nComputing exact Shapley values is computationally expensive, so SHAP uses approximations."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"2.2.2\"></a>\n### Example Code\n\nWe'll demonstrate SHAP using a gradient boosting classifier."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install SHAP\n# !pip install shap\n\nimport shap\nimport xgboost\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\n# Load dataset\ndata = load_breast_cancer()\nX = pd.DataFrame(data.data, columns=data.feature_names)\ny = data.target\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train model\nmodel = xgboost.XGBClassifier()\nmodel.fit(X_train, y_train)\n\n# Explain predictions\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_test)\n\n# Visualize the first prediction's explanation\nshap.initjs()\nshap.force_plot(explainer.expected_value, shap_values[0,:], X_test.iloc[0,:])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Explanation:**\n\n- **TreeExplainer**: Optimized for tree-based models like XGBoost.\n- **shap_values**: SHAP values for the test set.\n- **force_plot**: Visualizes the SHAP values for a single prediction."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"3\"></a>\n# 3. Visualization Techniques\n\nVisualization techniques help interpret models by highlighting important features or regions in the input that influence the model's decisions."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"3.1\"></a>\n## 3.1 Saliency Maps\n\nSaliency maps highlight the pixels in an image that most affect the prediction. They are computed by taking the gradient of the output with respect to the input image:\n\n$[\nS = \\left| \\frac{\\partial y}{\\partial x} \\right|\n]$\n\nWhere:\n\n- $( y )$: Model output (e.g., class score).\n- $( x )$: Input image.\n- $( S )$: Saliency map."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"3.2\"></a>\n## 3.2 Grad-CAM (Gradient-weighted Class Activation Mapping)\n\nGrad-CAM [[3]](#ref3) generates visual explanations for convolutional neural networks by using the gradients of any target concept flowing into the final convolutional layer to produce a coarse localization map highlighting important regions.\n\n### Algorithm\n\n1. Compute the gradient of the target class score $( y^c )$ with respect to feature maps $( A^k )$ of a convolutional layer:\n\n   $[\n   \\frac{\\partial y^c}{\\partial A^k}\n   ]$\n\n2. Compute the weights $( \\alpha_k^c )$ by global average pooling over the gradients:\n\n   $[\n   \\alpha_k^c = \\frac{1}{Z} \\sum_i \\sum_j \\frac{\\partial y^c}{\\partial A_{ij}^k}\n   ]$\n\n3. Compute the weighted combination of feature maps:\n\n   $[\n   L_{\\text{Grad-CAM}}^c = \\text{ReLU}\\left( \\sum_k \\alpha_k^c A^k \\right)\n   ]$"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Explanation:**\n\n- **Feature Maps ($( A^k )$)**: Activations from a convolutional layer.\n- **Weights ($( \\alpha_k^c )$)**: Importance of each feature map for the target class."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"4\"></a>\n# 4. Case Study: Interpreting a Neural Network for Classification\n\nWe'll apply LIME and SHAP to interpret a neural network trained on the MNIST dataset."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"4.1\"></a>\n## 4.1 Dataset Preparation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import libraries\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load MNIST dataset\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n\n# Normalize and reshape data\nx_train = x_train.astype('float32') / 255.0\nx_test = x_test.astype('float32') / 255.0\nx_train = np.expand_dims(x_train, -1)\nx_test = np.expand_dims(x_test, -1)\n\n# One-hot encode labels\ny_train_cat = tf.keras.utils.to_categorical(y_train, 10)\ny_test_cat = tf.keras.utils.to_categorical(y_test, 10)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"4.2\"></a>\n## 4.2 Training the Model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build a simple CNN model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n\nmodel = Sequential([\n    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n    MaxPooling2D(pool_size=(2, 2)),\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dense(10, activation='softmax')\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(x_train, y_train_cat, epochs=5, batch_size=128, validation_split=0.1)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Evaluate the Model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate on test data\nloss, accuracy = model.evaluate(x_test, y_test_cat)\nprint(f'Test accuracy: {accuracy:.4f}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"4.3\"></a>\n## 4.3 Applying LIME"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install LIME for image explanations\n# !pip install lime\n\nfrom lime import lime_image\nfrom skimage.segmentation import mark_boundaries\n\n# Initialize explainer\nexplainer = lime_image.LimeImageExplainer()\n\n# Choose an instance to explain\nidx = 12\nimage = x_test[idx]\n\n# Define a prediction function\ndef predict_fn(images):\n    images = np.array(images)\n    return model.predict(images)\n\n# Generate explanation\nexplanation = explainer.explain_instance(image.astype('double'), \n                                         predict_fn, \n                                         top_labels=5, \n                                         hide_color=0, \n                                         num_samples=1000)\n\n# Get image and mask\nfrom skimage.color import gray2rgb\n\ntemp, mask = explanation.get_image_and_mask(y_test[idx], positive_only=True, num_features=5, hide_rest=False)\n\n# Display the image\nfig, ax = plt.subplots(1, 2, figsize=(8, 4))\nax[0].imshow(image.squeeze(), cmap='gray')\nax[0].set_title('Original Image')\nax[0].axis('off')\n\nax[1].imshow(mark_boundaries(gray2rgb(image.squeeze()), mask))\nax[1].set_title('LIME Explanation')\nax[1].axis('off')\n\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"4.4\"></a>\n## 4.4 Applying SHAP"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install SHAP\n# !pip install shap\n\nimport shap\n\n# Create a subset of data for background\nbackground = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]\n\n# Explain predictions\nexplainer = shap.DeepExplainer(model, background)\nshap_values = explainer.shap_values(x_test[idx:idx+1])\n\n# Plot the SHAP values for the given image\nshap.image_plot(shap_values, x_test[idx:idx+1])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"4.5\"></a>\n## 4.5 Visualizing Results\n\nThe LIME and SHAP visualizations highlight the regions of the image that contribute most to the model's prediction. These visual explanations can help us understand what the model is focusing on."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"5\"></a>\n# 5. Latest Developments in Explainable AI\n\n<a id=\"5.1\"></a>\n## 5.1 Integrated Gradients\n\nIntegrated Gradients [[4]](#ref4) is a method that attributes the prediction of a deep network to its input features by integrating the gradients of the output with respect to the input along a straight line from a baseline to the input."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Mathematical Formulation\n\nThe integrated gradients along the $( i )$-th dimension are computed as:\n\n$[\nIG_i(x) = (x_i - x_i') \\times \\int_{\\alpha=0}^1 \\frac{\\partial F(x' + \\alpha \\times (x - x'))}{\\partial x_i} d\\alpha\n]$\n\nWhere:\n\n- $( x )$: Input.\n- $( x' )$: Baseline input.\n- $( F )$: Model function.\n- $( \\alpha )$: Scalar between 0 and 1."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"5.2\"></a>\n## 5.2 DeepLIFT\n\nDeepLIFT [[5]](#ref5) (Deep Learning Important FeaTures) is a method that compares the activation of each neuron to its reference activation and assigns contribution scores according to the difference."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Key Concepts\n\n- **Reference Activation**: Baseline activation when input is a reference (e.g., zero image).\n- **Contribution Scores**: Difference between activation for actual input and reference input."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"6\"></a>\n# 6. Conclusion\n\nExplainable AI is essential for building trust, ensuring compliance, and gaining insights into machine learning models. Methods like LIME and SHAP provide model-agnostic explanations, while visualization techniques like saliency maps and Grad-CAM help interpret neural networks. Understanding and applying these methods enables practitioners to create more transparent and accountable AI systems."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"7\"></a>\n# 7. References\n\n1. <a id=\"ref1\"></a>Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). *\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier*. [arXiv:1602.04938](https://arxiv.org/abs/1602.04938)\n2. <a id=\"ref2\"></a>Lundberg, S. M., & Lee, S.-I. (2017). *A Unified Approach to Interpreting Model Predictions*. [arXiv:1705.07874](https://arxiv.org/abs/1705.07874)\n3. <a id=\"ref3\"></a>Selvaraju, R. R., et al. (2017). *Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization*. [arXiv:1610.02391](https://arxiv.org/abs/1610.02391)\n4. <a id=\"ref4\"></a>Sundararajan, M., Taly, A., & Yan, Q. (2017). *Axiomatic Attribution for Deep Networks*. [arXiv:1703.01365](https://arxiv.org/abs/1703.01365)\n5. <a id=\"ref5\"></a>Shrikumar, A., Greenside, P., & Kundaje, A. (2017). *Learning Important Features Through Propagating Activation Differences*. [arXiv:1704.02685](https://arxiv.org/abs/1704.02685)\n\n---\n\nThis notebook provides an in-depth exploration of Explainable AI and Model Interpretability. You can run the code cells to see how these methods are implemented and experiment with different models and datasets."
  }
 ]
}
