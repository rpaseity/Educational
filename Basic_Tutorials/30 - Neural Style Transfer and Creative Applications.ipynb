{
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.x",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Neural Style Transfer and Creative Applications\n\n## Introduction\n\nNeural Style Transfer is an exciting application of deep learning that allows us to create artistic images by combining the content of one image with the style of another. This technique leverages convolutional neural networks (CNNs) to extract and recombine image content and style representations.\n\nIn this tutorial, we'll explore how neural networks can create artistic images through style transfer. We'll delve into the underlying mathematics, provide example code using PyTorch, and explain the processes involved. We'll reference key papers and discuss some of the latest developments in this field. Relevant imagery (generated by code) will be included to enhance understanding."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Table of Contents\n\n1. [Understanding Neural Style Transfer](#1)\n   - [What is Neural Style Transfer?](#1.1)\n   - [Underlying Mathematics](#1.2)\n2. [Implementing Neural Style Transfer](#2)\n   - [Prerequisites](#2.1)\n   - [Code Implementation with PyTorch](#2.2)\n3. [Creative Applications](#3)\n   - [Advanced Techniques](#3.1)\n   - [Latest Developments](#3.2)\n4. [Conclusion](#4)\n5. [References](#5)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"1\"></a>\n# 1. Understanding Neural Style Transfer"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"1.1\"></a>\n## 1.1 What is Neural Style Transfer?\n\nNeural Style Transfer (NST) is a technique that takes two images—a **content image** and a **style image**—and blends them together so that the output image looks like the content image painted in the style of the style image.\n\n**Example:**\n\n- **Content Image**: A photograph of a cityscape.\n- **Style Image**: A painting by Vincent van Gogh.\n- **Output Image**: The cityscape rendered in the style of van Gogh's painting.\n\nThis technique was introduced by [Gatys et al., 2015](#ref1), who demonstrated that deep CNNs can be used to separate and recombine content and style of natural images."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"1.2\"></a>\n## 1.2 Underlying Mathematics\n\nNeural Style Transfer relies on the representations learned by a pre-trained CNN, typically the VGG network. The process involves minimizing a loss function that captures the differences in content and style between the generated image and the input images.\n\n### Content Representation\n\nThe content loss measures the difference in high-level features between the generated image $( \\mathbf{G} )$ and the content image $( \\mathbf{C} )$. This is computed using the activations from a specific layer (e.g., layer $( l )$) of the CNN:\n\n$[\n\\mathcal{L}_{\\text{content}}(\\mathbf{G}, \\mathbf{C}) = \\frac{1}{2} \\sum_{i,j} (F_{ij}^{l} - P_{ij}^{l})^{2}\n]$\n\n- $( F^{l} )$: Feature representation of $( \\mathbf{G} )$ at layer $( l )$.\n- $( P^{l} )$: Feature representation of $( \\mathbf{C} )$ at layer $( l )$.\n\n### Style Representation\n\nThe style loss measures the difference in style between $( \\mathbf{G} )$ and the style image $( \\mathbf{S} )$. This is done by comparing the **Gram matrices** of their feature maps:\n\n$[\n\\mathcal{L}_{\\text{style}}(\\mathbf{G}, \\mathbf{S}) = \\sum_{l} w_{l} E_{l}\n]$\n\nWhere $( E_{l} )$ is the style loss for layer $( l )$:\n\n$[\nE_{l} = \\frac{1}{4 N_{l}^{2} M_{l}^{2}} \\sum_{i,j} (G_{ij}^{l} - A_{ij}^{l})^{2}\n]$\n\n- $( G^{l} )$: Gram matrix of $( \\mathbf{G} )$ at layer $( l )$.\n- $( A^{l} )$: Gram matrix of $( \\mathbf{S} )$ at layer $( l )$.\n- $( N_{l} )$: Number of feature maps at layer $( l )$.\n- $( M_{l} )$: Height $( \\times )$ Width of the feature map at layer $( l )$.\n- $( w_{l} )$: Weight assigned to layer $( l )$.\n\nThe Gram matrix $( G^{l} )$ is computed as:\n\n$[\nG_{ij}^{l} = \\sum_{k} F_{ik}^{l} F_{jk}^{l}\n]$\n\n### Total Variation Loss (Optional)\n\nTo encourage spatial smoothness in the generated image, a total variation loss can be added:\n\n$[\n\\mathcal{L}_{\\text{tv}}(\\mathbf{G}) = \\sum_{i,j} ((G_{i,j+1} - G_{i,j})^{2} + (G_{i+1,j} - G_{i,j})^{2})\n]$\n\n### Total Loss\n\nThe total loss is a weighted sum of the content loss, style loss, and total variation loss:\n\n$[\n\\mathcal{L}_{\\text{total}} = \\alpha \\, \\mathcal{L}_{\\text{content}} + \\beta \\, \\mathcal{L}_{\\text{style}} + \\gamma \\, \\mathcal{L}_{\\text{tv}}\n]$\n\n- $( \\alpha )$: Weight for content loss.\n- $( \\beta )$: Weight for style loss.\n- $( \\gamma )$: Weight for total variation loss."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"2\"></a>\n# 2. Implementing Neural Style Transfer"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"2.1\"></a>\n## 2.1 Prerequisites\n\nWe'll implement Neural Style Transfer using PyTorch. Before we begin, ensure you have the following libraries installed:\n\n- **PyTorch**\n- **Torchvision**\n- **PIL** (Python Imaging Library)\n\n**Note:** GPU acceleration is highly recommended for faster computation."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Import necessary libraries\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport copy\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f'Using device: {device}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"2.2\"></a>\n## 2.2 Code Implementation with PyTorch\n\n### 2.2.1 Loading and Preprocessing Images\n\nWe'll start by loading the content and style images. We'll define functions to load and preprocess the images."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Image loading and preprocessing functions\ndef image_loader(image_name, imsize):\n    loader = transforms.Compose([\n        transforms.Resize(imsize),\n        transforms.ToTensor()\n    ])\n    image = Image.open(image_name)\n    image = loader(image).unsqueeze(0)\n    return image.to(device, torch.float)\n\n# Define image size\nimsize = 512 if torch.cuda.is_available() else 128  # Use small size if no GPU\n\n# Load content and style images\ncontent_img = image_loader(\"path_to_content_image.jpg\", imsize)\nstyle_img = image_loader(\"path_to_style_image.jpg\", imsize)\n\nassert content_img.size() == style_img.size(), \"Content and style images must be the same size.\"\n\n# Display images\ndef imshow(tensor, title=None):\n    image = tensor.cpu().clone()\n    image = image.squeeze(0)\n    unloader = transforms.ToPILImage()\n    image = unloader(image)\n    plt.imshow(image)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\nplt.figure()\nimshow(content_img, title='Content Image')\n\nplt.figure()\nimshow(style_img, title='Style Image')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Note:** Replace `\"path_to_content_image.jpg\"` and `\"path_to_style_image.jpg\"` with the actual file paths to your images."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.2.2 Defining Content and Style Loss Modules\n\nWe'll create custom modules to compute the content and style losses."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Content Loss Module\nclass ContentLoss(nn.Module):\n    def __init__(self, target,):\n        super(ContentLoss, self).__init__()\n        self.target = target.detach()\n    def forward(self, input):\n        self.loss = F.mse_loss(input, self.target)\n        return input\n\n# Style Loss Module\ndef gram_matrix(input):\n    batch_size, feature_maps, h, w = input.size()\n    features = input.view(batch_size * feature_maps, h * w)\n    G = torch.mm(features, features.t())\n    return G.div(batch_size * feature_maps * h * w)\n\nclass StyleLoss(nn.Module):\n    def __init__(self, target_feature):\n        super(StyleLoss, self).__init__()\n        self.target = gram_matrix(target_feature).detach()\n    def forward(self, input):\n        G = gram_matrix(input)\n        self.loss = F.mse_loss(G, self.target)\n        return input"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.2.3 Loading the Pre-trained Model\n\nWe'll use a pre-trained VGG19 model from `torchvision.models`."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Load pre-trained VGG19 model\ncnn = models.vgg19(pretrained=True).features.to(device).eval()\n\n# Normalization module\ncnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\ncnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n\nclass Normalization(nn.Module):\n    def __init__(self, mean, std):\n        super(Normalization, self).__init__()\n        self.mean = mean.view(-1, 1, 1)\n        self.std = std.view(-1, 1, 1)\n    def forward(self, img):\n        return (img - self.mean) / self.std"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.2.4 Building the Model\n\nWe'll build a new model that incorporates the content and style loss modules."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import copy\n\ndef get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n                               style_img, content_img,\n                               content_layers=['conv_4'],\n                               style_layers=['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']):\n    cnn = copy.deepcopy(cnn)\n    \n    normalization = Normalization(normalization_mean, normalization_std).to(device)\n    \n    content_losses = []\n    style_losses = []\n    \n    model = nn.Sequential(normalization)\n    \n    i = 0  # Increment every time we see a conv\n    for layer in cnn.children():\n        if isinstance(layer, nn.Conv2d):\n            i += 1\n            name = 'conv_{}'.format(i)\n        elif isinstance(layer, nn.ReLU):\n            name = 'relu_{}'.format(i)\n            layer = nn.ReLU(inplace=False)\n        elif isinstance(layer, nn.MaxPool2d):\n            name = 'pool_{}'.format(i)\n        elif isinstance(layer, nn.BatchNorm2d):\n            name = 'bn_{}'.format(i)\n        else:\n            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n        \n        model.add_module(name, layer)\n        \n        if name in content_layers:\n            target = model(content_img).detach()\n            content_loss = ContentLoss(target)\n            model.add_module(\"content_loss_{}\".format(i), content_loss)\n            content_losses.append(content_loss)\n        \n        if name in style_layers:\n            target_feature = model(style_img).detach()\n            style_loss = StyleLoss(target_feature)\n            model.add_module(\"style_loss_{}\".format(i), style_loss)\n            style_losses.append(style_loss)\n    \n    # Trim off the layers after the last content and style losses\n    for i in range(len(model) -1, -1, -1):\n        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n            break\n    model = model[:(i+1)]\n    \n    return model, style_losses, content_losses"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.2.5 Setting Up the Input Image\n\nWe'll use the content image as the starting point for the generated image."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Input image\nimage = content_img.clone().requires_grad_(True)\n\n# If you want to start from a white noise image, uncomment the following:\n# input_img = torch.randn(content_img.data.size(), device=device)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.2.6 Defining the Optimization Algorithm\n\nWe'll use the LBFGS optimizer as recommended by the original paper."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def get_input_optimizer(input_img):\n    optimizer = optim.LBFGS([input_img])\n    return optimizer"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.2.7 Running the Style Transfer\n\nWe'll define the `run_style_transfer` function to perform the optimization."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def run_style_transfer(cnn, normalization_mean, normalization_std,\n                        content_img, style_img, input_img, num_steps=300,\n                        style_weight=1000000, content_weight=1):\n    print('Building the style transfer model...')\n    model, style_losses, content_losses = get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n                                                                     style_img, content_img)\n    optimizer = get_input_optimizer(input_img)\n    \n    print('Optimizing...')\n    run = [0]\n    while run[0] <= num_steps:\n        def closure():\n            input_img.data.clamp_(0, 1)\n            optimizer.zero_grad()\n            model(input_img)\n            style_score = 0\n            content_score = 0\n            for sl in style_losses:\n                style_score += sl.loss\n            for cl in content_losses:\n                content_score += cl.loss\n            \n            style_score *= style_weight\n            content_score *= content_weight\n            \n            loss = style_score + content_score\n            loss.backward()\n            \n            run[0] += 1\n            if run[0] % 50 == 0:\n                print(\"Run {}:\").format(run)\n                print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n                    style_score.item(), content_score.item()))\n                print()\n            \n            return style_score + content_score\n        optimizer.step(closure)\n    \n    # Clamp the final image\n    input_img.data.clamp_(0, 1)\n    return input_img"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.2.8 Performing Style Transfer"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Run the style transfer\noutput = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n                            content_img, style_img, image, num_steps=300)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.2.9 Displaying the Output Image"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Display the output image\nplt.figure()\nimshow(output, title='Output Image')\nplt.ioff()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Result:** The output image should display the content of the content image rendered in the style of the style image."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"3\"></a>\n# 3. Creative Applications"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"3.1\"></a>\n## 3.1 Advanced Techniques\n\n### 3.1.1 Adjusting Style and Content Weights\n\nBy modifying the `style_weight` and `content_weight` parameters, you can control the emphasis on style versus content in the generated image.\n\n- **Higher `style_weight`**: The output image will more closely mimic the style image.\n- **Higher `content_weight`**: The output image will retain more of the content image's structure."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.1.2 Multi-Style Transfer\n\nYou can blend multiple styles by combining their style losses. This involves computing the style loss for each style image and summing them with appropriate weights."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.1.3 Style Transfer on Videos\n\nApplying neural style transfer to each frame of a video allows for the creation of stylized videos. Temporal consistency techniques are used to ensure smooth transitions between frames."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"3.2\"></a>\n## 3.2 Latest Developments\n\n### 3.2.1 Real-Time Style Transfer\n\nJohnson et al. [[2]](#ref2) introduced methods for real-time style transfer using feed-forward networks trained to approximate the optimization process.\n\n### 3.2.2 Adaptive Instance Normalization (AdaIN)\n\nHuang and Belongie [[3]](#ref3) proposed AdaIN, which aligns the mean and variance of the content features with those of the style features, enabling arbitrary style transfer in real-time.\n\n### 3.2.3 StyleGAN and GAN-based Methods\n\nGenerative Adversarial Networks (GANs) have been used to generate high-quality stylized images. StyleGAN [[4]](#ref4) allows for fine-grained control over style at different levels of detail."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"4\"></a>\n# 4. Conclusion\n\nNeural Style Transfer is a fascinating intersection of art and deep learning, demonstrating the creative capabilities of neural networks. By understanding the underlying mathematics and implementation details, you can experiment with creating your own stylized images and explore further advancements in this field."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"5\"></a>\n# 5. References\n\n1. <a id=\"ref1\"></a>Gatys, L. A., Ecker, A. S., & Bethge, M. (2015). *A Neural Algorithm of Artistic Style*. [arXiv:1508.06576](https://arxiv.org/abs/1508.06576)\n2. <a id=\"ref2\"></a>Johnson, J., Alahi, A., & Fei-Fei, L. (2016). *Perceptual Losses for Real-Time Style Transfer and Super-Resolution*. [arXiv:1603.08155](https://arxiv.org/abs/1603.08155)\n3. <a id=\"ref3\"></a>Huang, X., & Belongie, S. (2017). *Arbitrary Style Transfer in Real-Time with Adaptive Instance Normalization*. [arXiv:1703.06868](https://arxiv.org/abs/1703.06868)\n4. <a id=\"ref4\"></a>Karras, T., Laine, S., & Aila, T. (2019). *A Style-Based Generator Architecture for Generative Adversarial Networks*. [arXiv:1812.04948](https://arxiv.org/abs/1812.04948)\n\n---\n\nThis notebook provides an in-depth exploration of Neural Style Transfer and its creative applications. You can run the code cells to see how the style transfer is implemented and experiment with different images and parameters."
  }
 ]
}
