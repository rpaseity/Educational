{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Computer Vision Techniques\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Object detection and segmentation are fundamental tasks in computer vision that involve identifying and localizing objects within images. Advanced models like Faster R-CNN, YOLO, and Mask R-CNN have significantly improved the accuracy and speed of these tasks, enabling applications in autonomous driving, medical imaging, and surveillance.\n",
    "\n",
    "In this tutorial, we'll study these models in depth, including the underlying mathematics, implementation details, and the latest developments in the field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Object Detection Overview](#1)\n",
    "   - [Problem Definition](#1.1)\n",
    "   - [Evaluation Metrics](#1.2)\n",
    "2. [Faster R-CNN](#2)\n",
    "   - [Architecture](#2.1)\n",
    "   - [Region Proposal Network (RPN)](#2.2)\n",
    "   - [Mathematical Formulation](#2.3)\n",
    "   - [Implementation](#2.4)\n",
    "3. [You Only Look Once (YOLO)](#3)\n",
    "   - [Architecture](#3.1)\n",
    "   - [Mathematical Formulation](#3.2)\n",
    "   - [Implementation](#3.3)\n",
    "4. [Mask R-CNN](#4)\n",
    "   - [Architecture](#4.1)\n",
    "   - [Mathematical Formulation](#4.2)\n",
    "   - [Implementation](#4.3)\n",
    "5. [Latest Developments](#5)\n",
    "   - [EfficientDet](#5.1)\n",
    "   - [DETR (Detection Transformer)](#5.2)\n",
    "6. [Conclusion](#6)\n",
    "7. [References](#7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# 1. Object Detection Overview\n",
    "\n",
    "Object detection involves not only classifying objects within an image but also localizing them by drawing bounding boxes around each object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.1\"></a>\n",
    "## 1.1 Problem Definition\n",
    "\n",
    "Given an input image $( I )$, the goal is to produce a set of bounding boxes $( B = \\{b_1, b_2, ..., b_n\\} )$ and their corresponding class labels $( C = \\{c_1, c_2, ..., c_n\\} )$, where each bounding box $( b_i )$ defines the location of an object in the image, and $( c_i )$ is the class label of that object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.2\"></a>\n",
    "## 1.2 Evaluation Metrics\n",
    "\n",
    "- **Intersection over Union (IoU)**: Measures the overlap between the predicted bounding box and the ground truth bounding box.\n",
    "\n",
    "$[\n",
    "IoU = \\frac{\\text{Area of Overlap}}{\\text{Area of Union}}\n",
    "]$\n",
    "\n",
    "- **Mean Average Precision (mAP)**: Computes the average precision across all classes at different IoU thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "# 2. Faster R-CNN\n",
    "\n",
    "Faster R-CNN [[1]](#ref1) is a two-stage object detection model that significantly improved detection speed and accuracy compared to its predecessors (R-CNN and Fast R-CNN). It introduces the Region Proposal Network (RPN) for generating region proposals efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.1\"></a>\n",
    "## 2.1 Architecture\n",
    "\n",
    "The architecture of Faster R-CNN consists of three main components:\n",
    "\n",
    "1. **Convolutional Neural Network (CNN)**: Serves as a backbone (e.g., VGG, ResNet) to extract feature maps from the input image.\n",
    "2. **Region Proposal Network (RPN)**: Generates region proposals from the feature maps.\n",
    "3. **Region of Interest (RoI) Pooling and Classification**: Extracts fixed-size feature vectors from the proposals and classifies them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.2\"></a>\n",
    "## 2.2 Region Proposal Network (RPN)\n",
    "\n",
    "The RPN is a fully convolutional network that predicts object bounds and objectness scores at each position. It slides a small network over the feature map output by the backbone CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anchors\n",
    "\n",
    "Anchors are reference boxes centered at the sliding window's center. They have different scales and aspect ratios to detect objects of various sizes and shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.3\"></a>\n",
    "## 2.3 Mathematical Formulation\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "The RPN is trained with a multitask loss function:\n",
    "\n",
    "$[\n",
    "L(\\{p_i\\}, \\{t_i\\}) = \\frac{1}{N_{cls}} \\sum_i L_{cls}(p_i, p_i^*) + \\lambda \\frac{1}{N_{reg}} \\sum_i p_i^* L_{reg}(t_i, t_i^*)\n",
    "]$\n",
    "\n",
    "- $( p_i )$: Predicted objectness score for anchor $( i )$.\n",
    "- $( p_i^* )$: Ground truth label (1 if positive, 0 if negative).\n",
    "- $( t_i )$: Predicted bounding box regression targets.\n",
    "- $( t_i^* )$: Ground truth bounding box regression targets.\n",
    "- $( L_{cls} )$: Classification loss (e.g., cross-entropy).\n",
    "- $( L_{reg} )$: Regression loss (e.g., smooth L1 loss).\n",
    "- $( N_{cls} )$, $( N_{reg} )$: Normalization terms.\n",
    "- $( \\lambda )$: Balancing parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounding Box Regression\n",
    "\n",
    "The bounding box regression targets are parameterized as:\n",
    "\n",
    "$[\n",
    "\\begin{align}\n",
    "t_x &= (x - x_a) / w_a, & t_y &= (y - y_a) / h_a, \\\\\n",
    "t_w &= \\log(w / w_a), & t_h &= \\log(h / h_a),\n",
    "\\end{align}\n",
    "]$\n",
    "\n",
    "- $( (x, y, w, h) )$: Coordinates of the ground truth box.\n",
    "- $( (x_a, y_a, w_a, h_a) )$: Coordinates of the anchor box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.4\"></a>\n",
    "## 2.4 Implementation\n",
    "\n",
    "We'll implement Faster R-CNN using PyTorch and the `torchvision` library, which provides pre-trained models and utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# Load a pre-trained model for classification and return only the features\n",
    "backbone = torchvision.models.resnet50(pretrained=True)\n",
    "backbone = torch.nn.Sequential(*(list(backbone.children())[:-2]))\n",
    "backbone.out_channels = 2048\n",
    "\n",
    "# Define the anchor generator\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "# Define the ROI pooling\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "\n",
    "# Put the pieces together inside a FasterRCNN model\n",
    "model = FasterRCNN(backbone,\n",
    "                   num_classes=91,  # 90 classes + background\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   box_roi_pool=roi_pooler)\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Backbone**: We use ResNet-50 pretrained on ImageNet as the feature extractor.\n",
    "- **Anchor Generator**: Defines the sizes and aspect ratios of the anchors.\n",
    "- **RoI Pooler**: Extracts fixed-size feature maps for each proposal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "Training an object detection model requires a dataset with images and annotations (bounding boxes and labels). The COCO dataset is commonly used but is large and requires significant resources. For demonstration purposes, we'll skip the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Let's perform inference using the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Load an image\n",
    "image = Image.open('path_to_image.jpg').convert('RGB')\n",
    "\n",
    "# Transform the image\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "image = transform(image)\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    prediction = model([image.to(device)])\n",
    "\n",
    "# Print predictions\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Replace `'path_to_image.jpg'` with the actual path to an image file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "# 3. You Only Look Once (YOLO)\n",
    "\n",
    "<a id=\"3.1\"></a>\n",
    "## 3.1 Architecture\n",
    "\n",
    "YOLO [[2]](#ref2) is a single-stage object detection model that formulates detection as a regression problem, directly predicting bounding boxes and class probabilities from the entire image in one evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Division\n",
    "\n",
    "The image is divided into an $( S \\times S )$ grid. Each grid cell predicts $( B )$ bounding boxes and confidence scores, along with class probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.2\"></a>\n",
    "## 3.2 Mathematical Formulation\n",
    "\n",
    "### Prediction Vector\n",
    "\n",
    "Each grid cell predicts a vector:\n",
    "\n",
    "$[\n",
    "\\text{Prediction} = [p_c, x, y, w, h, c_1, c_2, ..., c_N]\n",
    "]$\n",
    "\n",
    "- $( p_c )$: Confidence score (probability that an object is present and the IoU between predicted and ground truth boxes).\n",
    "- $( x, y )$: Coordinates relative to the grid cell.\n",
    "- $( w, h )$: Width and height relative to the whole image.\n",
    "- $( c_i )$: Class probability for class $( i )$.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "The loss function combines localization error, confidence error, and classification error:\n",
    "\n",
    "$[\n",
    "L = \\lambda_{coord} \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{obj} \\left[ (x_i - \\hat{x}_i)^2 + (y_i - \\hat{y}_i)^2 \\right] + \\cdots\n",
    "]$\n",
    "\n",
    "- $( \\mathbb{1}_{ij}^{obj} )$: Indicator function (1 if object is present in grid cell $( i )$ and bounding box $( j )$, 0 otherwise).\n",
    "- $( \\lambda_{coord} )$: Weighting parameter for coordinate loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.3\"></a>\n",
    "## 3.3 Implementation\n",
    "\n",
    "We'll use the `torch.hub` interface to load a pre-trained YOLOv5 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import torch.hub\n",
    "import torch\n",
    "\n",
    "# Load pre-trained YOLOv5 model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Load an image\n",
    "img = 'path_to_image.jpg'  # or URL or OpenCV image\n",
    "\n",
    "# Inference\n",
    "results = model(img)\n",
    "\n",
    "# Results\n",
    "results.print()\n",
    "# results.show()  # Display image with predictions\n",
    "# results.save()  # Save image with predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Replace `'path_to_image.jpg'` with the actual path to an image file. The `results.show()` and `results.save()` methods can display and save the image with predicted bounding boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "# 4. Mask R-CNN\n",
    "\n",
    "<a id=\"4.1\"></a>\n",
    "## 4.1 Architecture\n",
    "\n",
    "Mask R-CNN [[3]](#ref3) extends Faster R-CNN by adding a branch for predicting segmentation masks on each Region of Interest (RoI). It enables instance segmentation, which involves detecting objects and delineating their shapes at the pixel level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Components\n",
    "\n",
    "- **Backbone CNN**: Extracts feature maps from the input image.\n",
    "- **Region Proposal Network (RPN)**: Generates region proposals.\n",
    "- **RoI Align**: Improves upon RoI Pooling by reducing quantization errors.\n",
    "- **Bounding Box Head**: Classifies and refines bounding boxes.\n",
    "- **Mask Head**: Predicts a binary mask for each RoI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.2\"></a>\n",
    "## 4.2 Mathematical Formulation\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "The loss function combines classification loss, bounding box regression loss, and mask loss:\n",
    "\n",
    "$[\n",
    "L = L_{cls} + L_{box} + L_{mask}\n",
    "]$\n",
    "\n",
    "- **Classification Loss ($( L_{cls} )$)**: Cross-entropy loss over classes.\n",
    "- **Bounding Box Loss ($( L_{box} )$)**: Smooth L1 loss for bounding box regression.\n",
    "- **Mask Loss ($( L_{mask} )$)**: Average binary cross-entropy loss over pixels for the predicted mask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.3\"></a>\n",
    "## 4.3 Implementation\n",
    "\n",
    "We'll use the `torchvision` library to load a pre-trained Mask R-CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# Load pre-trained Mask R-CNN model\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load and transform an image\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "image = Image.open('path_to_image.jpg').convert('RGB')\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "image = transform(image).to(device)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    output = model([image])[0]\n",
    "\n",
    "# Print output keys\n",
    "print(output.keys())\n",
    "\n",
    "# Output includes 'boxes', 'labels', 'scores', 'masks'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing the Results**\n",
    "\n",
    "Although we cannot include images here, in practice, you can use libraries like `matplotlib` or `cv2` to overlay masks and bounding boxes on the original image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "# 5. Latest Developments\n",
    "\n",
    "<a id=\"5.1\"></a>\n",
    "## 5.1 EfficientDet\n",
    "\n",
    "EfficientDet [[4]](#ref4) is a family of object detection models that achieve state-of-the-art accuracy while being computationally efficient. It introduces:\n",
    "\n",
    "- **EfficientNet Backbones**: Scalable and efficient CNN architectures.\n",
    "- **BiFPN (Bi-directional Feature Pyramid Network)**: Enhances feature fusion at different scales.\n",
    "- **Compound Scaling**: Simultaneously scales depth, width, and resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.2\"></a>\n",
    "## 5.2 DETR (Detection Transformer)\n",
    "\n",
    "DETR [[5]](#ref5) formulates object detection as a direct set prediction problem using transformers.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Set-based Loss**: Uses bipartite matching loss to ensure unique predictions.\n",
    "- **Transformers**: Capture global context with self-attention mechanisms.\n",
    "- **Simplified Pipeline**: Eliminates the need for hand-designed components like NMS (Non-Maximum Suppression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Formulation\n",
    "\n",
    "DETR predicts a fixed-size set of bounding boxes and class labels by minimizing the following loss:\n",
    "\n",
    "$[\n",
    "L_{\\text{DETR}} = \\sum_{i=1}^{N} \\left[ L_{\\text{cls}}(c_i, \\hat{c}_{\\sigma(i)}) + \\mathbb{1}_{\\{\\hat{c}_{\\sigma(i)} \\neq \\varnothing\\}} L_{\\text{box}}(b_i, \\hat{b}_{\\sigma(i)}) \\right]\n",
    "]$\n",
    "\n",
    "- $( N )$: Number of objects.\n",
    "- $( c_i )$, $( b_i )$: Ground truth class label and bounding box.\n",
    "- $( \\hat{c}_{\\sigma(i)} )$, $( \\hat{b}_{\\sigma(i)} )$: Predicted class label and bounding box after optimal assignment $( \\sigma )$.\n",
    "- $( L_{\\text{cls}} )$: Classification loss.\n",
    "- $( L_{\\text{box}} )$: Bounding box loss (e.g., $( L_1 )$ loss and generalized IoU loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "# 6. Conclusion\n",
    "\n",
    "Advanced object detection and segmentation techniques like Faster R-CNN, YOLO, and Mask R-CNN have transformed computer vision, enabling accurate and efficient detection and segmentation of objects in images. Understanding the underlying mathematics and implementation details of these models is crucial for applying them effectively in real-world applications. The field continues to evolve with innovative models like EfficientDet and DETR pushing the boundaries of what's possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "# 7. References\n",
    "\n",
    "1. <a id=\"ref1\"></a>Ren, S., He, K., Girshick, R., & Sun, J. (2015). *Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks*. [arXiv:1506.01497](https://arxiv.org/abs/1506.01497)\n",
    "2. <a id=\"ref2\"></a>Redmon, J., Divvala, S., Girshick, R., & Farhadi, A. (2016). *You Only Look Once: Unified, Real-Time Object Detection*. [arXiv:1506.02640](https://arxiv.org/abs/1506.02640)\n",
    "3. <a id=\"ref3\"></a>He, K., Gkioxari, G., Dollár, P., & Girshick, R. (2017). *Mask R-CNN*. [arXiv:1703.06870](https://arxiv.org/abs/1703.06870)\n",
    "4. <a id=\"ref4\"></a>Tan, M., Pang, R., & Le, Q. V. (2020). *EfficientDet: Scalable and Efficient Object Detection*. [arXiv:1911.09070](https://arxiv.org/abs/1911.09070)\n",
    "5. <a id=\"ref5\"></a>Carion, N., et al. (2020). *End-to-End Object Detection with Transformers*. [arXiv:2005.12872](https://arxiv.org/abs/2005.12872)\n",
    "\n",
    "---\n",
    "\n",
    "This notebook provides an in-depth exploration of advanced computer vision techniques for object detection and segmentation. You can run the code cells to see how these models are implemented and experiment with different architectures and datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
