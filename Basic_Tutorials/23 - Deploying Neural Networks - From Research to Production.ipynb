{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying Neural Networks: From Research to Production\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In recent years, neural networks have achieved remarkable success in various domains such as computer vision, natural language processing, and speech recognition. However, building a model is only half the battle. Deploying trained models into production environments where they can serve real users is a critical step in the machine learning lifecycle.\n",
    "\n",
    "This tutorial aims to guide you through the process of deploying neural networks from research to production. We will explore different deployment strategies, including TensorFlow Serving, building APIs with Flask, and deploying models using cloud services. We'll also discuss considerations for scalability and efficiency to ensure that your deployed models can handle production workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Understanding Model Deployment](#1)\n",
    "   - [Importance of Deployment](#1.1)\n",
    "   - [Challenges in Deployment](#1.2)\n",
    "2. [Considerations for Scalability and Efficiency](#2)\n",
    "   - [Scalability Factors](#2.1)\n",
    "   - [Efficiency Considerations](#2.2)\n",
    "3. [Deploying with TensorFlow Serving](#3)\n",
    "   - [Overview of TensorFlow Serving](#3.1)\n",
    "   - [Saving a Model for Serving](#3.2)\n",
    "   - [Serving the Model](#3.3)\n",
    "   - [Making Predictions](#3.4)\n",
    "4. [Building APIs with Flask](#4)\n",
    "   - [Overview of Flask for APIs](#4.1)\n",
    "   - [Creating a REST API for the Model](#4.2)\n",
    "   - [Testing the API](#4.3)\n",
    "5. [Deploying on Cloud Services](#5)\n",
    "   - [Overview of Cloud Deployment Options](#5.1)\n",
    "   - [Deploying with AWS SageMaker](#5.2)\n",
    "   - [Deploying with Google Cloud AI Platform](#5.3)\n",
    "   - [Deploying with Azure Machine Learning](#5.4)\n",
    "6. [Latest Developments in Model Deployment](#6)\n",
    "   - [Kubernetes and Container Orchestration](#6.1)\n",
    "   - [Model Serving Platforms](#6.2)\n",
    "   - [MLOps and Continuous Deployment](#6.3)\n",
    "7. [Conclusion](#7)\n",
    "8. [References](#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# 1. Understanding Model Deployment\n",
    "\n",
    "<a id=\"1.1\"></a>\n",
    "## 1.1 Importance of Deployment\n",
    "\n",
    "Deploying machine learning models is a crucial step in delivering AI-powered solutions to end-users. Without deployment, models remain as prototypes or proof-of-concepts. Deployment enables:\n",
    "\n",
    "- **Real-Time Predictions**: Serving models in production allows for real-time inference, providing immediate value to users.\n",
    "- **Scalability**: Deployed models can be scaled to handle large volumes of requests.\n",
    "- **Integration**: Models can be integrated into existing systems, applications, or services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.2\"></a>\n",
    "## 1.2 Challenges in Deployment\n",
    "\n",
    "Deploying models comes with several challenges:\n",
    "\n",
    "- **Scalability**: Ensuring the model can handle high traffic and large volumes of data.\n",
    "- **Latency**: Minimizing response time for real-time applications.\n",
    "- **Resource Utilization**: Efficiently using computational resources (CPU, GPU, memory).\n",
    "- **Model Versioning**: Managing multiple versions of models.\n",
    "- **Security**: Protecting the model and data from unauthorized access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "# 2. Considerations for Scalability and Efficiency\n",
    "\n",
    "<a id=\"2.1\"></a>\n",
    "## 2.1 Scalability Factors\n",
    "\n",
    "Scalability refers to the ability of a system to handle increasing loads by adding resources. Factors affecting scalability include:\n",
    "\n",
    "- **Horizontal Scaling**: Adding more instances of the service.\n",
    "- **Vertical Scaling**: Increasing the resources of existing instances.\n",
    "- **Load Balancing**: Distributing incoming requests across multiple instances.\n",
    "- **Caching**: Storing frequently accessed data to reduce computation.\n",
    "\n",
    "Mathematically, scalability can be assessed by measuring the throughput (requests per second) as a function of resources:\n",
    "\n",
    "$[\n",
    "\\text{Throughput} = f(\\text{Number of Instances}, \\text{Instance Capacity})\n",
    "]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.2\"></a>\n",
    "## 2.2 Efficiency Considerations\n",
    "\n",
    "Efficiency involves optimizing resource utilization while maintaining performance. Considerations include:\n",
    "\n",
    "- **Batching Requests**: Processing multiple inputs together to improve computational efficiency.\n",
    "- **Model Optimization**: Techniques like quantization, pruning, or distillation to reduce model size and inference time.\n",
    "- **Asynchronous Processing**: Handling requests asynchronously to improve throughput."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "# 3. Deploying with TensorFlow Serving\n",
    "\n",
    "<a id=\"3.1\"></a>\n",
    "## 3.1 Overview of TensorFlow Serving\n",
    "\n",
    "TensorFlow Serving is a flexible, high-performance serving system for machine learning models, designed for production environments. It provides:\n",
    "\n",
    "- **Out-of-the-Box Integration**: Easily deploy TensorFlow models without extensive changes.\n",
    "- **High Performance**: Optimized for low latency and high throughput.\n",
    "- **Dynamic Model Management**: Supports model versioning and hot-swapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.2\"></a>\n",
    "## 3.2 Saving a Model for Serving\n",
    "\n",
    "To serve a model, we first need to save it in the TensorFlow SavedModel format.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Suppose we have trained a simple neural network on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load and preprocess data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train[..., tf.newaxis]/255.0\n",
    "x_test = x_test[..., tf.newaxis]/255.0\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28,28,1)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=1)\n",
    "\n",
    "# Save the model in SavedModel format\n",
    "MODEL_DIR = 'saved_model/1'\n",
    "tf.saved_model.save(model, MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- We save the model under `saved_model/1`, where `1` is the version number.\n",
    "- The `tf.saved_model.save` function exports the model in a format compatible with TensorFlow Serving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.3\"></a>\n",
    "## 3.3 Serving the Model\n",
    "\n",
    "To serve the model, we need to install TensorFlow Serving and start the server.\n",
    "\n",
    "**Installation:**\n",
    "\n",
    "- For Linux, you can install TensorFlow Serving using `apt-get`:\n",
    "\n",
    "```bash\n",
    "# Install TensorFlow Serving\n",
    "sudo apt-get update && sudo apt-get install tensorflow-model-server\n",
    "```\n",
    "\n",
    "**Starting the Server:**\n",
    "\n",
    "```bash\n",
    "tensorflow_model_server \\\n",
    "  --rest_api_port=8501 \\\n",
    "  --model_name=mnist \\\n",
    "  --model_base_path=\"$(pwd)/saved_model\" &\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `--rest_api_port=8501`: Specifies the port for the REST API.\n",
    "- `--model_name=mnist`: The name of the model.\n",
    "- `--model_base_path`: The path where the model is saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.4\"></a>\n",
    "## 3.4 Making Predictions\n",
    "\n",
    "We can send requests to the server using `curl` or any HTTP client.\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the REST API\n",
    "import json\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "# Prepare the data\n",
    "data = json.dumps({\n",
    "    \"signature_name\": \"serving_default\",\n",
    "    \"instances\": x_test[0:3].tolist()\n",
    "})\n",
    "\n",
    "# Send the request\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "json_response = requests.post('http://localhost:8501/v1/models/mnist:predict', data=data, headers=headers)\n",
    "predictions = json.loads(json_response.text)['predictions']\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- We serialize the test images and send them in a JSON payload.\n",
    "- The server returns the predictions, which we can process as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "# 4. Building APIs with Flask\n",
    "\n",
    "<a id=\"4.1\"></a>\n",
    "## 4.1 Overview of Flask for APIs\n",
    "\n",
    "Flask is a lightweight web framework for Python, ideal for building web applications and APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.2\"></a>\n",
    "## 4.2 Creating a REST API for the Model\n",
    "\n",
    "We can create an API endpoint that accepts input data, feeds it to the model, and returns predictions.\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Flask API\n",
    "from flask import Flask, request, jsonify\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model('saved_model/1')\n",
    "\n",
    "# Create the Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    json_data = request.get_json()\n",
    "    instances = json_data['instances']\n",
    "    inputs = tf.convert_to_tensor(instances)\n",
    "    predictions = model(inputs)\n",
    "    predictions = tf.argmax(predictions, axis=1)\n",
    "    return jsonify({'predictions': predictions.numpy().tolist()})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- We define a `/predict` endpoint that accepts POST requests.\n",
    "- The model makes predictions on the input data and returns the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.3\"></a>\n",
    "## 4.3 Testing the API\n",
    "\n",
    "We can test the API using `curl` or a Python script.\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Flask API\n",
    "import requests\n",
    "import json\n",
    "\n",
    "data = json.dumps({\n",
    "    \"instances\": x_test[0:3].tolist()\n",
    "})\n",
    "\n",
    "response = requests.post('http://localhost:5000/predict', data=data, headers={\"Content-Type\": \"application/json\"})\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "# 5. Deploying on Cloud Services\n",
    "\n",
    "<a id=\"5.1\"></a>\n",
    "## 5.1 Overview of Cloud Deployment Options\n",
    "\n",
    "Cloud services provide scalable infrastructure and managed services for deploying machine learning models.\n",
    "\n",
    "- **AWS SageMaker**\n",
    "- **Google Cloud AI Platform**\n",
    "- **Azure Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.2\"></a>\n",
    "## 5.2 Deploying with AWS SageMaker\n",
    "\n",
    "AWS SageMaker is a fully managed service that covers the entire machine learning workflow.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Upload the Model to S3:**\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "s3.upload_file('model.tar.gz', 'my-bucket', 'model/model.tar.gz')\n",
    "```\n",
    "\n",
    "2. **Create a SageMaker Model:**\n",
    "\n",
    "```python\n",
    "import sagemaker\n",
    "\n",
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "\n",
    "model = TensorFlowModel(model_data='s3://my-bucket/model/model.tar.gz',\n",
    "                        role='AWS_SageMaker_Role',\n",
    "                        framework_version='2.3',\n",
    "                        entry_point='inference.py')\n",
    "\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.m5.large')\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `inference.py` contains the code defining how the model handles inference requests.\n",
    "- SageMaker handles provisioning of infrastructure and deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.3\"></a>\n",
    "## 5.3 Deploying with Google Cloud AI Platform\n",
    "\n",
    "Google Cloud AI Platform allows you to train and serve models at scale.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Upload the Model to Google Cloud Storage:**\n",
    "\n",
    "```bash\n",
    "gsutil cp -r saved_model/ gs://my-bucket/models/mnist/\n",
    "```\n",
    "\n",
    "2. **Create a Model and Version:**\n",
    "\n",
    "```bash\n",
    "gcloud ai-platform models create mnist_model\n",
    "\n",
    "gcloud ai-platform versions create v1 \\\n",
    "    --model=mnist_model \\\n",
    "    --origin=gs://my-bucket/models/mnist/ \\\n",
    "    --runtime-version=2.3 \\\n",
    "    --framework=TENSORFLOW \\\n",
    "    --python-version=3.7\n",
    "```\n",
    "\n",
    "3. **Make Predictions:**\n",
    "\n",
    "```bash\n",
    "gcloud ai-platform predict \\\n",
    "    --model mnist_model \\\n",
    "    --version v1 \\\n",
    "    --json-instances input.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.4\"></a>\n",
    "## 5.4 Deploying with Azure Machine Learning\n",
    "\n",
    "Azure Machine Learning provides tools for deploying models as web services.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Register the Model:**\n",
    "\n",
    "```python\n",
    "from azureml.core import Workspace, Model\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "model = Model.register(ws, model_path='saved_model/', model_name='mnist_model')\n",
    "```\n",
    "\n",
    "2. **Create Inference Configuration:**\n",
    "\n",
    "```python\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "inference_config = InferenceConfig(entry_script='score.py',\n",
    "                                   environment=myenv)\n",
    "```\n",
    "\n",
    "3. **Deploy the Model:**\n",
    "\n",
    "```python\n",
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name='mnist-service',\n",
    "                       models=[model],\n",
    "                       inference_config=inference_config,\n",
    "                       deployment_config=deployment_config)\n",
    "service.wait_for_deployment(show_output=True)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `score.py` contains the code to handle inference requests.\n",
    "- `myenv` is an environment that specifies dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "# 6. Latest Developments in Model Deployment\n",
    "\n",
    "<a id=\"6.1\"></a>\n",
    "## 6.1 Kubernetes and Container Orchestration\n",
    "\n",
    "Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.\n",
    "\n",
    "- **Benefits:**\n",
    "  - Scalability\n",
    "  - High availability\n",
    "  - Rolling updates\n",
    "- **Model Serving with Kubernetes:**\n",
    "  - Use containers (e.g., Docker) to package the model.\n",
    "  - Deploy containers using Kubernetes clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6.2\"></a>\n",
    "## 6.2 Model Serving Platforms\n",
    "\n",
    "Platforms like **KServe** (formerly KFServing) and **Seldon Core** provide Kubernetes-based model serving.\n",
    "\n",
    "- **Features:**\n",
    "  - Advanced routing and traffic splitting.\n",
    "  - Canary deployments.\n",
    "  - Support for multiple frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6.3\"></a>\n",
    "## 6.3 MLOps and Continuous Deployment\n",
    "\n",
    "MLOps refers to practices that bring continuous integration and deployment to machine learning.\n",
    "\n",
    "- **Continuous Integration (CI):** Automated building and testing of code changes.\n",
    "- **Continuous Deployment (CD):** Automated deployment of code changes to production.\n",
    "- **Tools:**\n",
    "  - **MLFlow**\n",
    "  - **TensorFlow Extended (TFX)**\n",
    "  - **Kubeflow**\n",
    "\n",
    "**Reference:**\n",
    "\n",
    "- Sculley, D., et al. (2015). Hidden Technical Debt in Machine Learning Systems. *Advances in Neural Information Processing Systems*, 28."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "# 7. Conclusion\n",
    "\n",
    "Deploying neural networks from research to production involves several steps and considerations. Understanding the different deployment options, scalability factors, and efficiency considerations is crucial for delivering robust and performant AI solutions. Whether you choose TensorFlow Serving, build APIs with Flask, or leverage cloud services, the key is to select the approach that best fits your application's needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>\n",
    "# 8. References\n",
    "\n",
    "1. **TensorFlow Serving:** TensorFlow Serving: Flexible, High-Performance ML Serving. [Link](https://www.tensorflow.org/tfx/guide/serving)\n",
    "2. **Flask:** Flask Official Documentation. [Link](https://flask.palletsprojects.com/)\n",
    "3. **AWS SageMaker:** Amazon SageMaker Developer Guide. [Link](https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html)\n",
    "4. **Google Cloud AI Platform:** AI Platform Documentation. [Link](https://cloud.google.com/ai-platform)\n",
    "5. **Azure Machine Learning:** Azure Machine Learning Documentation. [Link](https://docs.microsoft.com/en-us/azure/machine-learning/)\n",
    "6. **Kubernetes:** Kubernetes Documentation. [Link](https://kubernetes.io/docs/home/)\n",
    "7. **MLOps:** Sculley, D., et al. (2015). Hidden Technical Debt in Machine Learning Systems. *Advances in Neural Information Processing Systems*, 28.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook provides a comprehensive guide to deploying neural networks from research to production. You can run the code cells to see how models are saved, served, and integrated into applications. Feel free to modify and extend the examples to suit your specific deployment needs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
