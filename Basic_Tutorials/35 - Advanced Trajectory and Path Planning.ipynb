{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Trajectory and Path Planning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Trajectory and path planning are critical components in robotics, autonomous vehicles, and animation. They involve finding a feasible path from a starting point to a goal while satisfying constraints such as obstacle avoidance, kinematics, and dynamics. Advanced methods have been developed to address the complexities of real-world environments.\n",
    "\n",
    "In this tutorial, we'll delve into advanced methods of trajectory and path planning, exploring the underlying mathematics, implementing example code, and explaining the processes. We'll reference key papers and discuss the latest developments in the field. Relevant imagery generated through code will be included to enhance understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Overview of Path and Trajectory Planning](#1)\n",
    "2. [Sampling-Based Methods](#2)\n",
    "   - [Rapidly-Exploring Random Trees (RRT)](#2.1)\n",
    "   - [RRT* Algorithm](#2.2)\n",
    "   - [Implementation of RRT and RRT*](#2.3)\n",
    "3. [Optimization-Based Methods](#3)\n",
    "   - [Trajectory Optimization](#3.1)\n",
    "   - [Sequential Quadratic Programming (SQP)](#3.2)\n",
    "   - [Implementation of Trajectory Optimization](#3.3)\n",
    "4. [Kinodynamic Planning](#4)\n",
    "   - [Differential Constraints](#4.1)\n",
    "   - [Implementation with RRT](#4.2)\n",
    "5. [Model Predictive Control (MPC)](#5)\n",
    "   - [Formulation](#5.1)\n",
    "   - [Implementation of MPC](#5.2)\n",
    "6. [Latest Developments](#6)\n",
    "   - [Machine Learning in Path Planning](#6.1)\n",
    "   - [Reinforcement Learning Approaches](#6.2)\n",
    "7. [Conclusion](#7)\n",
    "8. [References](#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# 1. Overview of Path and Trajectory Planning\n",
    "\n",
    "Path planning involves finding a collision-free path from a start point to a goal point in an environment with obstacles. Trajectory planning extends this by considering time, ensuring the path is feasible with respect to the dynamics and kinematics of the system.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Configuration Space (C-space)**: The space of all possible positions and orientations of the robot.\n",
    "- **State Space**: Includes both the configuration and its derivatives (e.g., velocities).\n",
    "- **Obstacle Avoidance**: Ensuring the path does not intersect with obstacles in the environment.\n",
    "- **Optimality**: Finding the shortest or most efficient path according to a defined cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "# 2. Sampling-Based Methods\n",
    "\n",
    "Sampling-based algorithms are widely used in path planning due to their ability to handle high-dimensional spaces and complex environments without explicitly constructing the configuration space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.1\"></a>\n",
    "## 2.1 Rapidly-Exploring Random Trees (RRT)\n",
    "\n",
    "The RRT algorithm [[1]](#ref1) incrementally builds a space-filling tree by randomly sampling the configuration space.\n",
    "\n",
    "### Algorithm Steps\n",
    "\n",
    "1. **Initialization**: Start with an initial node at the starting position.\n",
    "2. **Sampling**: Randomly sample a point $( q_{\text{rand}} )$ in the configuration space.\n",
    "3. **Nearest Neighbor**: Find the nearest node $( q_{\text{near}} )$ in the tree to $( q_{\text{rand}} )$.\n",
    "4. **Steer**: Move from $( q_{\text{near}} )$ towards $( q_{\text{rand}} )$ by a fixed step size $( \\epsilon )$, obtaining $( q_{\\text{new}} )$.\n",
    "5. **Collision Check**: If the path from $( q_{\\text{near}} )$ to $( q_{\\text{new}} )$ is obstacle-free, add $( q_{\\text{new}} )$ to the tree.\n",
    "6. **Goal Check**: If $( q_{\\text{new}} )$ is close to the goal, terminate.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "- **Nearest Neighbor**:\n",
    "\n",
    "  $[\n",
    "  q_{\\text{near}} = \\operatorname{argmin}_{q \\in T} \\| q - q_{\\text{rand}} |\n",
    "  ]$\n",
    "\n",
    "- **Steering Function**:\n",
    "\n",
    "  $[\n",
    "  q_{\text{new}} = q_{\\text{near}} + \\epsilon \\cdot \\frac{q_{\text{rand}} - q_{\\text{near}}}{| q_{\\text{rand}} - q_{\\text{near}} |}\n",
    "  ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.2\"></a>\n",
    "## 2.2 RRT* Algorithm\n",
    "\n",
    "RRT* [[2]](#ref2) is an extension of RRT that provides asymptotic optimality, meaning it converges to the optimal path over time.\n",
    "\n",
    "### Modifications from RRT\n",
    "\n",
    "- **Rewiring**: After adding $( q_{\\text{new}} )$, check nearby nodes to see if they can be reached more efficiently via $( q_{\\text{new}} )$.\n",
    "- **Cost Function**: Maintain the cost-to-come for each node.\n",
    "\n",
    "### Algorithm Steps\n",
    "\n",
    "1. **Same as RRT Steps 1-5**.\n",
    "2. **Near Nodes**: Find all nodes within a radius $( r )$ of $( q_{\\text{new}} )$.\n",
    "3. **Choose Parent**: Select the node $( q_{\\text{min}} )$ that minimizes the cost to $( q_{\\text{new}} )$.\n",
    "4. **Rewire**: For each node in the near set, see if reaching it via $( q_{\\text{new}} )$ reduces the cost.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "- **Cost Function**:\n",
    "\n",
    "  $[\n",
    "  c(q) = c(q_{\\text{parent}}) + \\text{Cost}(q_{\\text{parent}}, q)\n",
    "  ]$\n",
    "\n",
    "- **Near Nodes Radius**:\n",
    "\n",
    "  $[\n",
    "  r = \\min\\left\\{ \\gamma \\left( \\frac{\\log n}{n} \\right)^{1/d}, \\eta \\right\\}\n",
    "  ]$\n",
    "\n",
    "  Where:\n",
    "\n",
    "  - $( n )$: Number of nodes in the tree.\n",
    "  - $( d )$: Dimension of the space.\n",
    "  - $( \\gamma )$: A constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.3\"></a>\n",
    "## 2.3 Implementation of RRT and RRT*\n",
    "\n",
    "We'll implement RRT and RRT* algorithms in a 2D environment with obstacles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Node class\n",
    "class Node:\n",
    "    def __init__(self, position):\n",
    "        self.position = position\n",
    "        self.parent = None\n",
    "        self.cost = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RRT algorithm\n",
    "import random\n",
    "\n",
    "class RRT:\n",
    "    def __init__(self, start, goal, obstacles, x_limit, y_limit, max_iter=500, step_size=1.0):\n",
    "        self.start = Node(start)\n",
    "        self.goal = Node(goal)\n",
    "        self.obstacles = obstacles\n",
    "        self.x_limit = x_limit\n",
    "        self.y_limit = y_limit\n",
    "        self.max_iter = max_iter\n",
    "        self.step_size = step_size\n",
    "        self.nodes = [self.start]\n",
    "    \n",
    "    def get_random_point(self):\n",
    "        x = random.uniform(self.x_limit[0], self.x_limit[1])\n",
    "        y = random.uniform(self.y_limit[0], self.y_limit[1])\n",
    "        return np.array([x, y])\n",
    "    \n",
    "    def nearest_neighbor(self, q_rand):\n",
    "        distances = [np.linalg.norm(q_rand - node.position) for node in self.nodes]\n",
    "        min_index = distances.index(min(distances))\n",
    "        return self.nodes[min_index]\n",
    "    \n",
    "    def steer(self, q_near, q_rand):\n",
    "        direction = q_rand - q_near.position\n",
    "        length = np.linalg.norm(direction)\n",
    "        if length > self.step_size:\n",
    "            direction = direction / length * self.step_size\n",
    "        q_new = q_near.position + direction\n",
    "        return q_new\n",
    "    \n",
    "    def collision_free(self, q_near, q_new):\n",
    "        for obs in self.obstacles:\n",
    "            if self.check_collision(q_near.position, q_new, obs):\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def check_collision(self, p1, p2, obs):\n",
    "        # Simple rectangle collision checking\n",
    "        x_min, y_min, width, height = obs\n",
    "        x_max = x_min + width\n",
    "        y_max = y_min + height\n",
    "        \n",
    "        # Check if line segment intersects with rectangle\n",
    "        # For simplicity, we'll discretize the line segment\n",
    "        num_steps = int(np.linalg.norm(p2 - p1) / 0.1)\n",
    "        x_vals = np.linspace(p1[0], p2[0], num_steps)\n",
    "        y_vals = np.linspace(p1[1], p2[1], num_steps)\n",
    "        for x, y in zip(x_vals, y_vals):\n",
    "            if x_min <= x <= x_max and y_min <= y <= y_max:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def build_rrt(self):\n",
    "        for i in range(self.max_iter):\n",
    "            q_rand = self.get_random_point()\n",
    "            q_near = self.nearest_neighbor(q_rand)\n",
    "            q_new = self.steer(q_near, q_rand)\n",
    "            new_node = Node(q_new)\n",
    "            if self.collision_free(q_near, new_node.position):\n",
    "                new_node.parent = q_near\n",
    "                self.nodes.append(new_node)\n",
    "                if np.linalg.norm(new_node.position - self.goal.position) < self.step_size:\n",
    "                    print(\"Goal reached!\")\n",
    "                    return self.generate_path(new_node)\n",
    "        print(\"Goal not reached within max iterations.\")\n",
    "        return None\n",
    "    \n",
    "    def generate_path(self, node):\n",
    "        path = [node.position]\n",
    "        while node.parent is not None:\n",
    "            node = node.parent\n",
    "            path.append(node.position)\n",
    "        return path[::-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment\n",
    "start = np.array([0, 0])\n",
    "goal = np.array([10, 10])\n",
    "obstacles = [\n",
    "    (3, 3, 2, 6),  # (x_min, y_min, width, height)\n",
    "    (6, 0, 2, 6)\n",
    "]\n",
    "x_limit = (0, 15)\n",
    "y_limit = (0, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the RRT algorithm\n",
    "rrt = RRT(start, goal, obstacles, x_limit, y_limit, max_iter=1000, step_size=0.5)\n",
    "path = rrt.build_rrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the result\n",
    "def plot_rrt(rrt, path):\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    \n",
    "    # Plot obstacles\n",
    "    for obs in rrt.obstacles:\n",
    "        rect = Rectangle((obs[0], obs[1]), obs[2], obs[3], color='gray')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    # Plot nodes and edges\n",
    "    for node in rrt.nodes:\n",
    "        if node.parent is not None:\n",
    "            x_vals = [node.position[0], node.parent.position[0]]\n",
    "            y_vals = [node.position[1], node.parent.position[1]]\n",
    "            ax.plot(x_vals, y_vals, color='blue')\n",
    "    \n",
    "    # Plot path\n",
    "    if path is not None:\n",
    "        path = np.array(path)\n",
    "        ax.plot(path[:,0], path[:,1], color='red', linewidth=2)\n",
    "    \n",
    "    # Plot start and goal\n",
    "    ax.plot(rrt.start.position[0], rrt.start.position[1], 'go', markersize=10)\n",
    "    ax.plot(rrt.goal.position[0], rrt.goal.position[1], 'ro', markersize=10)\n",
    "    \n",
    "    ax.set_xlim(rrt.x_limit)\n",
    "    ax.set_ylim(rrt.y_limit)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_title('RRT Path Planning')\n",
    "    plt.show()\n",
    "\n",
    "plot_rrt(rrt, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Node Class**: Represents a point in the space with position, parent, and cost.\n",
    "- **RRT Class**: Implements the RRT algorithm with methods for sampling, nearest neighbor search, steering, collision checking, and building the tree.\n",
    "- **Visualization**: Uses matplotlib to display the obstacles, tree, and final path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Implementing RRT* involves additional steps for rewiring. Due to complexity, the full implementation is beyond this example's scope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "# 3. Optimization-Based Methods\n",
    "\n",
    "Optimization-based methods formulate the path planning problem as an optimization problem, minimizing a cost function subject to constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.1\"></a>\n",
    "## 3.1 Trajectory Optimization\n",
    "\n",
    "Trajectory optimization involves finding a trajectory that minimizes a cost function while satisfying dynamics and constraints.\n",
    "\n",
    "### Problem Formulation\n",
    "\n",
    "Minimize the cost function:\n",
    "\n",
    "$[\n",
    "J = \\int_{t_0}^{t_f} L(x(t), u(t), t) dt + \\phi(x(t_f))\n",
    "]$\n",
    "\n",
    "Subject to:\n",
    "\n",
    "- **Dynamics**:\n",
    "\n",
    "  $[\n",
    "  \\dot{x}(t) = f(x(t), u(t), t)\n",
    "  ]$\n",
    "\n",
    "- **Constraints**:\n",
    "\n",
    "  $[\n",
    "  h(x(t), u(t), t) \\leq 0\n",
    "  ]$\n",
    "\n",
    "### Methods\n",
    "\n",
    "- **Direct Methods**: Discretize the trajectory and solve the optimization problem.\n",
    "- **Indirect Methods**: Use calculus of variations and solve the resulting boundary value problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.2\"></a>\n",
    "## 3.2 Sequential Quadratic Programming (SQP)\n",
    "\n",
    "SQP is an iterative method for nonlinear optimization problems.\n",
    "\n",
    "### Algorithm Steps\n",
    "\n",
    "1. **Initialization**: Start with an initial guess.\n",
    "2. **Quadratic Approximation**: At each iteration, approximate the problem by a quadratic programming (QP) subproblem.\n",
    "3. **Solve QP Subproblem**: Solve the QP to find a search direction.\n",
    "4. **Line Search**: Perform a line search to determine the step size.\n",
    "5. **Update**: Update the variables and repeat.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "At iteration $( k )$, solve the QP:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\\min_{\\Delta x} \\quad & \\frac{1}{2} \\Delta x^T \\nabla^2 L(x_k, \\lambda_k) \\Delta x + \\nabla f(x_k)^T\\Delta x \n",
    "\\\\\n",
    "\\\\\n",
    "\\text{ subject to} \\quad & c(x_k) + \\nabla c(x_k)^T \\Delta x = 0\\end{align*}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $( L )$: Lagrangian.\n",
    "- $( \\lambda_k )$: Lagrange multipliers.\n",
    "- $( c(x) )$: Constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.3\"></a>\n",
    "## 3.3 Implementation of Trajectory Optimization\n",
    "\n",
    "We'll implement a simple trajectory optimization using a direct collocation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from scipy.optimize import minimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dynamics of a double integrator\n",
    "\n",
    "def dynamics(x, u):\n",
    "    dxdt = np.array([x[1], u])\n",
    "    return dxdt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cost function\n",
    "\n",
    "def cost(z, N, dt):\n",
    "    cost = 0\n",
    "    for i in range(N):\n",
    "        u = z[N*2 + i]\n",
    "        cost += u**2 * dt  # Minimize control effort\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constraints\n",
    "\n",
    "def constraints(z, x0, xf, N, dt):\n",
    "    cons = []\n",
    "    for i in range(N):\n",
    "        x_i = z[2*i:2*(i+1)]\n",
    "        x_next = z[2*(i+1):2*(i+2)] if i < N-1 else xf\n",
    "        u_i = z[N*2 + i]\n",
    "        \n",
    "        # Discretize dynamics using Euler method\n",
    "        x_next_est = x_i + dynamics(x_i, u_i) * dt\n",
    "        \n",
    "        cons.extend(x_next - x_next_est)\n",
    "    \n",
    "    # Initial condition constraint\n",
    "    cons_init = z[0:2] - x0\n",
    "    cons.extend(cons_init)\n",
    "    return cons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the optimization problem\n",
    "N = 50  # Number of time steps\n",
    "dt = 0.1  # Time step\n",
    "x0 = np.array([0, 0])  # Initial position and velocity\n",
    "xf = np.array([10, 0])  # Final position and velocity\n",
    "\n",
    "# Initial guess\n",
    "z0 = np.zeros(N*2 + N)  # [x0, v0, x1, v1, ..., u0, u1, ...]\n",
    "\n",
    "# Bounds on variables\n",
    "bounds = []\n",
    "for _ in range(N):\n",
    "    bounds.extend([(-np.inf, np.inf), (-np.inf, np.inf)])  # x and v bounds\n",
    "for _ in range(N):\n",
    "    bounds.append((-1, 1))  # Control input bounds\n",
    "\n",
    "# Define constraint function\n",
    "cons = ({'type': 'eq', 'fun': lambda z: constraints(z, x0, xf, N, dt)})\n",
    "\n",
    "# Perform optimization\n",
    "res = minimize(lambda z: cost(z, N, dt), z0, bounds=bounds, constraints=cons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract solution\n",
    "z_opt = res.x\n",
    "x_opt = z_opt[:N*2].reshape(N, 2)\n",
    "u_opt = z_opt[N*2:]\n",
    "t = np.linspace(0, N*dt, N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(t, x_opt[:,0], label='Position')\n",
    "plt.plot(t, x_opt[:,1], label='Velocity')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('State')\n",
    "plt.legend()\n",
    "plt.title('Optimal Trajectory')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.step(t, u_opt, where='post')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Control Input')\n",
    "plt.title('Optimal Control Input')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Dynamics**: We model a double integrator system (e.g., a simple car moving in one dimension).\n",
    "- **Cost Function**: Minimize the control effort over the trajectory.\n",
    "- **Constraints**: Enforce the dynamics and boundary conditions.\n",
    "- **Optimization**: Use `scipy.optimize.minimize` to solve the constrained optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "# 4. Kinodynamic Planning\n",
    "\n",
    "Kinodynamic planning involves planning in both the configuration space and the velocity space, considering the system's dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.1\"></a>\n",
    "## 4.1 Differential Constraints\n",
    "\n",
    "The planner must account for the system's differential constraints, i.e., its dynamics and kinematics.\n",
    "\n",
    "### Examples\n",
    "\n",
    "- **Car-Like Robots**: Nonholonomic constraints prevent certain motions.\n",
    "- **Quadrotors**: Must consider acceleration and velocity limits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.2\"></a>\n",
    "## 4.2 Implementation with RRT\n",
    "\n",
    "Kinodynamic RRT extends RRT by incorporating the system's dynamics.\n",
    "\n",
    "### Modifications\n",
    "\n",
    "- **State Space**: Include both position and velocity.\n",
    "- **Steering Function**: Use control inputs to simulate the system's dynamics over time.\n",
    "- **Edge Costs**: Can include time or control effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Implementing a full kinodynamic RRT is complex and beyond the scope of this tutorial. Specialized libraries like OMPL (Open Motion Planning Library) provide implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "# 5. Model Predictive Control (MPC)\n",
    "\n",
    "MPC is an optimization-based control strategy that solves an optimization problem at each time step, considering a finite prediction horizon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.1\"></a>\n",
    "## 5.1 Formulation\n",
    "\n",
    "At each time step, MPC solves:\n",
    "\n",
    "$[\n",
    "\\min_{u_0, ..., u_{N-1}} \\sum_{k=0}^{N-1} L(x_k, u_k) + L_f(x_N)\n",
    "]$\n",
    "\n",
    "Subject to:\n",
    "\n",
    "- **Dynamics**: $( x_{k+1} = f(x_k, u_k) )$\n",
    "- **Constraints**: $( h(x_k, u_k) \\leq 0 )$\n",
    "- **Initial Condition**: $( x_0 = x(t) )$\n",
    "\n",
    "Only the first control input $( u_0 )$ is applied before the optimization is repeated at the next time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.2\"></a>\n",
    "## 5.2 Implementation of MPC\n",
    "\n",
    "We'll implement a simple MPC controller for the double integrator system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPC parameters\n",
    "N = 10  # Prediction horizon\n",
    "dt = 0.1\n",
    "x_target = np.array([10, 0])\n",
    "\n",
    "# MPC cost function\n",
    "def mpc_cost(u_seq, x_current):\n",
    "    cost = 0\n",
    "    x = x_current.copy()\n",
    "    u_seq = u_seq.reshape(N, 1)\n",
    "    for i in range(N):\n",
    "        u = u_seq[i]\n",
    "        x = x + dynamics(x, u) * dt\n",
    "        cost += np.sum((x - x_target)**2) + u**2\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation parameters\n",
    "x_current = np.array([0, 0])\n",
    "T = 20  # Total simulation time\n",
    "num_steps = int(T / dt)\n",
    "x_history = []\n",
    "u_history = []\n",
    "\n",
    "for _ in range(num_steps):\n",
    "    # Initial guess for control inputs\n",
    "    u0 = np.zeros(N)\n",
    "    \n",
    "    # Optimize control sequence\n",
    "    res = minimize(lambda u: mpc_cost(u, x_current), u0, bounds=[(-1, 1)]*N)\n",
    "    u_opt = res.x\n",
    "    \n",
    "    # Apply the first control input\n",
    "    u_current = u_opt[0]\n",
    "    x_current = x_current + dynamics(x_current, u_current) * dt\n",
    "    \n",
    "    # Save history\n",
    "    x_history.append(x_current.copy())\n",
    "    u_history.append(u_current)\n",
    "\n",
    "x_history = np.array(x_history)\n",
    "u_history = np.array(u_history)\n",
    "t = np.linspace(0, T, num_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(t, x_history[:,0], label='Position')\n",
    "plt.plot(t, x_history[:,1], label='Velocity')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('State')\n",
    "plt.legend()\n",
    "plt.title('MPC Trajectory')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.step(t, u_history, where='post')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Control Input')\n",
    "plt.title('MPC Control Input')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Cost Function**: Minimize the sum of squared differences from the target state and control effort.\n",
    "- **Constraints**: Control inputs are bounded between -1 and 1.\n",
    "- **Optimization**: At each time step, solve for the optimal control sequence over the prediction horizon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "# 6. Latest Developments\n",
    "\n",
    "Path and trajectory planning continue to evolve with advances in computational power and algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6.1\"></a>\n",
    "## 6.1 Machine Learning in Path Planning\n",
    "\n",
    "Machine learning techniques are being integrated into path planning to handle complex environments.\n",
    "\n",
    "### Learning-Based Methods\n",
    "\n",
    "- **Imitation Learning**: Learning from expert demonstrations.\n",
    "- **Deep Learning**: Using neural networks to approximate cost functions or policies.\n",
    "\n",
    "### Example\n",
    "\n",
    "- **Neural RRT** [[3]](#ref3): Uses neural networks to guide the sampling process in RRT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6.2\"></a>\n",
    "## 6.2 Reinforcement Learning Approaches\n",
    "\n",
    "Reinforcement learning (RL) can be used for path planning by learning policies that map observations to actions.\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- **Adaptability**: Can handle dynamic and uncertain environments.\n",
    "- **End-to-End Learning**: From raw sensor data to control inputs.\n",
    "\n",
    "### Challenges\n",
    "\n",
    "- **Sample Efficiency**: RL often requires a large number of interactions.\n",
    "- **Safety**: Ensuring safety during exploration.\n",
    "\n",
    "### Example\n",
    "\n",
    "- **Deep Deterministic Policy Gradient (DDPG)**: An RL algorithm suitable for continuous action spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "# 7. Conclusion\n",
    "\n",
    "Advanced trajectory and path planning methods are essential for autonomous systems operating in complex environments. Sampling-based methods like RRT and RRT* provide probabilistic completeness and, in the case of RRT*, asymptotic optimality. Optimization-based methods offer precise control over constraints and cost functions, while kinodynamic planning incorporates system dynamics. MPC provides a real-time control strategy by solving optimization problems at each time step. The integration of machine learning and reinforcement learning opens new possibilities for adaptive and intelligent planning.\n",
    "\n",
    "Understanding these methods' underlying mathematics and implementation details equips practitioners to develop robust and efficient path planning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>\n",
    "# 8. References\n",
    "\n",
    "1. <a id=\"ref1\"></a>LaValle, S. M., & Kuffner, J. J. (2001). *Rapidly-Exploring Random Trees: Progress and Prospects*. Algorithmic and Computational Robotics: New Directions, 293–308.\n",
    "2. <a id=\"ref2\"></a>Karaman, S., & Frazzoli, E. (2011). *Sampling-based Algorithms for Optimal Motion Planning*. The International Journal of Robotics Research, 30(7), 846–894.\n",
    "3. <a id=\"ref3\"></a>Ichter, B., Harrison, J., & Pavone, M. (2018). *Learning Sampling Distributions for Robot Motion Planning*. 2018 IEEE International Conference on Robotics and Automation (ICRA), 7087–7094.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook provides an in-depth exploration of advanced trajectory and path planning methods. You can run the code cells to see how the algorithms are implemented and experiment with different parameters and environments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
