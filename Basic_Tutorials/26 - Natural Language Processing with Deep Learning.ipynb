{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Deep Learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Natural Language Processing (NLP) is a field at the intersection of computer science, artificial intelligence, and linguistics, concerned with the interactions between computers and human language. Deep learning has revolutionized NLP by enabling models to learn complex patterns in text data.\n",
    "\n",
    "In this tutorial, we'll delve into NLP tasks using deep learning. We'll implement models for sentiment analysis, named entity recognition (NER), and question answering. We'll cover the underlying mathematics, provide example code, and explain the processes involved. We'll also reference key papers and discuss some of the latest developments in this field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Understanding Natural Language Processing](#1)\n",
    "   - [Overview of NLP](#1.1)\n",
    "   - [Challenges in NLP](#1.2)\n",
    "2. [Sentiment Analysis](#2)\n",
    "   - [Introduction to Sentiment Analysis](#2.1)\n",
    "   - [Mathematical Foundations](#2.2)\n",
    "   - [Implementing Sentiment Analysis with LSTM](#2.3)\n",
    "3. [Named Entity Recognition (NER)](#3)\n",
    "   - [Introduction to NER](#3.1)\n",
    "   - [Mathematical Foundations](#3.2)\n",
    "   - [Implementing NER with Bi-LSTM and CRF](#3.3)\n",
    "4. [Question Answering](#4)\n",
    "   - [Introduction to Question Answering](#4.1)\n",
    "   - [Mathematical Foundations](#4.2)\n",
    "   - [Implementing Question Answering with BERT](#4.3)\n",
    "5. [Latest Developments in NLP](#5)\n",
    "   - [Transformers and Attention Mechanisms](#5.1)\n",
    "   - [GPT Models](#5.2)\n",
    "6. [Conclusion](#6)\n",
    "7. [References](#7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# 1. Understanding Natural Language Processing\n",
    "\n",
    "<a id=\"1.1\"></a>\n",
    "## 1.1 Overview of NLP\n",
    "\n",
    "Natural Language Processing involves enabling computers to understand, interpret, and generate human language. It combines computational linguistics with statistical, machine learning, and deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.2\"></a>\n",
    "## 1.2 Challenges in NLP\n",
    "\n",
    "- **Ambiguity**: Words and sentences can have multiple meanings depending on context.\n",
    "- **Contextual Understanding**: Requires capturing long-range dependencies in text.\n",
    "- **Data Sparsity**: Language is vast; models need to generalize well.\n",
    "- **Complex Structure**: Language has hierarchical structures that need to be modeled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "# 2. Sentiment Analysis\n",
    "\n",
    "<a id=\"2.1\"></a>\n",
    "## 2.1 Introduction to Sentiment Analysis\n",
    "\n",
    "Sentiment Analysis is the task of classifying text into predefined sentiment categories, such as positive, negative, or neutral. It's widely used in areas like customer feedback analysis, social media monitoring, and market research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.2\"></a>\n",
    "## 2.2 Mathematical Foundations\n",
    "\n",
    "### Word Embeddings\n",
    "\n",
    "Word embeddings map words to continuous vector representations. Common methods include Word2Vec [[1]](#ref1) and GloVe [[2]](#ref2).\n",
    "\n",
    "Given a corpus, embeddings are learned such that words with similar context have similar vectors.\n",
    "\n",
    "### Recurrent Neural Networks (RNNs)\n",
    "\n",
    "RNNs process sequences by maintaining a hidden state $( h_t )$:\n",
    "\n",
    "$[\n",
    "    h_t = f(W_{hh} h_{t-1} + W_{xh} x_t)\n",
    "]$\n",
    "\n",
    "- $( x_t )$: Input at time $( t )$.\n",
    "- $( h_{t-1} )$: Previous hidden state.\n",
    "- $( f )$: Activation function.\n",
    "\n",
    "### Long Short-Term Memory (LSTM)\n",
    "\n",
    "LSTMs [[3]](#ref3) address the vanishing gradient problem in RNNs. They have gates controlling the flow of information:\n",
    "\n",
    "- **Forget Gate** $( f_t )$\n",
    "- **Input Gate** $( i_t )$\n",
    "- **Output Gate** $( o_t )$\n",
    "\n",
    "The cell state $( C_t )$ is updated using these gates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow torch transformers sklearn_crfsuite nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download NLTK Data:\n",
    "\n",
    "In the NER example, you might need to download NLTK data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('conll2002')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.3\"></a>\n",
    "## 2.3 Implementing Sentiment Analysis with LSTM\n",
    "\n",
    "We'll use the IMDb movie reviews dataset for binary sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Set parameters\n",
    "vocab_size = 10000  # Only consider the top 10,000 words\n",
    "maxlen = 200       # Only consider the first 200 words of each movie review\n",
    "\n",
    "# Load the data\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "# Pad sequences\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 128, input_length=maxlen))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=2, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print(f'Test score: {score:.4f}, Test accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Embedding Layer**: Converts word indices to embeddings.\n",
    "- **LSTM Layer**: Processes the sequence data.\n",
    "- **Dense Layer**: Outputs a probability for the positive class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "# 3. Named Entity Recognition (NER)\n",
    "\n",
    "<a id=\"3.1\"></a>\n",
    "## 3.1 Introduction to NER\n",
    "\n",
    "Named Entity Recognition involves identifying and classifying named entities in text into predefined categories such as person names, organizations, locations, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.2\"></a>\n",
    "## 3.2 Mathematical Foundations\n",
    "\n",
    "### Conditional Random Fields (CRF)\n",
    "\n",
    "CRFs [[4]](#ref4) are probabilistic models used for structured prediction. In NER, CRFs model the conditional probability of a label sequence given an input sequence.\n",
    "\n",
    "The probability of a label sequence $( y )$ given an input sequence $( x )$ is:\n",
    "\n",
    "$[\n",
    "    P(y | x) = \\frac{1}{Z(x)} \\exp\\left( \\sum_{t} \\sum_{k} \\lambda_k f_k(y_{t-1}, y_t, x, t) \\right)\n",
    "]$\n",
    "\n",
    "- $( f_k )$: Feature functions.\n",
    "- $( \\lambda_k )$: Parameters to learn.\n",
    "- $( Z(x) )$: Partition function for normalization.\n",
    "\n",
    "### Bi-directional LSTM with CRF\n",
    "\n",
    "Combining Bi-LSTM with CRF allows the model to capture both past and future information (via Bi-LSTM) and consider label dependencies (via CRF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.3\"></a>\n",
    "## 3.3 Implementing NER with Bi-LSTM and CRF\n",
    "\n",
    "We'll use the CoNLL-2003 dataset for NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "# !pip install sklearn_crfsuite\n",
    "\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "\n",
    "# Download the dataset\n",
    "nltk.download('conll2002')\n",
    "\n",
    "data = list(nltk.corpus.conll2002.iob_sents('esp.train'))\n",
    "\n",
    "# Feature extraction\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:postag': postag1,\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:postag': postag1,\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]\n",
    "\n",
    "# Prepare data\n",
    "X = [sent2features(s) for s in data]\n",
    "y = [sent2labels(s) for s in data]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Train CRF model\n",
    "crf = CRF(algorithm='lbfgs',\n",
    "          c1=0.1,\n",
    "          c2=0.1,\n",
    "          max_iterations=100,\n",
    "          all_possible_transitions=True)\n",
    "\n",
    "crf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = crf.predict(X_test)\n",
    "print(flat_classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Feature Extraction**: We extract features for each word in the sentence.\n",
    "- **CRF Model**: Trained to predict the label sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Implementing a Bi-LSTM with CRF requires more advanced code and often uses libraries like `keras_contrib` or PyTorch with the `torchcrf` module. For brevity, we're demonstrating a CRF implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "# 4. Question Answering\n",
    "\n",
    "<a id=\"4.1\"></a>\n",
    "## 4.1 Introduction to Question Answering\n",
    "\n",
    "Question Answering (QA) involves building systems that can automatically answer questions posed by humans in natural language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.2\"></a>\n",
    "## 4.2 Mathematical Foundations\n",
    "\n",
    "### Transformer Architecture\n",
    "\n",
    "Transformers [[5]](#ref5) use self-attention mechanisms to model dependencies in sequences without recurrent layers.\n",
    "\n",
    "**Scaled Dot-Product Attention**:\n",
    "\n",
    "$[\n",
    "    \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V\n",
    "]$\n",
    "\n",
    "- $( Q )$: Query matrix.\n",
    "- $( K )$: Key matrix.\n",
    "- $( V )$: Value matrix.\n",
    "- $( d_k )$: Dimension of the key vectors.\n",
    "\n",
    "### BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "BERT [[6]](#ref6) is a pre-trained language model based on Transformers. It uses masked language modeling and next sentence prediction for pre-training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.3\"></a>\n",
    "## 4.3 Implementing Question Answering with BERT\n",
    "\n",
    "We'll use the Hugging Face Transformers library to implement a QA model with BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install transformers library\n",
    "# !pip install transformers\n",
    "\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "# Define context and question\n",
    "context = r\"\"\"\n",
    "The Apollo program was the third United States human spaceflight program carried out by the National Aeronautics and Space Administration (NASA), which accomplished landing the first humans on the Moon from 1969 to 1972.\n",
    "\"\"\"\n",
    "question = \"When did the first humans land on the Moon?\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors='pt')\n",
    "input_ids = inputs['input_ids']\n",
    "token_type_ids = inputs['token_type_ids']\n",
    "\n",
    "# Get the answer\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, token_type_ids=token_type_ids)\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "\n",
    "# Find the tokens with the highest `start` and `end` scores.\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "answer = ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1])\n",
    "\n",
    "# Print the answer\n",
    "print('Answer:', answer.replace('##', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Tokenization**: The tokenizer prepares the inputs by adding special tokens and creating token type IDs.\n",
    "- **Model Prediction**: The model outputs start and end logits for the answer span.\n",
    "- **Answer Extraction**: We extract the tokens corresponding to the highest start and end scores and reconstruct the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "# 5. Latest Developments in NLP\n",
    "\n",
    "<a id=\"5.1\"></a>\n",
    "## 5.1 Transformers and Attention Mechanisms\n",
    "\n",
    "Transformers [[5]](#ref5) have revolutionized NLP by enabling models to capture global dependencies without recurrence. They rely entirely on self-attention mechanisms.\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- **Parallelization**: Transformers can process entire sequences simultaneously.\n",
    "- **Performance**: Achieve state-of-the-art results in various NLP tasks.\n",
    "\n",
    "<a id=\"5.2\"></a>\n",
    "## 5.2 GPT Models\n",
    "\n",
    "Generative Pre-trained Transformer (GPT) models [[7]](#ref7) are transformer-based models designed for language generation tasks.\n",
    "\n",
    "- **GPT-2 and GPT-3**: Demonstrated impressive capabilities in generating coherent and contextually relevant text.\n",
    "- **Applications**: Text completion, translation, summarization, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "# 6. Conclusion\n",
    "\n",
    "Deep learning has significantly advanced the field of Natural Language Processing. Models like LSTMs, Transformers, and pre-trained language models like BERT have enabled breakthroughs in tasks such as sentiment analysis, named entity recognition, and question answering. Understanding the underlying mathematics and being able to implement these models is crucial for leveraging their capabilities in real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "# 7. References\n",
    "\n",
    "1. <a id=\"ref1\"></a>Mikolov, T., et al. (2013). *Efficient Estimation of Word Representations in Vector Space*. [arXiv:1301.3781](https://arxiv.org/abs/1301.3781)\n",
    "2. <a id=\"ref2\"></a>Pennington, J., Socher, R., & Manning, C. D. (2014). *GloVe: Global Vectors for Word Representation*. [EMNLP 2014](https://www.aclweb.org/anthology/D14-1162/)\n",
    "3. <a id=\"ref3\"></a>Hochreiter, S., & Schmidhuber, J. (1997). *Long Short-Term Memory*. Neural Computation, 9(8), 1735-1780.\n",
    "4. <a id=\"ref4\"></a>Lafferty, J., McCallum, A., & Pereira, F. (2001). *Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data*. [ICML 2001](https://dl.acm.org/doi/10.5555/645530.655813)\n",
    "5. <a id=\"ref5\"></a>Vaswani, A., et al. (2017). *Attention Is All You Need*. [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)\n",
    "6. <a id=\"ref6\"></a>Devlin, J., et al. (2018). *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*. [arXiv:1810.04805](https://arxiv.org/abs/1810.04805)\n",
    "7. <a id=\"ref7\"></a>Radford, A., et al. (2019). *Language Models are Unsupervised Multitask Learners*. OpenAI Blog.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook provides an in-depth exploration of Natural Language Processing with deep learning. You can run the code cells to see how these models are implemented and experiment with different datasets and architectures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
