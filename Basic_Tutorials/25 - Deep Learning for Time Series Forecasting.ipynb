{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Time Series Forecasting\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Time series forecasting is a crucial task in various domains, including finance, weather prediction, supply chain management, and many others. Traditional methods like ARIMA and exponential smoothing have been widely used, but they often struggle with capturing complex patterns and long-term dependencies in data. Deep learning approaches, particularly Recurrent Neural Networks (RNNs) and their variants, have shown significant promise in modeling time series data.\n",
    "\n",
    "In this tutorial, we'll explore how to apply neural networks to time series data. We'll delve into architectures like Long Short-Term Memory Networks (LSTMs) and Temporal Convolutional Networks (TCNs) for forecasting. We'll also include the underlying mathematics, provide example code, and explain the processes involved. Additionally, we'll reference key papers and discuss some of the latest developments in this field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Understanding Time Series Data](#1)\n",
    "   - [Characteristics of Time Series Data](#1.1)\n",
    "   - [Challenges in Time Series Forecasting](#1.2)\n",
    "2. [Recurrent Neural Networks (RNNs)](#2)\n",
    "   - [Introduction to RNNs](#2.1)\n",
    "   - [Mathematical Foundations](#2.2)\n",
    "3. [Long Short-Term Memory Networks (LSTMs)](#3)\n",
    "   - [Introduction to LSTMs](#3.1)\n",
    "   - [Mathematical Foundations](#3.2)\n",
    "4. [Temporal Convolutional Networks (TCNs)](#4)\n",
    "   - [Introduction to TCNs](#4.1)\n",
    "   - [Mathematical Foundations](#4.2)\n",
    "5. [Implementing Time Series Forecasting with LSTM](#5)\n",
    "   - [Data Preparation](#5.1)\n",
    "   - [Building the LSTM Model](#5.2)\n",
    "   - [Training and Evaluation](#5.3)\n",
    "6. [Implementing Time Series Forecasting with TCN](#6)\n",
    "   - [Building the TCN Model](#6.1)\n",
    "   - [Training and Evaluation](#6.2)\n",
    "7. [Latest Developments in Time Series Forecasting](#7)\n",
    "   - [Attention Mechanisms](#7.1)\n",
    "   - [Transformers for Time Series](#7.2)\n",
    "8. [Conclusion](#8)\n",
    "9. [References](#9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# 1. Understanding Time Series Data\n",
    "\n",
    "Time series data is a sequence of data points collected or recorded at time-ordered intervals. It is ubiquitous in various fields such as finance, meteorology, medicine, and more. Understanding the characteristics and challenges of time series data is essential for effective forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.1\"></a>\n",
    "## 1.1 Characteristics of Time Series Data\n",
    "\n",
    "- **Temporal Dependence**: Observations in time series data are not independent; they are dependent on previous observations.\n",
    "- **Trend**: Long-term increase or decrease in the data.\n",
    "- **Seasonality**: Regularly occurring patterns or cycles in data.\n",
    "- **Noise**: Random variation or irregularities in the data.\n",
    "- **Stationarity**: Statistical properties (mean, variance) of the time series are constant over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.2\"></a>\n",
    "## 1.2 Challenges in Time Series Forecasting\n",
    "\n",
    "- **Non-Stationarity**: Time series data often exhibit trends and seasonality, making them non-stationary.\n",
    "- **Complex Patterns**: Traditional models may struggle with capturing nonlinear and complex temporal dependencies.\n",
    "- **High Dimensionality**: Multivariate time series can have many variables interacting in complex ways.\n",
    "- **Long-Term Dependencies**: Capturing long-range dependencies is challenging for traditional methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "# 2. Recurrent Neural Networks (RNNs)\n",
    "\n",
    "<a id=\"2.1\"></a>\n",
    "## 2.1 Introduction to RNNs\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a class of neural networks designed to process sequential data. They are characterized by their ability to maintain a hidden state that captures information about previous inputs, making them suitable for time series forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.2\"></a>\n",
    "## 2.2 Mathematical Foundations\n",
    "\n",
    "In an RNN, the hidden state $(h_t )$ at time step $(t )$ is computed based on the input $(x_t )$ and the previous hidden state $(h_{t-1} )$:\n",
    "\n",
    "$[\n",
    "    h_t = \\sigma(W_h x_t + U_h h_{t-1} + b_h)\n",
    "]$\n",
    "\n",
    "- $(W_h )$: Weight matrix for input to hidden state.\n",
    "- $(U_h )$: Weight matrix for hidden to hidden state.\n",
    "- $(b_h )$: Bias vector.\n",
    "- $(\\sigma )$: Activation function (e.g., tanh or ReLU).\n",
    "\n",
    "The output $(y_t )$ can be computed as:\n",
    "\n",
    "$[\n",
    "    y_t = \\phi(W_y h_t + b_y)\n",
    "]$\n",
    "\n",
    "- $(W_y )$: Weight matrix from hidden state to output.\n",
    "- $(b_y )$: Bias vector.\n",
    "- $(\\phi )$: Activation function (depending on the task).\n",
    "\n",
    "### Limitations of RNNs\n",
    "\n",
    "- **Vanishing/Exploding Gradients**: Difficulty in learning long-term dependencies due to gradients becoming very small or very large during backpropagation through time.\n",
    "- **Short-Term Memory**: Tendency to focus on recent inputs, failing to capture long-term dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "# 3. Long Short-Term Memory Networks (LSTMs)\n",
    "\n",
    "<a id=\"3.1\"></a>\n",
    "## 3.1 Introduction to LSTMs\n",
    "\n",
    "Long Short-Term Memory (LSTM) networks [[1]](#ref1) are a type of RNN designed to overcome the vanishing gradient problem. They introduce a memory cell that can maintain information over long periods, making them effective at capturing long-term dependencies in time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.2\"></a>\n",
    "## 3.2 Mathematical Foundations\n",
    "\n",
    "An LSTM cell consists of several components:\n",
    "\n",
    "1. **Forget Gate**:\n",
    "\n",
    "   $[\n",
    "   f_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f)\n",
    "   ]$\n",
    "\n",
    "2. **Input Gate**:\n",
    "\n",
    "   $[\n",
    "   i_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i)\n",
    "   ]$\n",
    "\n",
    "3. **Candidate Memory Cell**:\n",
    "\n",
    "   $[\n",
    "   \\tilde{C}_t = \\tanh(W_c x_t + U_c h_{t-1} + b_c)\n",
    "   ]$\n",
    "\n",
    "4. **Update Memory Cell**:\n",
    "\n",
    "   $[\n",
    "   C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t\n",
    "   ]$\n",
    "\n",
    "5. **Output Gate**:\n",
    "\n",
    "   $[\n",
    "   o_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o)\n",
    "   ]$\n",
    "\n",
    "6. **Hidden State**:\n",
    "\n",
    "   $[\n",
    "   h_t = o_t \\odot \\tanh(C_t)\n",
    "   ]$\n",
    "\n",
    "- $(\\sigma )$: Sigmoid activation function.\n",
    "- $(\\odot )$: Element-wise multiplication.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "- **Forget Gate**: Decides what information to discard from the previous cell state.\n",
    "- **Input Gate**: Decides what new information to add to the cell state.\n",
    "- **Cell State**: Maintains the long-term memory.\n",
    "- **Output Gate**: Decides what information to output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "# 4. Temporal Convolutional Networks (TCNs)\n",
    "\n",
    "<a id=\"4.1\"></a>\n",
    "## 4.1 Introduction to TCNs\n",
    "\n",
    "Temporal Convolutional Networks [[2]](#ref2) are a type of convolutional neural network designed for sequence modeling tasks. TCNs use causal convolutions and dilations to capture long-range dependencies without the need for recurrent connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.2\"></a>\n",
    "## 4.2 Mathematical Foundations\n",
    "\n",
    "### Causal Convolutions\n",
    "\n",
    "In a causal convolution, the output at time $(t )$ depends only on inputs from time $(t )$ and earlier. For a 1D convolution with filter $(f )$:\n",
    "\n",
    "$[\n",
    "   y_t = \\sum_{k=0}^{K-1} f_k \\cdot x_{t - k}\n",
    "]$\n",
    "\n",
    "- $(K )$: Filter size.\n",
    "\n",
    "### Dilated Convolutions\n",
    "\n",
    "Dilated convolutions allow the network to have a large receptive field without a large number of layers:\n",
    "\n",
    "$[\n",
    "   y_t = \\sum_{k=0}^{K-1} f_k \\cdot x_{t - d \\cdot k}\n",
    "]$\n",
    "\n",
    "- $(d )$: Dilation factor.\n",
    "\n",
    "### Residual Connections\n",
    "\n",
    "TCNs use residual connections to help with training deep networks:\n",
    "\n",
    "$[\n",
    "   H(x) = x + F(x)\n",
    "]$\n",
    "\n",
    "- $(F(x) )$: Output of the stacked layers.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "- **Causal Convolutions** ensure the model does not violate the temporal order.\n",
    "- **Dilations** allow the network to capture long-term dependencies efficiently.\n",
    "- **Residual Connections** aid in training deep networks by mitigating the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "# 5. Implementing Time Series Forecasting with LSTM\n",
    "\n",
    "<a id=\"5.1\"></a>\n",
    "## 5.1 Data Preparation\n",
    "\n",
    "We'll use a univariate time series dataset for this example. Let's consider the Air Passengers dataset, which contains monthly totals of international airline passengers from 1949 to 1960."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "data_url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv'\n",
    "df = pd.read_csv(data_url, usecols=['Passengers'])\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(df['Passengers'])\n",
    "plt.title('Air Passengers Dataset')\n",
    "plt.xlabel('Time (Months)')\n",
    "plt.ylabel('Number of Passengers')\n",
    "plt.show()\n",
    "\n",
    "# Convert data to numpy array\n",
    "data = df['Passengers'].values.astype('float32')\n",
    "\n",
    "# Normalize the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data = scaler.fit_transform(data.reshape(-1, 1))\n",
    "\n",
    "# Split into training and testing sets\n",
    "train_size = int(len(data) * 0.67)\n",
    "test_size = len(data) - train_size\n",
    "train, test = data[0:train_size,:], data[train_size:len(data),:]\n",
    "\n",
    "# Function to create dataset with look_back time steps\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        X.append(a)\n",
    "        Y.append(dataset[i + look_back, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "look_back = 3\n",
    "X_train, y_train = create_dataset(train, look_back)\n",
    "X_test, y_test = create_dataset(test, look_back)\n",
    "\n",
    "# Reshape input to be [samples, time steps, features]\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.2\"></a>\n",
    "## 5.2 Building the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(look_back, 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.3\"></a>\n",
    "## 5.3 Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=1, verbose=2)\n",
    "\n",
    "# Make predictions\n",
    "train_predict = model.predict(X_train)\n",
    "test_predict = model.predict(X_test)\n",
    "\n",
    "# Inverse transform predictions\n",
    "train_predict = scaler.inverse_transform(train_predict)\n",
    "y_train_inv = scaler.inverse_transform([y_train])\n",
    "test_predict = scaler.inverse_transform(test_predict)\n",
    "y_test_inv = scaler.inverse_transform([y_test])\n",
    "\n",
    "# Calculate RMSE\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "train_score = sqrt(mean_squared_error(y_train_inv[0], train_predict[:,0]))\n",
    "print(f'Train RMSE: {train_score:.2f}')\n",
    "test_score = sqrt(mean_squared_error(y_test_inv[0], test_predict[:,0]))\n",
    "print(f'Test RMSE: {test_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift train predictions for plotting\n",
    "train_predict_plot = np.empty_like(data)\n",
    "train_predict_plot[:, :] = np.nan\n",
    "train_predict_plot[look_back:len(train_predict)+look_back, :] = train_predict\n",
    "\n",
    "# Shift test predictions for plotting\n",
    "test_predict_plot = np.empty_like(data)\n",
    "test_predict_plot[:, :] = np.nan\n",
    "test_predict_plot[len(train_predict)+(look_back*2)+1:len(data)-1, :] = test_predict\n",
    "\n",
    "# Plot baseline and predictions\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(scaler.inverse_transform(data), label='Original Data')\n",
    "plt.plot(train_predict_plot, label='Training Prediction')\n",
    "plt.plot(test_predict_plot, label='Testing Prediction')\n",
    "plt.title('LSTM Time Series Prediction')\n",
    "plt.xlabel('Time (Months)')\n",
    "plt.ylabel('Number of Passengers')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "# 6. Implementing Time Series Forecasting with TCN\n",
    "\n",
    "<a id=\"6.1\"></a>\n",
    "## 6.1 Building the TCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the TCN package if not already installed\n",
    "# !pip install keras-tcn\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tcn import TCN\n",
    "\n",
    "# Build the TCN model\n",
    "tcn_model = Sequential()\n",
    "tcn_model.add(TCN(input_shape=(look_back, 1)))  # The TCN layer\n",
    "tcn_model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "tcn_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "tcn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6.2\"></a>\n",
    "## 6.2 Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the TCN model\n",
    "tcn_history = tcn_model.fit(X_train, y_train, epochs=100, batch_size=1, verbose=2)\n",
    "\n",
    "# Make predictions\n",
    "tcn_train_predict = tcn_model.predict(X_train)\n",
    "tcn_test_predict = tcn_model.predict(X_test)\n",
    "\n",
    "# Inverse transform predictions\n",
    "tcn_train_predict = scaler.inverse_transform(tcn_train_predict)\n",
    "tcn_test_predict = scaler.inverse_transform(tcn_test_predict)\n",
    "\n",
    "y_train_inv = scaler.inverse_transform([y_train])\n",
    "y_test_inv = scaler.inverse_transform([y_test])\n",
    "\n",
    "# Calculate RMSE\n",
    "tcn_train_score = sqrt(mean_squared_error(y_train_inv[0], tcn_train_predict[:,0]))\n",
    "print(f'TCN Train RMSE: {tcn_train_score:.2f}')\n",
    "tcn_test_score = sqrt(mean_squared_error(y_test_inv[0], tcn_test_predict[:,0]))\n",
    "print(f'TCN Test RMSE: {tcn_test_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift train predictions for plotting\n",
    "tcn_train_predict_plot = np.empty_like(data)\n",
    "tcn_train_predict_plot[:, :] = np.nan\n",
    "tcn_train_predict_plot[look_back:len(tcn_train_predict)+look_back, :] = tcn_train_predict\n",
    "\n",
    "# Shift test predictions for plotting\n",
    "tcn_test_predict_plot = np.empty_like(data)\n",
    "tcn_test_predict_plot[:, :] = np.nan\n",
    "tcn_test_predict_plot[len(tcn_train_predict)+(look_back*2)+1:len(data)-1, :] = tcn_test_predict\n",
    "\n",
    "# Plot baseline and predictions\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(scaler.inverse_transform(data), label='Original Data')\n",
    "plt.plot(tcn_train_predict_plot, label='TCN Training Prediction')\n",
    "plt.plot(tcn_test_predict_plot, label='TCN Testing Prediction')\n",
    "plt.title('TCN Time Series Prediction')\n",
    "plt.xlabel('Time (Months)')\n",
    "plt.ylabel('Number of Passengers')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "# 7. Latest Developments in Time Series Forecasting\n",
    "\n",
    "Time series forecasting has seen significant advancements with the introduction of attention mechanisms and transformer-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7.1\"></a>\n",
    "## 7.1 Attention Mechanisms\n",
    "\n",
    "Attention mechanisms allow models to focus on specific parts of the input sequence when making predictions. This can improve performance by enabling the model to capture important dependencies regardless of their distance in the sequence.\n",
    "\n",
    "### Self-Attention\n",
    "\n",
    "Self-attention computes a representation of the sequence by relating different positions of the sequence to each other.\n",
    "\n",
    "**Scaled Dot-Product Attention** [[3]](#ref3):\n",
    "\n",
    "$[\n",
    "   \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V\n",
    "]$\n",
    "\n",
    "- $(Q )$: Query matrix.\n",
    "- $(K )$: Key matrix.\n",
    "- $(V )$: Value matrix.\n",
    "- $(d_k )$: Dimension of the key vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7.2\"></a>\n",
    "## 7.2 Transformers for Time Series\n",
    "\n",
    "Transformer models [[3]](#ref3) have revolutionized NLP and are now being applied to time series forecasting. They rely entirely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- **Parallelization**: Transformers can process entire sequences simultaneously.\n",
    "- **Long-Term Dependencies**: Capable of capturing dependencies over long sequences.\n",
    "\n",
    "### Challenges\n",
    "\n",
    "- **Computational Complexity**: Self-attention scales quadratically with sequence length.\n",
    "\n",
    "### Recent Models\n",
    "\n",
    "- **Informer** [[4]](#ref4): Introduces ProbSparse self-attention to handle long sequences efficiently.\n",
    "- **Temporal Fusion Transformer (TFT)** [[5]](#ref5): Combines recurrent layers with attention mechanisms for interpretable time series forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>\n",
    "# 8. Conclusion\n",
    "\n",
    "Deep learning has significantly advanced the field of time series forecasting. Architectures like LSTMs and TCNs have proven effective in capturing complex temporal dependencies. Recent developments with attention mechanisms and transformer-based models continue to push the boundaries, offering improved performance and interpretability. Understanding these models and their underlying mathematics is essential for leveraging their capabilities in real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"9\"></a>\n",
    "# 9. References\n",
    "\n",
    "1. <a id=\"ref1\"></a>Hochreiter, S., & Schmidhuber, J. (1997). *Long Short-Term Memory*. Neural Computation, 9(8), 1735-1780.\n",
    "2. <a id=\"ref2\"></a>Bai, S., Kolter, J. Z., & Koltun, V. (2018). *An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling*. [arXiv:1803.01271](https://arxiv.org/abs/1803.01271)\n",
    "3. <a id=\"ref3\"></a>Vaswani, A., et al. (2017). *Attention Is All You Need*. [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)\n",
    "4. <a id=\"ref4\"></a>Zhou, H., et al. (2020). *Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting*. [arXiv:2012.07436](https://arxiv.org/abs/2012.07436)\n",
    "5. <a id=\"ref5\"></a>Lim, B., et al. (2019). *Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting*. [arXiv:1912.09363](https://arxiv.org/abs/1912.09363)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
