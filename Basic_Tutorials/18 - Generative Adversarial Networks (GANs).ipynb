{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks (GANs)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Generative Adversarial Networks (GANs), introduced by Ian Goodfellow et al. in 2014 [[1]](#ref1), have revolutionized the field of generative modeling. GANs consist of two neural networks, a generator and a discriminator, that are trained simultaneously through an adversarial process. The generator aims to produce data that is indistinguishable from real data, while the discriminator attempts to distinguish between real and generated data.\n",
    "\n",
    "In this tutorial, we'll explore the architecture of GANs, understand how generators and discriminators work, delve into the mathematical foundations, and implement a GAN for image generation using the MNIST dataset. We'll also discuss some of the latest developments in GAN research.\n",
    "\n",
    "![GAN Architecture](https://miro.medium.com/max/1400/1*LsfkZXI1i1OcsYbG_H8bOw.png)\n",
    "\n",
    "*Image Source: [Medium](https://medium.com/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Understanding GAN Architecture](#1)\n",
    "   - [Generator Network](#1.1)\n",
    "   - [Discriminator Network](#1.2)\n",
    "2. [Mathematical Foundations](#2)\n",
    "   - [GAN Objective Function](#2.1)\n",
    "   - [Training Process](#2.2)\n",
    "3. [Implementing a GAN for Image Generation](#3)\n",
    "   - [Dataset Preparation](#3.1)\n",
    "   - [Building the Generator](#3.2)\n",
    "   - [Building the Discriminator](#3.3)\n",
    "   - [Defining the GAN](#3.4)\n",
    "   - [Training the GAN](#3.5)\n",
    "   - [Generating Images](#3.6)\n",
    "4. [Latest Developments in GANs](#4)\n",
    "   - [DCGAN](#4.1)\n",
    "   - [Wasserstein GAN](#4.2)\n",
    "   - [StyleGAN](#4.3)\n",
    "   - [CycleGAN](#4.4)\n",
    "5. [Conclusion](#5)\n",
    "6. [References](#6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. Understanding GAN Architecture\n",
    "\n",
    "A GAN consists of two neural networks:\n",
    "\n",
    "- **Generator (G)**: Learns to generate fake data resembling the real data.\n",
    "- **Discriminator (D)**: Learns to distinguish between real and fake data.\n",
    "\n",
    "They are trained simultaneously in a minimax game.\n",
    "\n",
    "![GAN Training Process](https://miro.medium.com/max/1400/1*yAPb-BHZUAD4vRYPkYcMhA.png)\n",
    "\n",
    "*Image Source: [Medium](https://medium.com/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.1\"></a>\n",
    "### Generator Network\n",
    "\n",
    "The generator takes a random noise vector \\( z \\) sampled from a prior distribution (e.g., Gaussian) and transforms it into data resembling the real data distribution.\n",
    "\n",
    "$[\n",
    "G(z; \\theta_g)\n",
    "]$\n",
    "\n",
    "- $( \\theta_g )$: Parameters of the generator network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.2\"></a>\n",
    "### Discriminator Network\n",
    "\n",
    "The discriminator receives an input (either real data or generated data) and outputs a probability indicating whether the input is real or fake.\n",
    "\n",
    "$[\n",
    "D(x; \\theta_d) \\in [0, 1]\n",
    "]$\n",
    "\n",
    "- $( \\theta_d )$: Parameters of the discriminator network.\n",
    "- $( x )$: Input data (real or generated)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. Mathematical Foundations\n",
    "\n",
    "GANs are trained using a minimax game where the generator and discriminator have opposing objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.1\"></a>\n",
    "### GAN Objective Function\n",
    "\n",
    "The objective function for the GAN is defined as:\n",
    "\n",
    "$[\n",
    "\\min_{G} \\max_{D} V(D, G) = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log (1 - D(G(z)))]\n",
    "]$\n",
    "\n",
    "- **Generator Objective:** Minimize $( \\log (1 - D(G(z))) )$, i.e., generate data that the discriminator classifies as real.\n",
    "- **Discriminator Objective:** Maximize $( \\log D(x) + \\log (1 - D(G(z))) )$, i.e., correctly classify real and fake data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.2\"></a>\n",
    "### Training Process\n",
    "\n",
    "1. **Update Discriminator:**\n",
    "   - Maximize the probability of assigning the correct label to both real and generated data.\n",
    "2. **Update Generator:**\n",
    "   - Minimize the probability that the discriminator correctly identifies generated data as fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode for Training\n",
    "for number of training iterations:\n",
    "    for k steps:\n",
    "        Sample real data x ~ p_data(x)\n",
    "        Sample noise z ~ p_z(z)\n",
    "        Update the discriminator by ascending its stochastic gradient:\n",
    "            ∇_θd [log D(x) + log(1 - D(G(z)))]\n",
    "    Sample noise z ~ p_z(z)\n",
    "    Update the generator by descending its stochastic gradient:\n",
    "        ∇_θg [log(1 - D(G(z)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. Implementing a GAN for Image Generation\n",
    "\n",
    "We'll implement a simple GAN using TensorFlow and Keras to generate images similar to the MNIST handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.1\"></a>\n",
    "### Dataset Preparation\n",
    "\n",
    "We'll use the MNIST dataset, which consists of 28x28 grayscale images of handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "\n",
    "# Normalize the images to [-1, 1]\n",
    "train_images = (train_images - 127.5) / 127.5\n",
    "\n",
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Batch and shuffle the data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.2\"></a>\n",
    "### Building the Generator\n",
    "\n",
    "The generator network transforms a random noise vector into a 28x28x1 image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Model\n",
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((7, 7, 256)))\n",
    "    assert model.output_shape == (None, 7, 7, 256)  # None is the batch size\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 7, 7, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the generator\n",
    "generator = make_generator_model()\n",
    "\n",
    "# Generate a sample noise vector\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "# Display the generated image\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator starts with a dense layer that transforms the input noise vector into a 7x7x256 tensor. Then, it uses transposed convolutions (also known as deconvolutions) to upsample the tensor to 28x28x1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.3\"></a>\n",
    "### Building the Discriminator\n",
    "\n",
    "The discriminator network classifies 28x28x1 images as real or fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator Model\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[28, 28, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the discriminator\n",
    "discriminator = make_discriminator_model()\n",
    "\n",
    "# Test the discriminator\n",
    "decision = discriminator(generated_image)\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discriminator uses convolutional layers to extract features from the input image and outputs a single value representing the probability that the input image is real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.4\"></a>\n",
    "### Defining the GAN\n",
    "\n",
    "We'll define the loss functions and optimizers for both the generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss functions and optimizers\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "# Discriminator loss\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "# Generator loss\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "# Optimizers\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.5\"></a>\n",
    "### Training the GAN\n",
    "\n",
    "We'll define the training loop to update the generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "EPOCHS = 50\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# Seed for visualization\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
    "\n",
    "# Training step\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "# Training loop\n",
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for image_batch in dataset:\n",
    "            train_step(image_batch)\n",
    "\n",
    "        # Produce images for the GIF\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator, epoch + 1, seed)\n",
    "\n",
    "    # Generate after the final epoch\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator, epochs, seed)\n",
    "\n",
    "# Generate and save images\n",
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    predictions = model(test_input, training=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()\n",
    "\n",
    "# Start training\n",
    "from IPython import display\n",
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, the generator tries to produce images that can fool the discriminator, while the discriminator tries to become better at distinguishing real images from fake ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.6\"></a>\n",
    "### Generating Images\n",
    "\n",
    "After training, we can use the generator to produce new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new images\n",
    "import numpy as np\n",
    "\n",
    "noise = tf.random.normal([num_examples_to_generate, noise_dim])\n",
    "predictions = generator(noise, training=False)\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "for i in range(predictions.shape[0]):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated images resemble handwritten digits from the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4. Latest Developments in GANs\n",
    "\n",
    "GANs have evolved significantly since their introduction, with numerous variants addressing different challenges and applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.1\"></a>\n",
    "### 4.1 Deep Convolutional GAN (DCGAN)\n",
    "\n",
    "**DCGANs** [[2]](#ref2) introduce architectural guidelines to improve the stability of GANs:\n",
    "\n",
    "- Replace pooling layers with strided convolutions and transposed convolutions.\n",
    "- Use batch normalization in both the generator and discriminator.\n",
    "- Remove fully connected hidden layers.\n",
    "- Use ReLU activation in the generator and LeakyReLU in the discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.2\"></a>\n",
    "### 4.2 Wasserstein GAN (WGAN)\n",
    "\n",
    "**Wasserstein GANs** [[3]](#ref3) address training instability and mode collapse by using the Wasserstein distance (Earth Mover's Distance) as the loss metric.\n",
    "\n",
    "**Key Changes:**\n",
    "\n",
    "- Replace the discriminator with a critic that outputs real-valued scores.\n",
    "- Remove the sigmoid activation in the output layer of the critic.\n",
    "- Use weight clipping to enforce a Lipschitz constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WGAN Critic Loss\n",
    "def critic_loss(real_output, fake_output):\n",
    "    return tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n",
    "\n",
    "# WGAN Generator Loss\n",
    "def generator_loss(fake_output):\n",
    "    return -tf.reduce_mean(fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.3\"></a>\n",
    "### 4.3 StyleGAN\n",
    "\n",
    "**StyleGAN** [[4]](#ref4) introduces a new generator architecture capable of synthesizing high-resolution, photorealistic images.\n",
    "\n",
    "**Key Innovations:**\n",
    "\n",
    "- Style-based generator architecture.\n",
    "- Adaptive Instance Normalization (AdaIN) layers.\n",
    "- Stochastic variation through noise inputs.\n",
    "\n",
    "**Applications:**\n",
    "\n",
    "- Generating human faces indistinguishable from real photos.\n",
    "- Fine-grained control over image attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.4\"></a>\n",
    "### 4.4 CycleGAN\n",
    "\n",
    "**CycleGAN** [[5]](#ref5) enables image-to-image translation without paired training data.\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "- **Cycle Consistency Loss:** Ensures that translating an image from one domain to another and back results in the original image.\n",
    "- **Two Generators and Two Discriminators:** For mapping between the two domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## 5. Conclusion\n",
    "\n",
    "GANs have opened up exciting possibilities in generative modeling, enabling the creation of realistic images, videos, and more. Understanding the interplay between the generator and discriminator is crucial for effectively training GANs. Ongoing research continues to address challenges such as training stability, mode collapse, and scalability, leading to more powerful and versatile GAN architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "## 6. References\n",
    "\n",
    "1. <a id=\"ref1\"></a>Goodfellow, I., et al. (2014). *Generative Adversarial Nets*. [NeurIPS 2014](https://papers.nips.cc/paper/5423-generative-adversarial-nets)\n",
    "2. <a id=\"ref2\"></a>Radford, A., Metz, L., & Chintala, S. (2015). *Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks*. [arXiv:1511.06434](https://arxiv.org/abs/1511.06434)\n",
    "3. <a id=\"ref3\"></a>Arjovsky, M., Chintala, S., & Bottou, L. (2017). *Wasserstein GAN*. [arXiv:1701.07875](https://arxiv.org/abs/1701.07875)\n",
    "4. <a id=\"ref4\"></a>Karras, T., Laine, S., & Aila, T. (2018). *A Style-Based Generator Architecture for Generative Adversarial Networks*. [arXiv:1812.04948](https://arxiv.org/abs/1812.04948)\n",
    "5. <a id=\"ref5\"></a>Zhu, J., et al. (2017). *Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks*. [arXiv:1703.10593](https://arxiv.org/abs/1703.10593)\n",
    "\n",
    "---\n",
    "\n",
    "This notebook provides an in-depth exploration of GANs. You can run the code cells to see how GANs are implemented and experiment with different architectures and datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
