{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data Generation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Synthetic data generation involves creating artificial datasets that mimic real-world data. This technique is invaluable in situations where data is scarce, sensitive, or imbalanced. Synthetic data can be used to enhance machine learning models, protect privacy, and test algorithms under various conditions.\n",
    "\n",
    "In this tutorial, we'll explore different methods of synthetic data generation, including their underlying mathematics, implementations, and use cases. We'll reference key papers and discuss the latest developments in the field. Relevant imagery and code examples will be provided to enhance understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Why Synthetic Data?](#1)\n",
    "2. [Basic Statistical Methods](#2)\n",
    "   - [Random Sampling](#2.1)\n",
    "   - [Bootstrapping](#2.2)\n",
    "3. [Data Augmentation](#3)\n",
    "   - [Image Data Augmentation](#3.1)\n",
    "   - [Text Data Augmentation](#3.2)\n",
    "4. [Generative Models](#4)\n",
    "   - [Variational Autoencoders (VAEs)](#4.1)\n",
    "   - [Generative Adversarial Networks (GANs)](#4.2)\n",
    "5. [Synthetic Tabular Data Generation](#5)\n",
    "   - [CTGAN](#5.1)\n",
    "   - [Copulas](#5.2)\n",
    "6. [Applications of Synthetic Data](#6)\n",
    "7. [Latest Developments](#7)\n",
    "   - [Differential Privacy in Synthetic Data](#7.1)\n",
    "   - [Self-Supervised Learning](#7.2)\n",
    "8. [Conclusion](#8)\n",
    "9. [References](#9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# 1. Why Synthetic Data?\n",
    "\n",
    "- **Privacy Preservation**: Synthetic data can be shared without exposing sensitive information.\n",
    "- **Data Augmentation**: Enhances model performance by providing more training examples.\n",
    "- **Imbalanced Datasets**: Balances classes by generating minority class samples.\n",
    "- **Testing and Validation**: Allows testing algorithms under various controlled conditions.\n",
    "- **Cost Reduction**: Reduces the need for expensive data collection and labeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "# 2. Basic Statistical Methods\n",
    "\n",
    "Simple statistical techniques can generate synthetic data by sampling from predefined distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.1\"></a>\n",
    "## 2.1 Random Sampling\n",
    "\n",
    "We can generate synthetic data by sampling from probability distributions such as uniform, normal, or custom distributions.\n",
    "\n",
    "### Uniform Distribution\n",
    "\n",
    "Generate data uniformly distributed between two values $( a )$ and $( b )$:\n",
    "\n",
    "$[\n",
    "X \\sim \\text{Uniform}(a, b)\n",
    "]$\n",
    "\n",
    "### Normal Distribution\n",
    "\n",
    "Generate data following a normal distribution with mean $( \\mu )$ and standard deviation $( \\sigma )$:\n",
    "\n",
    "$[\n",
    "X \\sim \\mathcal{N}(\\mu, \\sigma^2)\n",
    "]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate uniform data\n",
    "uniform_data = np.random.uniform(low=0.0, high=1.0, size=1000)\n",
    "\n",
    "# Generate normal data\n",
    "normal_data = np.random.normal(loc=0.0, scale=1.0, size=1000)\n",
    "\n",
    "# Plot histograms\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].hist(uniform_data, bins=30, color='skyblue', edgecolor='black')\n",
    "axes[0].set_title('Uniform Distribution')\n",
    "axes[1].hist(normal_data, bins=30, color='salmon', edgecolor='black')\n",
    "axes[1].set_title('Normal Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- We use `np.random.uniform` and `np.random.normal` to generate synthetic data.\n",
    "- Histograms visualize the distribution of the generated data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.2\"></a>\n",
    "## 2.2 Bootstrapping\n",
    "\n",
    "Bootstrapping involves resampling with replacement from the original dataset to create new synthetic datasets. It's useful for estimating the sampling distribution of a statistic.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. Given a dataset of size \\( n \\), sample \\( n \\) data points with replacement.\n",
    "2. Repeat the process multiple times to create multiple synthetic datasets.\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "Bootstrapping approximates the sampling distribution of a statistic by resampling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original dataset\n",
    "original_data = np.random.normal(0, 1, 100)\n",
    "\n",
    "# Bootstrap samples\n",
    "bootstrap_samples = []\n",
    "num_samples = 1000\n",
    "for _ in range(num_samples):\n",
    "    sample = np.random.choice(original_data, size=len(original_data), replace=True)\n",
    "    bootstrap_samples.append(sample.mean())\n",
    "\n",
    "# Plot the distribution of sample means\n",
    "plt.hist(bootstrap_samples, bins=30, color='lightgreen', edgecolor='black')\n",
    "plt.title('Bootstrap Distribution of Sample Means')\n",
    "plt.xlabel('Sample Mean')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- Resample the original data with replacement to create bootstrap samples.\n",
    "- Compute the mean of each sample to estimate the sampling distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "# 3. Data Augmentation\n",
    "\n",
    "Data augmentation creates synthetic data by applying transformations to existing data. It's widely used in computer vision and natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.1\"></a>\n",
    "## 3.1 Image Data Augmentation\n",
    "\n",
    "Common techniques include:\n",
    "\n",
    "- **Rotation**\n",
    "- **Flipping**\n",
    "- **Scaling**\n",
    "- **Cropping**\n",
    "- **Color Jitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Load an example image\n",
    "url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/5/54/Coat_of_Arms_of_Granada.png/600px-Coat_of_Arms_of_Granada.png'\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "# Define augmentation transforms\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=1.0),\n",
    "    transforms.RandomRotation(degrees=45),\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5),\n",
    "])\n",
    "\n",
    "# Apply augmentation\n",
    "augmented_img = augmentation(img)\n",
    "\n",
    "# Display original and augmented images\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axes[0].imshow(img)\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(augmented_img)\n",
    "axes[1].set_title('Augmented Image')\n",
    "axes[1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- Use `torchvision.transforms` to define augmentation operations.\n",
    "- Apply transformations to the image to create augmented data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.2\"></a>\n",
    "## 3.2 Text Data Augmentation\n",
    "\n",
    "Techniques include:\n",
    "\n",
    "- **Synonym Replacement**\n",
    "- **Random Insertion**\n",
    "- **Random Swap**\n",
    "- **Random Deletion**\n",
    "\n",
    "Reference: Wei and Zou (2019) proposed **EDA (Easy Data Augmentation)** [[1]](#ref1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install NLTK and download wordnet\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synonym Replacement function\n",
    "def synonym_replacement(sentence, n):\n",
    "    words = sentence.split()\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set(words))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(synonyms)\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "    sentence = ' '.join(new_words)\n",
    "    return sentence\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lem in syn.lemmas():\n",
    "            synonym = lem.name()\n",
    "            if synonym != word:\n",
    "                synonyms.append(synonym)\n",
    "    return list(set(synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original sentence\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# Apply synonym replacement\n",
    "augmented_sentence = synonym_replacement(sentence, n=3)\n",
    "\n",
    "# Display results\n",
    "print(\"Original:\", sentence)\n",
    "print(\"Augmented:\", augmented_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- Replace words in the sentence with their synonyms.\n",
    "- This creates new sentences while preserving the original meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "# 4. Generative Models\n",
    "\n",
    "Generative models learn the underlying distribution of data and can generate new data samples. Two popular generative models are Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.1\"></a>\n",
    "## 4.1 Variational Autoencoders (VAEs)\n",
    "\n",
    "VAEs are probabilistic generative models that learn latent representations of data.\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "VAEs introduce a probabilistic encoder (recognition model) and decoder (generative model).\n",
    "\n",
    "**Encoder:**\n",
    "\n",
    "$[\n",
    "q_{\\phi}(\\mathbf{z} | \\mathbf{x}) \\approx p_{\\theta}(\\mathbf{z} | \\mathbf{x})\n",
    "]$\n",
    "\n",
    "**Decoder:**\n",
    "\n",
    "$[\n",
    "p_{\\theta}(\\mathbf{x} | \\mathbf{z})\n",
    "]$\n",
    "\n",
    "**Objective Function:**\n",
    "\n",
    "Maximize the Evidence Lower Bound (ELBO):\n",
    "\n",
    "$[\n",
    "\\mathcal{L}(\\theta, \\phi; \\mathbf{x}) = -\\text{KL}(q_{\\phi}(\\mathbf{z} | \\mathbf{x}) \\| p_{\\theta}(\\mathbf{z})) + \\mathbb{E}_{q_{\\phi}(\\mathbf{z} | \\mathbf{x})} [ \\log p_{\\theta}(\\mathbf{x} | \\mathbf{z}) ]\n",
    "]$\n",
    "\n",
    "- **KL Divergence**: Regularizes the latent space to match a prior distribution (e.g., standard normal).\n",
    "- **Reconstruction Loss**: Measures how well the decoder reconstructs the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)  # Mean\n",
    "        self.fc22 = nn.Linear(400, 20)  # Log variance\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h1 = torch.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h3 = torch.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# Loss function\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch}, Loss: {train_loss / len(train_loader.dataset):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic images\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(64, 20).to(device)\n",
    "    sample = model.decode(z).cpu()\n",
    "\n",
    "# Plot the generated images\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Generated Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(sample.view(64,1,28,28), padding=2, normalize=True), (1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- The VAE consists of an encoder, a reparameterization layer, and a decoder.\n",
    "- The loss function combines reconstruction loss and KL divergence.\n",
    "- After training, we sample from the latent space to generate new images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.2\"></a>\n",
    "## 4.2 Generative Adversarial Networks (GANs)\n",
    "\n",
    "GANs consist of two neural networks, a generator and a discriminator, competing against each other.\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "- **Generator (G)**: Tries to generate data that resembles the real data.\n",
    "- **Discriminator (D)**: Tries to distinguish between real data and generated data.\n",
    "\n",
    "**Objective Function:**\n",
    "\n",
    "$[\n",
    "\\min_G \\max_D V(D, G) = \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}(\\mathbf{x})} [\\log D(\\mathbf{x})] + \\mathbb{E}_{\\mathbf{z} \\sim p_{\\mathbf{z}}(\\mathbf{z})} [\\log(1 - D(G(\\mathbf{z})))]\n",
    "]$\n",
    "\n",
    "- **$( p_{\\text{data}}(\\mathbf{x}) )$**: Real data distribution.\n",
    "- **$( p_{\\mathbf{z}}(\\mathbf{z}) )$**: Prior noise distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Generator and Discriminator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(100, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 784),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(784, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Loss function and optimizers\n",
    "adversarial_loss = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for i, (imgs, _) in enumerate(train_loader):\n",
    "        # Adversarial ground truths\n",
    "        valid = torch.ones(imgs.size(0), 1, device=device)\n",
    "        fake = torch.zeros(imgs.size(0), 1, device=device)\n",
    "        \n",
    "        # Configure input\n",
    "        real_imgs = imgs.view(imgs.size(0), -1).to(device)\n",
    "        \n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # Sample noise as generator input\n",
    "        z = torch.randn(imgs.size(0), 100, device=device)\n",
    "        \n",
    "        # Generate images\n",
    "        gen_imgs = generator(z)\n",
    "        \n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "        \n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        \n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "    print(f'Epoch {epoch}, D loss: {d_loss.item():.4f}, G loss: {g_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic images\n",
    "generator.eval()\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(64, 100, device=device)\n",
    "    gen_imgs = generator(z).view(-1, 1, 28, 28).cpu()\n",
    "\n",
    "# Plot the generated images\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Generated Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(gen_imgs, padding=2, normalize=True), (1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- The generator and discriminator are trained in an adversarial manner.\n",
    "- The generator learns to produce images that the discriminator cannot distinguish from real images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "# 5. Synthetic Tabular Data Generation\n",
    "\n",
    "Generating synthetic tabular data poses unique challenges due to the complexity of relationships between features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.1\"></a>\n",
    "## 5.1 CTGAN\n",
    "\n",
    "CTGAN (Conditional Tabular GAN) [[2]](#ref2) is a GAN-based model designed specifically for generating synthetic tabular data, handling both continuous and categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install SDV (Synthetic Data Vault)\n",
    "!pip install sdv\n",
    "\n",
    "from sdv.tabular import CTGAN\n",
    "import pandas as pd\n",
    "\n",
    "# Load adult income dataset\n",
    "from sdv.datasets.demo import load_demo\n",
    "\n",
    "data = load_demo(metadata=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CTGAN model\n",
    "model = CTGAN()\n",
    "model.fit(data)\n",
    "\n",
    "# Generate synthetic data\n",
    "synthetic_data = model.sample(1000)\n",
    "\n",
    "# Compare real and synthetic data\n",
    "print('Real Data Sample:')\n",
    "print(data.head())\n",
    "print('\\nSynthetic Data Sample:')\n",
    "print(synthetic_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **SDV** provides tools for generating synthetic data using models like CTGAN.\n",
    "- **CTGAN** handles mixed data types and learns the distribution of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.2\"></a>\n",
    "## 5.2 Copulas\n",
    "\n",
    "Copulas are functions that allow modeling of the joint distribution of random variables by describing the dependencies between them separately from their marginal distributions.\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "Given cumulative distribution functions (CDFs) of random variables, copulas join them to form a multivariate CDF.\n",
    "\n",
    "**Sklar's Theorem** states that any multivariate joint distribution can be expressed in terms of univariate marginal distribution functions and a copula that captures the dependence structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Copulas package\n",
    "!pip install copulas\n",
    "\n",
    "from copulas.multivariate import GaussianMultivariate\n",
    "\n",
    "# Fit Copula model\n",
    "copula = GaussianMultivariate()\n",
    "copula.fit(data)\n",
    "\n",
    "# Generate synthetic data\n",
    "synthetic_data_copula = copula.sample(1000)\n",
    "\n",
    "# Compare real and synthetic data\n",
    "print('Real Data Sample:')\n",
    "print(data.head())\n",
    "print('\\nSynthetic Data Sample (Copula):')\n",
    "print(synthetic_data_copula.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- Copulas model dependencies between variables separately from their marginals.\n",
    "- **GaussianMultivariate** copula assumes a Gaussian dependence structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "# 6. Applications of Synthetic Data\n",
    "\n",
    "- **Privacy-Preserving Data Sharing**: Sharing synthetic data instead of real data to protect privacy.\n",
    "- **Balancing Imbalanced Datasets**: Generating samples for minority classes to improve model performance.\n",
    "- **Testing and Validation**: Creating datasets to test algorithms under various conditions.\n",
    "- **Simulation**: Modeling scenarios that are difficult or expensive to collect data for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "# 7. Latest Developments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7.1\"></a>\n",
    "## 7.1 Differential Privacy in Synthetic Data\n",
    "\n",
    "Differentially private synthetic data generation ensures that the synthetic data does not reveal sensitive information about any individual in the original dataset.\n",
    "\n",
    "### Differential Privacy Definition\n",
    "\n",
    "A mechanism $( \\mathcal{M} )$ is $( \\epsilon )$-differentially private if for all datasets $( D_1 )$ and $( D_2 )$ differing by one record, and all subsets $( S )$ of possible outputs:\n",
    "\n",
    "$[\n",
    "\\Pr[ \\mathcal{M}(D_1) \\in S ] \\leq e^{\\epsilon} \\Pr[ \\mathcal{M}(D_2) \\in S ]\n",
    "]$\n",
    "\n",
    "### Methods\n",
    "\n",
    "- **DP-SGD**: Incorporate noise into the optimization process.\n",
    "- **PATE-GAN** [[3]](#ref3): Uses Private Aggregation of Teacher Ensembles with GANs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7.2\"></a>\n",
    "## 7.2 Self-Supervised Learning\n",
    "\n",
    "Self-supervised learning leverages unlabeled data by generating labels from the data itself. Synthetic data can be used to create pretext tasks for self-supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Rotation Prediction\n",
    "\n",
    "- Rotate images by a certain angle.\n",
    "- Train a model to predict the rotation angle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rotation angles\n",
    "angles = [0, 90, 180, 270]\n",
    "\n",
    "# Prepare dataset\n",
    "class RotationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) * len(angles)\n",
    "    def __getitem__(self, idx):\n",
    "        img_idx = idx // len(angles)\n",
    "        angle_idx = idx % len(angles)\n",
    "        img, _ = self.dataset[img_idx]\n",
    "        rotated_img = transforms.functional.rotate(img, angles[angle_idx])\n",
    "        return rotated_img, angle_idx\n",
    "\n",
    "rotation_dataset = RotationDataset(train_dataset)\n",
    "rotation_loader = DataLoader(rotation_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define model\n",
    "class RotationNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RotationNet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 5 * 5, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 4)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = RotationNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data, target in rotation_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "    print(f'Epoch {epoch}, Loss: {train_loss / total:.4f}, Accuracy: {100. * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- The model learns to predict the rotation angle of images.\n",
    "- This self-supervised task helps the model learn useful feature representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>\n",
    "# 8. Conclusion\n",
    "\n",
    "Synthetic data generation is a powerful tool with a wide range of applications in machine learning and data science. From simple statistical methods to advanced generative models like GANs and VAEs, synthetic data can help overcome challenges related to data scarcity, privacy, and imbalance. Understanding the underlying mathematics and implementation details enables practitioners to effectively generate and utilize synthetic data in their projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"9\"></a>\n",
    "# 9. References\n",
    "\n",
    "1. <a id=\"ref1\"></a>Wei, J., & Zou, K. (2019). *EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks*. [arXiv:1901.11196](https://arxiv.org/abs/1901.11196)\n",
    "2. <a id=\"ref2\"></a>Xu, L., Skoularidou, M., Cuesta-Infante, A., & Veeramachaneni, K. (2019). *Modeling Tabular data using Conditional GAN*. In Advances in Neural Information Processing Systems (NeurIPS).\n",
    "3. <a id=\"ref3\"></a>Jordon, J., Yoon, J., & van der Schaar, M. (2019). *PATE-GAN: Generating Synthetic Data with Differential Privacy Guarantees*. In International Conference on Learning Representations (ICLR).\n",
    "\n",
    "---\n",
    "\n",
    "This notebook provides a comprehensive overview of synthetic data generation methods, including their mathematical foundations, implementations, and applications. You can run the code cells to see how these methods are applied and experiment with different datasets and parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
