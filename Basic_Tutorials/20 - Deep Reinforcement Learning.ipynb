{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Deep Reinforcement Learning (Deep RL) combines Reinforcement Learning (RL) with Deep Learning to create agents that can learn to make decisions by interacting with an environment. Deep RL has achieved remarkable success in various domains, including game playing, robotics, and autonomous vehicles.\n",
    "\n",
    "In this tutorial, we will explore the fundamentals of Deep RL, implement algorithms like Deep Q-Networks (DQN) and Policy Gradients, and understand the underlying mathematics. We will also dive into some of the latest developments in this field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Fundamentals of Reinforcement Learning](#1)\n",
    "   - [Markov Decision Processes](#1.1)\n",
    "   - [Key Concepts](#1.2)\n",
    "2. [Deep Reinforcement Learning](#2)\n",
    "   - [Combining Deep Learning and Reinforcement Learning](#2.1)\n",
    "3. [Deep Q-Networks (DQN)](#3)\n",
    "   - [Mathematical Foundations](#3.1)\n",
    "   - [Algorithm Explanation](#3.2)\n",
    "   - [Implementation](#3.3)\n",
    "4. [Policy Gradient Methods](#4)\n",
    "   - [Mathematical Foundations](#4.1)\n",
    "   - [Algorithm Explanation](#4.2)\n",
    "   - [Implementation](#4.3)\n",
    "5. [Latest Developments in Deep RL](#5)\n",
    "   - [Double DQN](#5.1)\n",
    "   - [Dueling DQN](#5.2)\n",
    "   - [Asynchronous Advantage Actor-Critic (A3C)](#5.3)\n",
    "   - [Proximal Policy Optimization (PPO)](#5.4)\n",
    "6. [Conclusion](#6)\n",
    "7. [References](#7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. Fundamentals of Reinforcement Learning\n",
    "\n",
    "Reinforcement Learning (RL) is a computational approach to learning from interaction. An agent learns to make decisions by performing actions in an environment to maximize cumulative rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.1\"></a>\n",
    "### Markov Decision Processes\n",
    "\n",
    "The RL problem is often formalized as a Markov Decision Process (MDP), defined by:\n",
    "\n",
    "- **States (S)**: The set of possible states the agent can be in.\n",
    "- **Actions (A)**: The set of actions the agent can take.\n",
    "- **Transition Probability (P)**: Probability of moving from one state to another given an action.\n",
    "- **Reward Function (R)**: Immediate reward received after transitioning from one state to another due to an action.\n",
    "- **Discount Factor (γ)**: A factor between 0 and 1 that reduces future rewards' importance.\n",
    "\n",
    "At each time step $( t )$, the agent observes a state $( s_t )$, takes an action $( a_t )$, receives a reward $( r_t )$, and transitions to a new state $( s_{t+1} )$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.2\"></a>\n",
    "### Key Concepts\n",
    "\n",
    "- **Policy (π)**: A mapping from states to actions. Determines the agent's behavior.\n",
    "- **Value Function (V)**: Estimates how good it is to be in a state, considering future rewards.\n",
    "- **Q-Function (Q)**: Estimates how good it is to take a specific action in a state.\n",
    "\n",
    "- **Objective**: Find a policy that maximizes the expected cumulative reward:\n",
    "\n",
    "$[\n",
    "J(\\pi) = \\mathbb{E}_{\\pi} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_t \\right]\n",
    "]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. Deep Reinforcement Learning\n",
    "\n",
    "Traditional RL methods struggle with high-dimensional state and action spaces. Deep Reinforcement Learning addresses this by using deep neural networks to approximate value functions and policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.1\"></a>\n",
    "### Combining Deep Learning and Reinforcement Learning\n",
    "\n",
    "- **Function Approximation**: Use neural networks to approximate value functions or policies.\n",
    "- **Experience Replay**: Store experiences and sample mini-batches to break correlations.\n",
    "- **Stability Techniques**: Use target networks, regularization, and other methods to stabilize training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. Deep Q-Networks (DQN)\n",
    "\n",
    "DQN is a seminal Deep RL algorithm that combines Q-Learning with deep neural networks to handle high-dimensional input spaces.\n",
    "\n",
    "**Reference:**\n",
    "\n",
    "- Mnih, V., et al. (2015). *Human-level control through deep reinforcement learning*. Nature, 518(7540), 529–533."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.1\"></a>\n",
    "### Mathematical Foundations\n",
    "\n",
    "**Q-Learning** aims to learn the optimal action-value function $( Q^*(s, a) )$, which satisfies the Bellman Equation:\n",
    "\n",
    "$[\n",
    "Q^*(s, a) = \\mathbb{E}_{s'} \\left[ r + \\gamma \\max_{a'} Q^*(s', a') \\mid s, a \\right]\n",
    "]$\n",
    "\n",
    "**Deep Q-Network** approximates $( Q^*(s, a) )$ using a neural network with parameters $( \\theta )$:\n",
    "\n",
    "$[\n",
    "Q(s, a; \\theta) \\approx Q^*(s, a)\n",
    "]$\n",
    "\n",
    "**Loss Function:**\n",
    "\n",
    "$[\n",
    "L(\\theta) = \\mathbb{E}_{(s, a, r, s')} \\left[ \\left( y_i - Q(s, a; \\theta) \\right)^2 \\right]\n",
    "]$\n",
    "\n",
    "Where:\n",
    "\n",
    "$[\n",
    "y_i = r + \\gamma \\max_{a'} Q(s', a'; \\theta^{-})\n",
    "]$\n",
    "\n",
    "- $( \\theta^{-} )$: Parameters of the target network (periodically updated)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.2\"></a>\n",
    "### Algorithm Explanation\n",
    "\n",
    "1. **Initialize** the Q-network $( Q(s, a; \\theta) )$ with random weights $( \\theta )$.\n",
    "2. **Initialize** the target network $( Q'(s, a; \\theta^{-}) )$ with weights $( \\theta^{-} = \\theta )$.\n",
    "3. **Initialize** the replay memory $( D )$.\n",
    "4. **For** each episode:\n",
    "   - **For** each step in the episode:\n",
    "     - Observe state $( s )$.\n",
    "     - Select action $( a )$ using an ε-greedy policy.\n",
    "     - Execute action $( a )$, observe reward $( r )$ and next state $( s' )$.\n",
    "     - Store transition $( (s, a, r, s') )$ in $( D )$.\n",
    "     - Sample mini-batch from $( D )$.\n",
    "     - Compute target $( y_i = r + \\gamma \\max_{a'} Q'(s', a'; \\theta^{-}) )$.\n",
    "     - Update $( \\theta )$ by minimizing the loss $( L(\\theta) )$.\n",
    "     - **Periodically** update $( \\theta^{-} = \\theta )$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.3\"></a>\n",
    "### Implementation\n",
    "\n",
    "We'll implement DQN using OpenAI Gym's CartPole environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from collections import deque\n",
    "\n",
    "# Set up the environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "env.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "# Define hyperparameters\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "batch_size = 64\n",
    "n_episodes = 500\n",
    "gamma = 0.99          # Discount factor\n",
    "epsilon = 1.0         # Exploration rate\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "learning_rate = 0.001\n",
    "memory = deque(maxlen=2000)\n",
    "\n",
    "# Build the Q-network\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(24, input_dim=state_size, activation='relu'))\n",
    "    model.add(layers.Dense(24, activation='relu'))\n",
    "    model.add(layers.Dense(action_size, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=optimizers.Adam(lr=learning_rate))\n",
    "    return model\n",
    "\n",
    "# Initialize networks\n",
    "model = build_model()\n",
    "target_model = build_model()\n",
    "target_model.set_weights(model.get_weights())\n",
    "\n",
    "# Function to choose an action\n",
    "def choose_action(state, epsilon):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return random.randrange(action_size)\n",
    "    act_values = model.predict(state)\n",
    "    return np.argmax(act_values[0])  # Returns action\n",
    "\n",
    "# Function to replay and train the network\n",
    "def replay(batch_size):\n",
    "    minibatch = random.sample(memory, batch_size)\n",
    "    for state, action, reward, next_state, done in minibatch:\n",
    "        target = model.predict(state)\n",
    "        if done:\n",
    "            target[0][action] = reward\n",
    "        else:\n",
    "            t = target_model.predict(next_state)[0]\n",
    "            target[0][action] = reward + gamma * np.amax(t)\n",
    "        model.fit(state, target, epochs=1, verbose=0)\n",
    "\n",
    "# Function to update the target network\n",
    "def update_target_model():\n",
    "    target_model.set_weights(model.get_weights())\n",
    "\n",
    "# Main training loop\n",
    "for e in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    for time in range(500):\n",
    "        # Uncomment to render the environment\n",
    "        # env.render()\n",
    "        action = choose_action(state, epsilon)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = reward if not done else -10\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(f\"Episode: {e}/{n_episodes}, Score: {time}, Epsilon: {epsilon:.2}\")\n",
    "            break\n",
    "        if len(memory) > batch_size:\n",
    "            replay(batch_size)\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "    update_target_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation uses experience replay and a target network for stable learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4. Policy Gradient Methods\n",
    "\n",
    "Policy gradient methods optimize the policy directly by adjusting parameters in the direction of greater expected reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.1\"></a>\n",
    "### Mathematical Foundations\n",
    "\n",
    "The objective is to maximize the expected return:\n",
    "\n",
    "$[\n",
    "J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_t \\right]\n",
    "]$\n",
    "\n",
    "Using the policy gradient theorem:\n",
    "\n",
    "$[\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) G_t \\right]\n",
    "]$\n",
    "\n",
    "Where $( G_t )$ is the return (cumulative future reward) from time step $( t )$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.2\"></a>\n",
    "### Algorithm Explanation\n",
    "\n",
    "1. **Initialize** policy network $( \\pi_\\theta(a|s) )$ with parameters $( \\theta )$.\n",
    "2. **Collect** episodes using the current policy.\n",
    "3. **Compute** returns $( G_t )$ for each time step.\n",
    "4. **Compute** policy gradient estimates:\n",
    "\n",
    "   $[\n",
    "   \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=0}^{T_i} \\nabla_\\theta \\log \\pi_\\theta(a_t^i | s_t^i) G_t^i\n",
    "   ]$\n",
    "\n",
    "5. **Update** policy parameters:\n",
    "\n",
    "   $[\n",
    "   \\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)\n",
    "   ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.3\"></a>\n",
    "### Implementation\n",
    "\n",
    "We'll implement the REINFORCE algorithm on the CartPole environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "\n",
    "# Set up the environment\n",
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "# Define the policy network\n",
    "def build_policy_network():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(24, input_dim=state_size, activation='relu'))\n",
    "    model.add(layers.Dense(24, activation='relu'))\n",
    "    model.add(layers.Dense(action_size, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "policy = build_policy_network()\n",
    "optimizer = optimizers.Adam(lr=0.01)\n",
    "\n",
    "# Function to select action based on policy probabilities\n",
    "def choose_action(state):\n",
    "    state = state.reshape([1, state_size])\n",
    "    probs = policy.predict(state).flatten()\n",
    "    action = np.random.choice(action_size, p=probs)\n",
    "    return action\n",
    "\n",
    "# Function to compute discounted rewards\n",
    "def discount_rewards(rewards, gamma=0.99):\n",
    "    discounted = np.zeros_like(rewards)\n",
    "    cumulative = 0\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        cumulative = cumulative * gamma + rewards[i]\n",
    "        discounted[i] = cumulative\n",
    "    return discounted\n",
    "\n",
    "# Main training loop\n",
    "episodes = 1000\n",
    "gamma = 0.99\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    states, actions, rewards = [], [], []\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        # env.render()\n",
    "        action = choose_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        states.append(state)\n",
    "        action_onehot = np.zeros(action_size)\n",
    "        action_onehot[action] = 1\n",
    "        actions.append(action_onehot)\n",
    "        rewards.append(reward)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    # Compute discounted rewards\n",
    "    discounted_rewards = discount_rewards(rewards, gamma)\n",
    "    # Convert lists to arrays\n",
    "    states = np.vstack(states)\n",
    "    actions = np.vstack(actions)\n",
    "    discounted_rewards = np.vstack(discounted_rewards)\n",
    "    # Normalize rewards\n",
    "    discounted_rewards = (discounted_rewards - np.mean(discounted_rewards)) / (np.std(discounted_rewards) + 1e-7)\n",
    "    # Train the policy network\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = policy(states)\n",
    "        neg_log_prob = tf.nn.softmax_cross_entropy_with_logits(labels=actions, logits=logits)\n",
    "        loss = tf.reduce_mean(neg_log_prob * discounted_rewards)\n",
    "    grads = tape.gradient(loss, policy.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, policy.trainable_variables))\n",
    "    print(f\"Episode: {episode}, Total Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this implementation, we collect episodes, compute discounted rewards, and update the policy network using the REINFORCE algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## 5. Latest Developments in Deep RL\n",
    "\n",
    "Deep RL has rapidly evolved, with numerous advancements improving stability, sample efficiency, and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.1\"></a>\n",
    "### 5.1 Double DQN\n",
    "\n",
    "Double DQN addresses the overestimation bias in Q-Learning by decoupling action selection from evaluation.\n",
    "\n",
    "**Reference:**\n",
    "\n",
    "- Hasselt, H. V., Guez, A., & Silver, D. (2016). *Deep Reinforcement Learning with Double Q-learning*. [arXiv:1509.06461](https://arxiv.org/abs/1509.06461)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.2\"></a>\n",
    "### 5.2 Dueling DQN\n",
    "\n",
    "Dueling DQN separates the estimation of state value and advantage, improving learning efficiency.\n",
    "\n",
    "**Reference:**\n",
    "\n",
    "- Wang, Z., et al. (2016). *Dueling Network Architectures for Deep Reinforcement Learning*. [arXiv:1511.06581](https://arxiv.org/abs/1511.06581)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.3\"></a>\n",
    "### 5.3 Asynchronous Advantage Actor-Critic (A3C)\n",
    "\n",
    "A3C uses multiple workers in parallel to stabilize and speed up training.\n",
    "\n",
    "**Reference:**\n",
    "\n",
    "- Mnih, V., et al. (2016). *Asynchronous Methods for Deep Reinforcement Learning*. [arXiv:1602.01783](https://arxiv.org/abs/1602.01783)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.4\"></a>\n",
    "### 5.4 Proximal Policy Optimization (PPO)\n",
    "\n",
    "PPO simplifies trust region policy optimization, balancing ease of implementation and performance.\n",
    "\n",
    "**Reference:**\n",
    "\n",
    "- Schulman, J., et al. (2017). *Proximal Policy Optimization Algorithms*. [arXiv:1707.06347](https://arxiv.org/abs/1707.06347)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "## 6. Conclusion\n",
    "\n",
    "Deep Reinforcement Learning combines the strengths of deep learning and reinforcement learning, enabling agents to learn complex tasks from raw sensory inputs. We explored foundational algorithms like DQN and policy gradients and touched upon advanced techniques that continue to push the boundaries of what's possible in AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "## 7. References\n",
    "\n",
    "1. Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction* (2nd ed.). MIT Press.\n",
    "2. Mnih, V., et al. (2015). *Human-level control through deep reinforcement learning*. Nature, 518(7540), 529–533.\n",
    "3. Schulman, J., et al. (2017). *Proximal Policy Optimization Algorithms*. [arXiv:1707.06347](https://arxiv.org/abs/1707.06347)\n",
    "4. Mnih, V., et al. (2016). *Asynchronous Methods for Deep Reinforcement Learning*. [arXiv:1602.01783](https://arxiv.org/abs/1602.01783)\n",
    "5. Hasselt, H. V., Guez, A., & Silver, D. (2016). *Deep Reinforcement Learning with Double Q-learning*. [arXiv:1509.06461](https://arxiv.org/abs/1509.06461)\n",
    "6. Wang, Z., et al. (2016). *Dueling Network Architectures for Deep Reinforcement Learning*. [arXiv:1511.06581](https://arxiv.org/abs/1511.06581)\n",
    "\n",
    "---\n",
    "\n",
    "This notebook provides an in-depth exploration of Deep Reinforcement Learning. You can run the code cells to see how DQN and policy gradient methods are implemented and experiment with the models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
