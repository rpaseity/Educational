{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning and Neural Networks\n",
    "\n",
    "The realm of artificial intelligence (AI) and machine learning (ML) has witnessed exponential growth over the past few decades. Central to this revolution are neural networks, computational models inspired by the human brain's architecture. These models have the remarkable ability to learn complex patterns from data, making them indispensable in tasks ranging from image recognition to natural language processing.\n",
    "\n",
    "In this tutorial, we'll delve deeper into the concepts introduced in our previous tutorial. We'll explore the mathematical underpinnings of neural networks, trace their historical development, and unpack the mechanisms that enable them to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamental Mathematics\n",
    "\n",
    "To truly understand neural networks, one must appreciate the mathematical principles that govern them. The core areas include linear algebra, calculus, and probability theory.\n",
    "\n",
    "## Linear Algebra\n",
    "\n",
    "**Historical Context**: Linear algebra's roots trace back to ancient civilizations' need to solve linear equations. However, it wasn't until the 19th century that mathematicians like Arthur Cayley formalized the study of matrices and determinants.\n",
    "\n",
    "**Vectors and Matrices**:\n",
    "\n",
    "- **Scalars**: Single numerical values.\n",
    "- **Vectors**: Ordered lists of numbers, representing points or directions in space.\n",
    "- **Matrices**: Two-dimensional arrays of numbers, essential for representing linear transformations.\n",
    "- **Tensors**: Generalizations of vectors and matrices to higher dimensions, crucial in deep learning for handling multidimensional data.\n",
    "\n",
    "**Key Operations**:\n",
    "\n",
    "- **Dot Product**: Measures the similarity between two vectors. Given two vectors\n",
    "\n",
    "\n",
    "$$\n",
    "( \\mathbf{a} ) and ( \\mathbf{b} )\n",
    "$$\n",
    "\n",
    ", their dot product is \n",
    "\n",
    "$$\n",
    "( \\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i} a_i b_i )\n",
    "$$\n",
    "\n",
    ".\n",
    "- **Matrix Multiplication**: Combines two matrices to produce a third matrix, following the rule \n",
    "\n",
    "$$\n",
    "( (AB)_{ij} = \\sum_{k} A_{ik} B_{kj} )\n",
    "$$\n",
    "\n",
    ".\n",
    "- **Transpose**: Flips a matrix over its diagonal, swapping rows with columns.\n",
    "\n",
    "These operations enable the compact representation and efficient computation of neural network layers.\n",
    "\n",
    "\n",
    "## Calculus\n",
    "\n",
    "**Historical Context**: Calculus was independently developed by Isaac Newton and Gottfried Wilhelm Leibniz in the 17th century. It provides tools to model and analyze continuous change.\n",
    "\n",
    "**Derivatives and Gradients**:\n",
    "\n",
    "- **Derivatives**: Measure how a function changes as its input changes. The derivative of $$( f(x) )$$ with respect to $$( x )$$ is $$( f'(x) = \\lim_{\\Delta x \\to 0} \\frac{f(x + \\Delta x) - f(x)}{\\Delta x} )$$.\n",
    "- **Gradients**: Generalize derivatives to multivariable functions. The gradient $$( \\nabla f )$$ is a vector of partial derivatives.\n",
    "\n",
    "**Chain Rule**: Essential for backpropagation, the chain rule states that the derivative of a composite function is the product of the derivatives of its constituent functions.\n",
    "\n",
    "## Probability Theory\n",
    "\n",
    "**Historical Context**: Probability theory emerged from the study of games of chance in the 16th and 17th centuries, with significant contributions from Pierre de Fermat and Blaise Pascal.\n",
    "\n",
    "**Core Concepts**:\n",
    "\n",
    "- **Random Variables**: Variables that can take on different values, each with an associated probability.\n",
    "- **Probability Distributions**: Functions that describe the likelihood of different outcomes (e.g., Gaussian distribution).\n",
    "- **Expectation and Variance**: The mean (expected value) and spread (variance) of a distribution.\n",
    "\n",
    "Probability theory helps in understanding uncertainties and modeling stochastic processes in neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Blocks of Basic Neural Networks\n",
    "\n",
    "## Artificial Neurons\n",
    "\n",
    "**Historical Context**: The concept of artificial neurons was first introduced by Warren McCulloch and Walter Pitts in 1943, modeling the human neuron's behavior using simple logic functions.\n",
    "![Artifical Neuron](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c6/Artificial_neuron_structure.svg/1280px-Artificial_neuron_structure.svg.png)\n",
    "\n",
    "*Image Source: [Wikipedia](https://en.wikipedia.org/wiki/Artificial_neuron)*\n",
    "**Mathematical Model**:\n",
    "\n",
    "An artificial neuron computes a weighted sum of its inputs and applies an activation function:\n",
    "\n",
    "$$\n",
    "z = \\sum_{i=1}^{n} w_i x_i + b\n",
    "$$\n",
    "\n",
    "- \\( x_i \\): Input features.\n",
    "- \\( w_i \\): Weights.\n",
    "- \\( b \\): Bias.\n",
    "- \\( z \\): Weighted sum.\n",
    "\n",
    "## Activation Functions\n",
    "\n",
    "Activation functions introduce non-linearity, allowing neural networks to model complex relationships.\n",
    "\n",
    "![Activation Functions](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZafDv3VUm60Eh10OeJu1vw.png)\n",
    "\n",
    "*Image Source: [Medium](https://medium.com/@shrutijadon/survey-on-activation-functions-for-deep-learning-9689331ba092)*\n",
    "\n",
    "- **Sigmoid**:\n",
    "  Introduced in the context of logistic regression, the sigmoid maps any real-valued number into the (0, 1) range:\n",
    "\n",
    "  $$\n",
    "  \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "  $$\n",
    "  Useful for binary classification tasks.\n",
    "\n",
    "- **ReLU (Rectified Linear Unit)**:\n",
    "  Popularized in the early 2010s, ReLU addresses the vanishing gradient problem:\n",
    "\n",
    "  $$\n",
    "  \\text{ReLU}(z) = \\max(0, z)\n",
    "  $$\n",
    "  It accelerates convergence in deep networks.\n",
    "\n",
    "- **Tanh**:\n",
    "  An alternative to the sigmoid, mapping inputs to (-1, 1):\n",
    "\n",
    "  $$\n",
    "  \\tanh(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation\n",
    "\n",
    "**Concept**: Forward propagation is the process by which input data passes through the network to generate an output.\n",
    "\n",
    "**Mathematical Representation**:\n",
    "\n",
    "1. **Compute Weighted Sum**:\n",
    "\n",
    "   $$\n",
    "   z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}\n",
    "   $$\n",
    "\n",
    "2. **Apply Activation Function**:\n",
    "\n",
    "   $$\n",
    "   a^{(l)} = \\phi(z^{(l)})\n",
    "   $$\n",
    "\n",
    "- \\( l \\): Layer index.\n",
    "- \\( W^{(l)} \\): Weights matrix for layer \\( l \\).\n",
    "- \\( a^{(l-1)} \\): Activations from the previous layer.\n",
    "- \\( b^{(l)} \\): Bias vector for layer \\( l \\).\n",
    "- \\( \\phi \\): Activation function.\n",
    "\n",
    "**Importance**: Forward propagation computes the network's prediction, which is then compared against the actual output to compute the loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "\n",
    "Loss functions measure how well the neural network's predictions align with the actual data.\n",
    "\n",
    "- **Mean Squared Error (MSE)**:\n",
    "  Commonly used in regression tasks:\n",
    "\n",
    "  $$\n",
    "  \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "  $$\n",
    "\n",
    "  - Actual value $$( y_i )$$\n",
    "  - Predicted value $$( \\hat{y}_i )$$\n",
    "\n",
    "- **Cross-Entropy Loss**:\n",
    "  Used in classification tasks, derived from information theory introduced by Claude Shannon:\n",
    "\n",
    "  $$\n",
    "  \\text{CE} = -\\sum_{i=1}^{n} y_i \\log(\\hat{y}_i)\n",
    "  $$\n",
    "  Measures the difference between two probability distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "**Historical Context**: Gradient descent was described by Cauchy in 1847. It's an optimization algorithm used to minimize functions.\n",
    "\n",
    "![Gradient Descent](https://miro.medium.com/v2/format:webp/1*f9a162GhpMbiTVTAua_lLQ.png)\n",
    "\n",
    "*Image Source: [Medium](https://medium.com/hackernoon/gradient-descent-aynk-7cbe95a778da)*\n",
    "\n",
    "**Mechanism**:\n",
    "\n",
    "- **Objective**: Find the parameters \\( w \\) that minimize the loss function \\( L(w) \\).\n",
    "- **Update Rule**:\n",
    "\n",
    "$$\n",
    "w := w - \\eta \\nabla_w L(w)\n",
    "$$\n",
    "\n",
    "- Weights $$( w )$$\n",
    "- Learning rate $$( \\eta )$$\n",
    "- Gradient of the loss with respect to the weights. $$( \\nabla_w L(w) )$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "**Historical Context**: Backpropagation, popularized by Rumelhart, Hinton, and Williams in 1986, is a method to compute gradients efficiently.\n",
    "\n",
    "![Backpropegation](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0qt5O-9iHj6PVMPm.png)\n",
    "\n",
    "*Image Source: [Medium](https://randomresearchai.medium.com/backpropagation-high-school-student-edition-11c8f77419c9)*\n",
    "\n",
    "**Mechanism**:\n",
    "\n",
    "1. **Compute Output Error**:\n",
    "\n",
    "   $$\n",
    "   \\delta^{(L)} = a^{(L)} - y\n",
    "   $$\n",
    "   - Output activations. $$( a^{(L)} )$$\n",
    "   - Actual outputs. $$( y )$$\n",
    "\n",
    "2. **Propagate Error Backwards**:\n",
    "\n",
    "   $$\n",
    "   \\delta^{(l)} = (W^{(l+1)})^T \\delta^{(l+1)} * \\phi'(z^{(l)})\n",
    "   $$\n",
    "\n",
    "   - Element-wise multiplication. $$( \\odot )$$\n",
    "   - Derivative of the activation function. $$( \\phi'(z^{(l)}) )$$\n",
    "\n",
    "3. **Compute Gradients**:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial W^{(l)}} = \\delta^{(l)} (a^{(l-1)})^T\n",
    "   $$\n",
    "\n",
    "**Importance**: Backpropagation leverages the chain rule to efficiently compute gradients for all network parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Neural Networks\n",
    "\n",
    "![Neural Networks](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*cuTSPlTq0a_327iTPJyD-Q.png)\n",
    "\n",
    "*Image Source: [Towards Data Science](https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464)*\n",
    "\n",
    "## Feedforward Neural Networks (FNN)\n",
    "\n",
    "**Description**: The simplest form, where data moves in one direction from input to output.\n",
    "\n",
    "**Historical Context**: Early neural networks, like the perceptron developed by Frank Rosenblatt in 1957, were feedforward.\n",
    "\n",
    "**Use Cases**:\n",
    "\n",
    "- Regression tasks (e.g., predicting continuous values).\n",
    "- Classification tasks (e.g., spam detection).\n",
    "\n",
    "## Convolutional Neural Networks (CNN)\n",
    "\n",
    "**Description**: Incorporate convolutional layers to process data with a grid-like topology, such as images.\n",
    "\n",
    "**Historical Context**:\n",
    "\n",
    "- Introduced by Yann LeCun in the late 1980s, notably with the LeNet architecture for digit recognition.\n",
    "- Revolutionized computer vision tasks in the 2010s with architectures like AlexNet, VGG, and ResNet.\n",
    "\n",
    "**Key Components**:\n",
    "\n",
    "- **Convolutional Layers**: Apply filters to detect local patterns.\n",
    "- **Pooling Layers**: Downsample spatial dimensions.\n",
    "\n",
    "**Use Cases**:\n",
    "\n",
    "- Image and video recognition.\n",
    "- Object detection.\n",
    "\n",
    "## Recurrent Neural Networks (RNN)\n",
    "\n",
    "**Description**: Designed to handle sequential data by maintaining a hidden state.\n",
    "\n",
    "**Historical Context**:\n",
    "\n",
    "- Developed in the 1980s but faced challenges like vanishing gradients.\n",
    "- Enhanced with architectures like LSTM (Long Short-Term Memory) by Hochreiter and Schmidhuber in 1997.\n",
    "\n",
    "**Use Cases**:\n",
    "\n",
    "- Language modeling.\n",
    "- Time series prediction.\n",
    "- Speech recognition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Cases of Different Neural Networks\n",
    "\n",
    "- **FNN**: Predicting house prices, classifying emails as spam or not spam.\n",
    "- **CNN**: Detecting objects in images, facial recognition.\n",
    "- **RNN**: Language translation, speech recognition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Implementation: A Simple Neural Network in NumPy\n",
    "\n",
    "To solidify our understanding, let's implement a simple neural network from scratch using NumPy.\n",
    "\n",
    "**Problem Statement**: Classify points in a 2D space into two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate dummy data\n",
    "np.random.seed(0)\n",
    "N = 100  # number of points per class\n",
    "D = 2    # dimensionality\n",
    "K = 2    # number of classes\n",
    "X = np.zeros((N*K, D))\n",
    "y = np.zeros(N*K, dtype='uint8')\n",
    "\n",
    "for j in range(K):\n",
    "    ix = range(N*j, N*(j+1))\n",
    "    X[ix] = np.random.randn(N, D) + np.array([j*2, j*2])\n",
    "    y[ix] = j\n",
    "\n",
    "# Visualize the data\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.title(\"Data Visualization\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Network Initialization**\n",
    "\n",
    "We set up a simple network with one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters randomly\n",
    "h = 100  # size of hidden layer\n",
    "W = 0.01 * np.random.randn(D, h)\n",
    "b = np.zeros((1, h))\n",
    "W2 = 0.01 * np.random.randn(h, K)\n",
    "b2 = np.zeros((1, K))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Training Loop**\n",
    "\n",
    "We train the network using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "step_size = 1e-0\n",
    "reg = 1e-3  # regularization strength\n",
    "\n",
    "num_examples = X.shape[0]\n",
    "\n",
    "for i in range(10000):\n",
    "\n",
    "    # Forward pass\n",
    "    hidden_layer = np.maximum(0, np.dot(X, W) + b)  # ReLU activation\n",
    "    scores = np.dot(hidden_layer, W2) + b2\n",
    "\n",
    "    # Compute class probabilities\n",
    "    exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))  # for numerical stability\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "    # Compute the loss\n",
    "    correct_logprobs = -np.log(probs[range(num_examples), y])\n",
    "    data_loss = np.sum(correct_logprobs) / num_examples\n",
    "    reg_loss = 0.5 * reg * (np.sum(W * W) + np.sum(W2 * W2))\n",
    "    loss = data_loss + reg_loss\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Iteration {i}: Loss {loss}\")\n",
    "\n",
    "    # Backpropagation\n",
    "    dscores = probs\n",
    "    dscores[range(num_examples), y] -= 1\n",
    "    dscores /= num_examples\n",
    "\n",
    "    dW2 = np.dot(hidden_layer.T, dscores)\n",
    "    db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "\n",
    "    dhidden = np.dot(dscores, W2.T)\n",
    "    dhidden[hidden_layer <= 0] = 0\n",
    "\n",
    "    dW = np.dot(X.T, dhidden)\n",
    "    db = np.sum(dhidden, axis=0, keepdims=True)\n",
    "\n",
    "    # Add regularization gradient\n",
    "    dW2 += reg * W2\n",
    "    dW += reg * W\n",
    "\n",
    "    # Update parameters\n",
    "    W += -step_size * dW\n",
    "    b += -step_size * db\n",
    "    W2 += -step_size * dW2\n",
    "    b2 += -step_size * db2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Evaluation**\n",
    "\n",
    "We assess the network's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate training set accuracy\n",
    "hidden_layer = np.maximum(0, np.dot(X, W) + b)\n",
    "scores = np.dot(hidden_layer, W2) + b2\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "print(f'Training accuracy: {np.mean(predicted_class == y)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Neural networks, inspired by the intricate workings of the human brain, have transformed the landscape of machine learning. By understanding the mathematics and mechanics behind them, we equip ourselves to harness their full potential.\n",
    "\n",
    "From the foundational perceptron to advanced architectures like CNNs and RNNs, neural networks continue to evolve, driven by both theoretical advancements and practical applications. As we move forward, they will undoubtedly play a pivotal role in shaping the future of AI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Reading\n",
    "\n",
    "- **Books**:\n",
    "\n",
    "  - *Deep Learning* by Ian Goodfellow, Yoshua Bengio, and Aaron Courville: A comprehensive resource covering a wide range of deep learning topics.\n",
    "\n",
    "- **Online Courses**:\n",
    "\n",
    "  - [Andrew Ng's Machine Learning Course](https://www.coursera.org/learn/machine-learning): A foundational course that introduces key concepts in ML.\n",
    "\n",
    "  - [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning): Delves deeper into neural networks and their applications.\n",
    "\n",
    "- **Frameworks and Libraries**:\n",
    "\n",
    "  - **TensorFlow**: An open-source platform by Google for machine learning.\n",
    "\n",
    "  - **PyTorch**: A flexible deep learning framework favored for research and development.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. **Neural Networks and Deep Learning**: [http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/)\n",
    "\n",
    "2. **CS231n: Convolutional Neural Networks for Visual Recognition**: [http://cs231n.github.io/](http://cs231n.github.io/)\n",
    "\n",
    "3. Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). *Learning representations by back-propagating errors*. Nature, 323(6088), 533-536.\n",
    "\n",
    "4. LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). *Gradient-based learning applied to document recognition*. Proceedings of the IEEE, 86(11), 2278-2324.\n",
    "\n",
    "5. Hochreiter, S., & Schmidhuber, J. (1997). *Long short-term memory*. Neural computation, 9(8), 1735-1780.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This tutorial provides a foundational understanding suitable for beginners in machine learning and neural networks. For advanced topics, consider exploring specialized architectures and training techniques.*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
