{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection using Autoencoders and GANs\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Anomaly detection is a critical aspect of data analysis, aiming to identify unusual patterns or outliers in datasets that do not conform to expected behavior. These anomalies can indicate significant events, such as fraud, system failures, or cyber-attacks. Leveraging deep learning techniques like Autoencoders and Generative Adversarial Networks (GANs), we can build robust models for detecting anomalies by learning data representations and identifying deviations.\n",
    "\n",
    "In this tutorial, we'll explore how to detect anomalies using Autoencoders and GANs. We'll delve into the underlying mathematics, provide example code, and explain the processes involved. We'll reference key papers and discuss the latest developments in the field. Relevant imagery will be included to enhance understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Understanding Anomaly Detection](#1)\n",
    "   - [What is Anomaly Detection?](#1.1)\n",
    "   - [Applications](#1.2)\n",
    "2. [Autoencoders for Anomaly Detection](#2)\n",
    "   - [Underlying Mathematics](#2.1)\n",
    "   - [Implementation](#2.2)\n",
    "3. [GANs for Anomaly Detection](#3)\n",
    "   - [Underlying Mathematics](#3.1)\n",
    "   - [Implementation](#3.2)\n",
    "4. [Latest Developments](#4)\n",
    "   - [Variational Autoencoders (VAEs)](#4.1)\n",
    "   - [Adversarial Autoencoders](#4.2)\n",
    "5. [Conclusion](#5)\n",
    "6. [References](#6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# 1. Understanding Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.1\"></a>\n",
    "## 1.1 What is Anomaly Detection?\n",
    "\n",
    "Anomaly detection refers to the identification of items, events, or observations that do not conform to an expected pattern or other items in a dataset. Anomalies are also known as outliers, novelties, noise, deviations, or exceptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Anomalies\n",
    "\n",
    "- **Point Anomalies**: Individual data instances that are anomalous with respect to the rest of the data.\n",
    "- **Contextual Anomalies**: Data instances that are anomalous in a specific context (e.g., time series data).\n",
    "- **Collective Anomalies**: A collection of related data instances that are anomalous together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.2\"></a>\n",
    "## 1.2 Applications\n",
    "\n",
    "- **Fraud Detection**: Identifying fraudulent transactions in finance.\n",
    "- **Cybersecurity**: Detecting intrusions and malicious activities in networks.\n",
    "- **Healthcare**: Identifying abnormal patterns in medical data.\n",
    "- **Manufacturing**: Detecting defects or faults in production processes.\n",
    "- **Environmental Monitoring**: Identifying unusual patterns in climate data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "# 2. Autoencoders for Anomaly Detection\n",
    "\n",
    "Autoencoders are unsupervised neural network models that learn to reconstruct input data by compressing it into a lower-dimensional representation and then reconstructing it back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.1\"></a>\n",
    "## 2.1 Underlying Mathematics\n",
    "\n",
    "An autoencoder consists of two main components:\n",
    "\n",
    "- **Encoder**: Maps the input data \\( \\mathbf{x} \\) to a latent representation \\( \\mathbf{z} \\).\n",
    "- **Decoder**: Reconstructs the input data from the latent representation.\n",
    "\n",
    "### Encoder Function\n",
    "\n",
    "$[\n",
    "\\mathbf{z} = f(\\mathbf{x}) = \\sigma(W_e \\mathbf{x} + \\mathbf{b}_e)\n",
    "]$\n",
    "\n",
    "### Decoder Function\n",
    "\n",
    "$[\n",
    "\\hat{\\mathbf{x}} = g(\\mathbf{z}) = \\sigma(W_d \\mathbf{z} + \\mathbf{b}_d)\n",
    "]$\n",
    "\n",
    "- $( \\sigma )$: Activation function (e.g., ReLU, sigmoid).\n",
    "- $( W_e, W_d )$: Weight matrices for encoder and decoder.\n",
    "- $( \\mathbf{b}_e, \\mathbf{b}_d )$: Bias vectors.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "The autoencoder is trained to minimize the reconstruction error between the input $( \\mathbf{x} )$ and the reconstruction $( \\hat{\\mathbf{x}} )$:\n",
    "\n",
    "$[\n",
    "\\mathcal{L}(\\mathbf{x}, \\hat{\\mathbf{x}}) = \\| \\mathbf{x} - \\hat{\\mathbf{x}} \\|^2\n",
    "]$\n",
    "\n",
    "### Anomaly Detection Principle\n",
    "\n",
    "- **Normal Data**: The autoencoder learns to reconstruct normal data with low error.\n",
    "- **Anomalous Data**: Reconstruction error is higher for anomalies, as they differ from the patterns learned during training.\n",
    "\n",
    "By setting a threshold on the reconstruction error, we can classify data points as normal or anomalous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.2\"></a>\n",
    "## 2.2 Implementation\n",
    "\n",
    "We'll implement an autoencoder for anomaly detection using the MNIST dataset. We'll simulate anomalies by introducing corrupted images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "\n",
    "# We will use digit '0' as normal data and others as anomalies\n",
    "class MNISTAnomalyDataset(Dataset):\n",
    "    def __init__(self, dataset, normal_digit=0):\n",
    "        self.normal_data = []\n",
    "        self.anomalous_data = []\n",
    "        for img, label in dataset:\n",
    "            if label == normal_digit:\n",
    "                self.normal_data.append((img, 0))  # Label 0 for normal\n",
    "            else:\n",
    "                self.anomalous_data.append((img, 1))  # Label 1 for anomaly\n",
    "    def __len__(self):\n",
    "        return len(self.normal_data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.normal_data[idx]\n",
    "\n",
    "# Create datasets\n",
    "normal_train_dataset = MNISTAnomalyDataset(train_dataset)\n",
    "normal_test_dataset = MNISTAnomalyDataset(test_dataset)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(dataset=normal_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=normal_test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 12),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(12, 3)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 12),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 28 * 28),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "model = Autoencoder().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Autoencoder\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for data, _ in train_loader:\n",
    "        img = data.view(data.size(0), -1).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(img)\n",
    "        loss = criterion(output, img)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * img.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Reconstruction\n",
    "\n",
    "Let's visualize some reconstructed images to see how well the autoencoder performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display original and reconstructed images\n",
    "model.eval()\n",
    "dataiter = iter(test_loader)\n",
    "images, _ = dataiter.next()\n",
    "images = images.view(images.size(0), -1).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "\n",
    "images = images.view(-1, 1, 28, 28).cpu().numpy()\n",
    "outputs = outputs.view(-1, 1, 28, 28).cpu().numpy()\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(nrows=2, ncols=8, sharex=True, sharey=True, figsize=(12,4))\n",
    "for images_row, row in zip([images, outputs], axes):\n",
    "    for img, ax in zip(images_row, row):\n",
    "        ax.imshow(img.squeeze(), cmap='gray')\n",
    "        ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection on Test Data\n",
    "\n",
    "We will use the reconstruction error to detect anomalies. Since the autoencoder was trained only on normal data (digit '0'), it should have higher reconstruction error on anomalous data (other digits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data with anomalies\n",
    "test_data = []\n",
    "test_labels = []\n",
    "for img, label in test_dataset:\n",
    "    test_data.append(img)\n",
    "    test_labels.append(0 if label == 0 else 1)  # Label 0: normal, 1: anomaly\n",
    "\n",
    "test_data = torch.stack(test_data)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Compute reconstruction errors\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_data_flat = test_data.view(test_data.size(0), -1).to(device)\n",
    "    reconstructions = model(test_data_flat)\n",
    "    mse = torch.mean((test_data_flat - reconstructions) ** 2, dim=1).cpu().numpy()\n",
    "\n",
    "# Set threshold\n",
    "threshold = np.percentile(mse, 95)  # Adjust percentile as needed\n",
    "\n",
    "# Predict anomalies\n",
    "predictions = (mse > threshold).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print('Classification Report:')\n",
    "print(classification_report(test_labels, predictions, target_names=['Normal', 'Anomaly']))\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- We prepare the test data containing both normal and anomalous samples.\n",
    "- Compute the reconstruction error (mean squared error) for each sample.\n",
    "- Set a threshold based on the reconstruction error distribution.\n",
    "- Predict anomalies by comparing reconstruction errors to the threshold.\n",
    "- Evaluate the model using classification metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "# 3. GANs for Anomaly Detection\n",
    "\n",
    "Generative Adversarial Networks (GANs) are composed of two neural networks, a generator and a discriminator, competing in a zero-sum game. GANs can be used for anomaly detection by learning the data distribution and identifying samples that do not conform to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.1\"></a>\n",
    "## 3.1 Underlying Mathematics\n",
    "\n",
    "### GAN Architecture\n",
    "\n",
    "- **Generator (G)**: Attempts to produce data that is indistinguishable from real data.\n",
    "- **Discriminator (D)**: Attempts to distinguish between real data and data produced by the generator.\n",
    "\n",
    "### Loss Functions\n",
    "\n",
    "The GAN is trained using the following minimax game:\n",
    "\n",
    "$[\n",
    "\\min_G \\max_D \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}(\\mathbf{x})} [\\log D(\\mathbf{x})] + \\mathbb{E}_{\\mathbf{z} \\sim p_{\\mathbf{z}}(\\mathbf{z})} [\\log (1 - D(G(\\mathbf{z})))]\n",
    "]$\n",
    "\n",
    "- $( p_{\\text{data}}(\\mathbf{x}) )$: Real data distribution.\n",
    "- $( p_{\\mathbf{z}}(\\mathbf{z}) )$: Prior noise distribution (e.g., Gaussian).\n",
    "\n",
    "### Anomaly Detection Principle\n",
    "\n",
    "- **Train GAN on Normal Data**: The generator learns to produce data similar to normal data.\n",
    "- **Anomaly Score**: Measure how well a sample fits into the learned data distribution.\n",
    "\n",
    "Common methods for anomaly scoring with GANs:\n",
    "\n",
    "- **Discriminator Score**: Use the output of the discriminator as an anomaly score.\n",
    "- **Reconstruction Error**: Combine generator and discriminator losses to compute an anomaly score.\n",
    "\n",
    "### AnoGAN [[1]](#ref1)\n",
    "\n",
    "- **Idea**: For a given test sample, find the closest point in the generator's latent space and measure the reconstruction error.\n",
    "- **Anomaly Score**:\n",
    "\n",
    "  $[\n",
    "  A(\\mathbf{x}) = (1 - \\lambda) \\| \\mathbf{x} - G(\\mathbf{z}^*) \\| + \\lambda \\cdot D(\\mathbf{x})\n",
    "  ]$\n",
    "\n",
    "  - $( \\mathbf{z}^* )$: Latent vector optimized to minimize reconstruction error.\n",
    "  - $( \\lambda )$: Weighting parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.2\"></a>\n",
    "## 3.2 Implementation\n",
    "\n",
    "We'll implement a simple GAN and use it for anomaly detection on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Generator and Discriminator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 28 * 28),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), 1, 28, 28)\n",
    "        return img\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.model(img_flat)\n",
    "        return validity\n",
    "\n",
    "latent_dim = 100\n",
    "generator = Generator(latent_dim).to(device)\n",
    "discriminator = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizers\n",
    "adversarial_loss = nn.BCELoss()\n",
    "\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=1e-3)\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the GAN\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (imgs, _) in enumerate(train_loader):\n",
    "        real_imgs = imgs.to(device)\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        valid = torch.ones(imgs.size(0), 1, device=device)\n",
    "        fake = torch.zeros(imgs.size(0), 1, device=device)\n",
    "        \n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # Sample noise as generator input\n",
    "        z = torch.randn(imgs.size(0), latent_dim, device=device)\n",
    "        \n",
    "        # Generate images\n",
    "        gen_imgs = generator(z)\n",
    "        \n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "        \n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        \n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Generated Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display images\n",
    "generator.eval()\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(16, latent_dim, device=device)\n",
    "    gen_imgs = generator(z).cpu()\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(nrows=2, ncols=8, sharex=True, sharey=True, figsize=(12,4))\n",
    "for img, ax in zip(gen_imgs, axes.flatten()):\n",
    "    ax.imshow(img.squeeze(), cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection\n",
    "\n",
    "We'll implement a simplified version of the AnoGAN method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly detection function\n",
    "def anomaly_score(x, generator, discriminator, lambda_=0.1, iterations=500):\n",
    "    z = torch.randn(1, latent_dim, requires_grad=True, device=device)\n",
    "    optimizer = torch.optim.Adam([z], lr=1e-2)\n",
    "    x = x.to(device)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        optimizer.zero_grad()\n",
    "        gen_x = generator(z)\n",
    "        residual_loss = torch.mean((gen_x - x) ** 2)\n",
    "        discrimination_loss = adversarial_loss(discriminator(gen_x), torch.ones(1, 1, device=device))\n",
    "        loss = (1 - lambda_) * residual_loss + lambda_ * discrimination_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    anomaly_score = loss.item()\n",
    "    return anomaly_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute anomaly scores\n",
    "anomaly_scores = []\n",
    "labels = []\n",
    "model.eval()\n",
    "for img, label in test_dataset:\n",
    "    score = anomaly_score(img.unsqueeze(0), generator, discriminator)\n",
    "    anomaly_scores.append(score)\n",
    "    labels.append(0 if label == 0 else 1)\n",
    "\n",
    "anomaly_scores = np.array(anomaly_scores)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Set threshold\n",
    "threshold = np.percentile(anomaly_scores, 95)  # Adjust percentile as needed\n",
    "\n",
    "# Predict anomalies\n",
    "predictions = (anomaly_scores > threshold).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "print('Classification Report:')\n",
    "print(classification_report(labels, predictions, target_names=['Normal', 'Anomaly']))\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- For each test image, we optimize a latent vector \\( \\mathbf{z} \\) to minimize the combined residual and discrimination losses.\n",
    "- Compute the anomaly score based on the final loss.\n",
    "- Set a threshold to classify samples as normal or anomalous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "# 4. Latest Developments\n",
    "\n",
    "Anomaly detection using deep learning continues to evolve, with new architectures and methods being proposed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.1\"></a>\n",
    "## 4.1 Variational Autoencoders (VAEs)\n",
    "\n",
    "VAEs [[2]](#ref2) introduce a probabilistic approach to autoencoders by learning a latent space that follows a predefined distribution (e.g., Gaussian). VAEs can be used for anomaly detection by comparing the likelihood of data points under the learned distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Concepts\n",
    "\n",
    "- **Encoder Outputs**: Mean $( \\mu )$ and standard deviation $( \\sigma )$ of the latent variables.\n",
    "- **Reparameterization Trick**: Allows backpropagation through stochastic nodes by expressing latent variables as $( \\mathbf{z} = \\mu + \\sigma \\odot \\epsilon )$, where $( \\epsilon \\sim \\mathcal{N}(0, 1) )$.\n",
    "- **Loss Function**: Combines reconstruction loss and Kullback-Leibler (KL) divergence:\n",
    "\n",
    "  $[\n",
    "  \\mathcal{L} = \\mathbb{E}_{q(\\mathbf{z}|\\mathbf{x})} [\\log p(\\mathbf{x}|\\mathbf{z})] - \\text{KL}(q(\\mathbf{z}|\\mathbf{x}) \\| p(\\mathbf{z}))\n",
    "  ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.2\"></a>\n",
    "## 4.2 Adversarial Autoencoders\n",
    "\n",
    "Adversarial Autoencoders [[3]](#ref3) combine autoencoders with adversarial training to match the aggregated posterior of the latent representation to a target distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Concepts\n",
    "\n",
    "- **Encoder and Decoder**: Similar to traditional autoencoders.\n",
    "- **Discriminator**: Trained to distinguish between encoded latent vectors and samples from the target distribution.\n",
    "- **Adversarial Loss**: Encourages the encoder to produce latent representations that match the target distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages for Anomaly Detection\n",
    "\n",
    "- By enforcing a specific distribution on the latent space, anomalies can be detected as samples that do not conform to this distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "# 5. Conclusion\n",
    "\n",
    "Anomaly detection is a vital task in various domains, and deep learning techniques like Autoencoders and GANs provide powerful tools for identifying anomalies in complex datasets. By leveraging reconstruction errors and adversarial training, we can build models that learn normal data patterns and detect deviations effectively. Understanding the underlying mathematics and implementation details enables practitioners to develop robust anomaly detection systems. The field continues to advance with new architectures like VAEs and Adversarial Autoencoders offering improved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "# 6. References\n",
    "\n",
    "1. <a id=\"ref1\"></a>Schlegl, T., Seeböck, P., Waldstein, S. M., Schmidt-Erfurth, U., & Langs, G. (2017). *Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery*. In Proceedings of the International Conference on Information Processing in Medical Imaging (IPMI).\n",
    "2. <a id=\"ref2\"></a>Kingma, D. P., & Welling, M. (2014). *Auto-Encoding Variational Bayes*. [arXiv:1312.6114](https://arxiv.org/abs/1312.6114)\n",
    "3. <a id=\"ref3\"></a>Makhzani, A., Shlens, J., Jaitly, N., Goodfellow, I., & Frey, B. (2016). *Adversarial Autoencoders*. [arXiv:1511.05644](https://arxiv.org/abs/1511.05644)\n",
    "\n",
    "---\n",
    "\n",
    "This notebook provides an in-depth exploration of anomaly detection using Autoencoders and GANs. You can run the code cells to see how these models are implemented and experiment with different datasets and parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
