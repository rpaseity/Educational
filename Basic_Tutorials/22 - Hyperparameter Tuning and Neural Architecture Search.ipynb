{
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.x",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Hyperparameter Tuning and Neural Architecture Search\n\n## Introduction\n\nOptimizing the performance of neural networks involves carefully selecting hyperparameters and designing effective architectures. Hyperparameter tuning and Neural Architecture Search (NAS) are crucial processes in machine learning that aim to automate and optimize these selections. This tutorial explores techniques for hyperparameter tuning, delves into automated architecture search methods, and provides practical examples with code implementations. We'll also discuss the underlying mathematics and reference key papers that have shaped this field."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Table of Contents\n\n1. [Understanding Hyperparameters](#1)\n   - [What are Hyperparameters?](#1.1)\n   - [Common Hyperparameters in Neural Networks](#1.2)\n2. [Hyperparameter Tuning Techniques](#2)\n   - [Grid Search](#2.1)\n   - [Random Search](#2.2)\n   - [Bayesian Optimization](#2.3)\n   - [Hyperband](#2.4)\n3. [Implementing Hyperparameter Tuning](#3)\n   - [Using Scikit-Learn](#3.1)\n   - [Using Keras Tuner](#3.2)\n4. [Neural Architecture Search (NAS)](#4)\n   - [What is NAS?](#4.1)\n   - [Reinforcement Learning for NAS](#4.2)\n   - [Evolutionary Algorithms for NAS](#4.3)\n5. [Implementing NAS with AutoKeras](#5)\n   - [AutoKeras Overview](#5.1)\n   - [Example Implementation](#5.2)\n6. [Latest Developments in NAS](#6)\n   - [Efficient NAS](#6.1)\n   - [Neural Architecture Optimization](#6.2)\n7. [Conclusion](#7)\n8. [References](#8)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"1\"></a>\n## 1. Understanding Hyperparameters\n\n<a id=\"1.1\"></a>\n### What are Hyperparameters?\n\nHyperparameters are configuration variables set before training a model. They are not learned from the data but significantly influence the model's performance. Examples include learning rates, number of layers, batch sizes, and activation functions.\n\n<a id=\"1.2\"></a>\n### Common Hyperparameters in Neural Networks\n\n- **Learning Rate (\\( \\eta \\))**: Controls the step size during optimization.\n- **Batch Size**: Number of samples processed before the model is updated.\n- **Number of Layers and Neurons**: Determines the depth and width of the network.\n- **Activation Functions**: Functions like ReLU, Sigmoid, or Tanh applied to neurons.\n- **Dropout Rate**: Fraction of neurons to drop during training to prevent overfitting.\n- **Weight Initialization Methods**: Strategies for initializing network weights.\n\n**Mathematical Representation of Learning Rate:**\n\nDuring gradient descent optimization, weights are updated as:\n\n\\[\n    w_{t+1} = w_t - \\eta \\nabla L(w_t)\n\\]\n\nWhere:\n\n- $( w_t )$: Current weights\n- $( \\eta )$: Learning rate\n- $( \\nabla L(w_t) )$: Gradient of the loss function"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"2\"></a>\n## 2. Hyperparameter Tuning Techniques\n\n<a id=\"2.1\"></a>\n### Grid Search\n\nGrid Search exhaustively tries all combinations from a predefined set of hyperparameter values.\n\n- **Advantages**:\n  - Simple to implement\n  - Guarantees finding the optimal combination within the grid\n- **Disadvantages**:\n  - Computationally expensive\n  - Not feasible with large numbers of hyperparameters\n\n<a id=\"2.2\"></a>\n### Random Search\n\nRandom Search samples hyperparameters randomly from a distribution.\n\n- **Advantages**:\n  - More efficient than Grid Search\n  - Can find good hyperparameters with fewer iterations\n- **Disadvantages**:\n  - May miss optimal values\n\n**Reference:**\n\n- Bergstra, J., & Bengio, Y. (2012). *Random Search for Hyper-Parameter Optimization*. Journal of Machine Learning Research, 13, 281–305."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"2.3\"></a>\n### Bayesian Optimization\n\nBayesian Optimization builds a probabilistic model to estimate the performance of hyperparameters and selects the next hyperparameters to evaluate based on past results.\n\n- **Advantages**:\n  - Efficient in exploring the hyperparameter space\n  - Incorporates prior knowledge\n- **Disadvantages**:\n  - Computational overhead in building the model\n\n**Mathematical Concept:**\n\nUses Gaussian Processes (GP) to model the objective function $( f )$. The next hyperparameter $( x )$ is chosen by maximizing the acquisition function $( a(x) )$:\n\n$[\n    x_{next} = \\arg\\max_{x} a(x | \\mathcal{D})\n]$\n\nWhere $( \\mathcal{D} )$ represents observed data."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"2.4\"></a>\n### Hyperband\n\nHyperband is a bandit-based approach that allocates resources to promising configurations.\n\n- **Advantages**:\n  - Efficient resource allocation\n  - Can handle a large number of hyperparameters\n- **Disadvantages**:\n  - Requires careful tuning of internal parameters\n\n**Reference:**\n\n- Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., & Talwalkar, A. (2018). *Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization*. Journal of Machine Learning Research, 18(185), 1–52."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"3\"></a>\n## 3. Implementing Hyperparameter Tuning\n\n<a id=\"3.1\"></a>\n### Using Scikit-Learn\n\nScikit-Learn provides tools for Grid Search and Random Search.\n\n**Example: Grid Search with Scikit-Learn**"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Import necessary libraries\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\n# Load data\ndata = load_iris()\nX = data.data\ny = data.target\n\n# Define the parameter grid\nparam_grid = {\n    'C': [0.1, 1, 10, 100],\n    'gamma': [1, 0.1, 0.01, 0.001],\n    'kernel': ['rbf', 'linear']\n}\n\n# Create a base model\nsvc = SVC()\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator=svc, param_grid=param_grid, cv=5, \n                           n_jobs=-1, verbose=2)\n\n# Fit the grid search to the data\ngrid_search.fit(X, y)\n\n# Best parameters\nprint(\"Best Parameters:\", grid_search.best_params_)\n\n# Best estimator\nbest_model = grid_search.best_estimator_",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"3.2\"></a>\n### Using Keras Tuner\n\nKeras Tuner is a library for hyperparameter tuning with TensorFlow and Keras.\n\n**Example: Hyperparameter Tuning with Keras Tuner**"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Install Keras Tuner (if not already installed)\n# !pip install keras-tuner\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom kerastuner import RandomSearch\n\n# Load data (e.g., MNIST)\n(x_train, y_train), (x_val, y_val) = tf.keras.datasets.mnist.load_data()\nx_train = x_train.astype('float32') / 255.0\nx_val = x_val.astype('float32') / 255.0\n\n# Define the model builder function\ndef build_model(hp):\n    model = keras.Sequential()\n    model.add(keras.layers.Flatten(input_shape=(28, 28)))\n    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n    model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n    model.add(keras.layers.Dense(10, activation='softmax'))\n    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model\n\n# Instantiate the tuner\ntuner = RandomSearch(\n    build_model,\n    objective='val_accuracy',\n    max_trials=5,\n    executions_per_trial=3,\n    directory='my_dir',\n    project_name='helloworld')\n\n# Perform hyperparameter search\ntuner.search(x_train, y_train,\n             epochs=5,\n             validation_data=(x_val, y_val))\n\n# Get the optimal hyperparameters\nbest_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\nprint(f\"Best number of units: {best_hps.get('units')}\")\nprint(f\"Best learning rate: {best_hps.get('learning_rate')}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"4\"></a>\n## 4. Neural Architecture Search (NAS)\n\n<a id=\"4.1\"></a>\n### What is NAS?\n\nNeural Architecture Search automates the process of designing neural network architectures.\n\n- **Goal**: Find optimal architectures without manual intervention.\n- **Challenges**: High computational cost and large search space.\n\n**Reference:**\n\n- Zoph, B., & Le, Q. V. (2017). *Neural Architecture Search with Reinforcement Learning*. [arXiv:1611.01578](https://arxiv.org/abs/1611.01578)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"4.2\"></a>\n### Reinforcement Learning for NAS\n\n- **Controller RNN**: Generates architectural hyperparameters.\n- **Training Process**:\n  - Sample an architecture.\n  - Train it on the dataset.\n  - Use the performance as a reward to update the controller.\n\n**Mathematical Representation:**\n\nThe controller aims to maximize expected reward $( J(\\theta) )$:\n\n$[\n    J(\\theta) = \\mathbb{E}_{P(a; \\theta)} [R(a)]\n]$\n\n- $( \\theta )$: Parameters of the controller\n- $( a )$: Architecture sampled\n- $( R(a) )$: Reward (e.g., validation accuracy)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"4.3\"></a>\n### Evolutionary Algorithms for NAS\n\n- **Population-Based Methods**: Evolve architectures over generations.\n- **Operations**:\n  - Mutation\n  - Crossover\n- **Selection**: Based on fitness scores (e.g., accuracy).\n\n**Reference:**\n\n- Real, E., et al. (2019). *Regularized Evolution for Image Classifier Architecture Search*. [arXiv:1802.01548](https://arxiv.org/abs/1802.01548)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"5\"></a>\n## 5. Implementing NAS with AutoKeras\n\n<a id=\"5.1\"></a>\n### AutoKeras Overview\n\nAutoKeras is an open-source library for AutoML, which includes NAS capabilities.\n\n- **Features**:\n  - Automatic model search\n  - Easy-to-use interface\n- **Installation**:\n  - `pip install autokeras`\n\n<a id=\"5.2\"></a>\n### Example Implementation"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Install AutoKeras (if not already installed)\n# !pip install autokeras\n\nimport autokeras as ak\nimport tensorflow as tf\n\n# Load data (e.g., CIFAR-10)\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n\n# Normalize data\nx_train = x_train.astype('float32') / 255.0\nx_test = x_test.astype('float32') / 255.0\n\n# Define the model\nautokeras_model = ak.ImageClassifier(\n    max_trials=3,  # It tries 3 different models.\n    overwrite=True)\n\n# Fit the model\nautokeras_model.fit(x_train, y_train, epochs=5)\n\n# Evaluate the best model\naccuracy = autokeras_model.evaluate(x_test, y_test)\nprint(f\"Test accuracy: {accuracy}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Explanation**:\n\n- **max_trials**: Number of different models to try.\n- **ImageClassifier**: Automatically searches for the best model for image classification."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"6\"></a>\n## 6. Latest Developments in NAS\n\n<a id=\"6.1\"></a>\n### Efficient NAS\n\n**Efficient NAS** aims to reduce the computational cost of architecture search.\n\n- **Techniques**:\n  - Weight sharing among architectures\n  - One-shot models\n\n**Reference:**\n\n- Pham, H., et al. (2018). *Efficient Neural Architecture Search via Parameter Sharing*. [arXiv:1802.03268](https://arxiv.org/abs/1802.03268)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"6.2\"></a>\n### Neural Architecture Optimization\n\n**Neural Architecture Optimization** integrates NAS with network pruning and quantization.\n\n- **Goals**:\n  - Optimize architectures for deployment on resource-constrained devices.\n\n**Reference:**\n\n- Liu, C., et al. (2018). *Progressive Neural Architecture Search*. [arXiv:1712.00559](https://arxiv.org/abs/1712.00559)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"7\"></a>\n## 7. Conclusion\n\nHyperparameter tuning and Neural Architecture Search are vital for optimizing neural network performance. While hyperparameter tuning focuses on finding the best settings for predefined models, NAS automates the design of optimal architectures. With the advancement of tools like Keras Tuner and AutoKeras, these processes have become more accessible. Ongoing research continues to make these methods more efficient and practical for real-world applications."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"8\"></a>\n## 8. References\n\n1. Bergstra, J., & Bengio, Y. (2012). *Random Search for Hyper-Parameter Optimization*. Journal of Machine Learning Research, 13, 281–305.\n2. Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., & Talwalkar, A. (2018). *Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization*. Journal of Machine Learning Research, 18(185), 1–52.\n3. Zoph, B., & Le, Q. V. (2017). *Neural Architecture Search with Reinforcement Learning*. [arXiv:1611.01578](https://arxiv.org/abs/1611.01578)\n4. Real, E., et al. (2019). *Regularized Evolution for Image Classifier Architecture Search*. [arXiv:1802.01548](https://arxiv.org/abs/1802.01548)\n5. Pham, H., et al. (2018). *Efficient Neural Architecture Search via Parameter Sharing*. [arXiv:1802.03268](https://arxiv.org/abs/1802.03268)\n6. Liu, C., et al. (2018). *Progressive Neural Architecture Search*. [arXiv:1712.00559](https://arxiv.org/abs/1712.00559)\n\n---\n\nThis notebook provides a comprehensive overview of hyperparameter tuning and neural architecture search. You can run the code cells to see how these techniques are implemented and experiment with different settings and tools."
  }
 ]
}
