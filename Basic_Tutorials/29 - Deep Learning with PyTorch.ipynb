{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with PyTorch\n",
    "\n",
    "## Introduction\n",
    "\n",
    "PyTorch is an open-source deep learning framework that provides a seamless path from research to production. It is known for its flexibility, dynamic computation graphs, and intuitive API. This tutorial will explore how to implement advanced neural network models using PyTorch, understand dynamic computation graphs, and create custom layers.\n",
    "\n",
    "We will delve into the underlying mathematics, provide example code, and explain the processes involved. We will reference key papers and discuss some of the latest developments in this field. Relevant imagery will be included to enhance understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Understanding PyTorch](#1)\n",
    "   - [What is PyTorch?](#1.1)\n",
    "   - [Dynamic Computation Graphs](#1.2)\n",
    "2. [Implementing Neural Networks with PyTorch](#2)\n",
    "   - [Building a Simple Neural Network](#2.1)\n",
    "   - [Training and Evaluating the Model](#2.2)\n",
    "3. [Custom Layers and Modules](#3)\n",
    "   - [Creating Custom Layers](#3.1)\n",
    "   - [Creating Custom Modules](#3.2)\n",
    "4. [Advanced Models](#4)\n",
    "   - [Implementing a Residual Network (ResNet)](#4.1)\n",
    "   - [Understanding the Mathematics Behind ResNets](#4.2)\n",
    "5. [Latest Developments in PyTorch](#5)\n",
    "   - [PyTorch Lightning](#5.1)\n",
    "   - [TorchScript and JIT Compilation](#5.2)\n",
    "6. [Conclusion](#6)\n",
    "7. [References](#7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# 1. Understanding PyTorch\n",
    "\n",
    "PyTorch is a popular open-source deep learning framework developed by Facebook's AI Research lab. It provides a Python package for high-level neural network APIs and is known for its flexibility and speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.1\"></a>\n",
    "## 1.1 What is PyTorch?\n",
    "\n",
    "PyTorch is a Python-based scientific computing package that uses the power of graphics processing units (GPUs). It is primarily used for:\n",
    "\n",
    "- **Tensor computation** (like NumPy) with strong GPU acceleration.\n",
    "- **Deep neural networks** built on a tape-based autograd system.\n",
    "\n",
    "PyTorch supports dynamic computation graphs, allowing network behavior to be changed programmatically at runtime. This is particularly useful for tasks where the input size or structure can vary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.2\"></a>\n",
    "## 1.2 Dynamic Computation Graphs\n",
    "\n",
    "Unlike static computation graphs used in frameworks like TensorFlow (prior to version 2.x), PyTorch builds the computation graph dynamically. This allows for more flexibility when building complex architectures.\n",
    "\n",
    "### Mathematical Background\n",
    "\n",
    "In traditional static graphs, you define the computation graph once, and it remains constant throughout training. In dynamic graphs, the graph is built on-the-fly during the forward pass.\n",
    "\n",
    "Consider the function:\n",
    "\n",
    "$[\n",
    "y = x^2 + 2x + 1\n",
    "]$\n",
    "\n",
    "In PyTorch, the computation graph for this function is created dynamically when you compute \\( y \\). This allows for constructs like loops and conditionals in your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic computation graph example in PyTorch\n",
    "import torch\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**2 + 2*x + 1\n",
    "\n",
    "y.backward()\n",
    "print(f'dy/dx at x=2 is {x.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- We define a tensor `x` with `requires_grad=True` to track computations.\n",
    "- The computation graph is built dynamically as we compute `y`.\n",
    "- Calling `y.backward()` computes the gradient of `y` with respect to `x`.\n",
    "- The gradient `dy/dx` at `x=2` is `6.0`, which matches the analytical derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "# 2. Implementing Neural Networks with PyTorch\n",
    "\n",
    "In this section, we'll implement a simple neural network using PyTorch and understand how to train and evaluate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.1\"></a>\n",
    "## 2.1 Building a Simple Neural Network\n",
    "\n",
    "We'll create a simple feedforward neural network to classify images from the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "input_size = 784  # 28x28\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                           train=True,\n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                          train=False,\n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "# Fully connected neural network class\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- We define a neural network with one hidden layer using `nn.Module`.\n",
    "- The `forward` method defines the forward pass.\n",
    "- We use ReLU activation and a linear output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.2\"></a>\n",
    "## 2.2 Training and Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Test the model with test data\n",
    "model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- We use `CrossEntropyLoss` as the loss function and `Adam` optimizer.\n",
    "- In each epoch, we perform the forward pass, compute the loss, backpropagate, and update the weights.\n",
    "- After training, we evaluate the model on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "# 3. Custom Layers and Modules\n",
    "\n",
    "PyTorch allows you to create custom layers and modules, enabling the implementation of novel architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.1\"></a>\n",
    "## 3.1 Creating Custom Layers\n",
    "\n",
    "Let's create a custom linear layer with weight normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(CustomLinear, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "        bound = 1 / math.sqrt(fan_in)\n",
    "        nn.init.uniform_(self.bias, -bound, bound)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        weight = nn.functional.normalize(self.weight, dim=1)\n",
    "        return nn.functional.linear(input, weight, self.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- We define a custom linear layer that normalizes its weights before performing the linear operation.\n",
    "- This can help with training stability and convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.2\"></a>\n",
    "## 3.2 Creating Custom Modules\n",
    "\n",
    "Custom modules can encapsulate complex layers or blocks. Let's create a custom module that combines convolutional layers with batch normalization and activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- The `ConvBlock` module performs convolution, batch normalization, and ReLU activation.\n",
    "- This block can be used to build more complex architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "# 4. Advanced Models\n",
    "\n",
    "In this section, we'll implement a Residual Network (ResNet) and understand the mathematics behind it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.1\"></a>\n",
    "## 4.1 Implementing a Residual Network (ResNet)\n",
    "\n",
    "ResNets [[1]](#ref1) are deep neural networks that use skip connections to mitigate the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=1000):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = ResNet(ResidualBlock, [2, 2, 2, 2])  # ResNet-18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- The `ResidualBlock` class defines the basic building block of ResNet.\n",
    "- The `ResNet` class assembles multiple residual blocks to form the network.\n",
    "- We instantiate ResNet-18 by specifying the number of layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.2\"></a>\n",
    "## 4.2 Understanding the Mathematics Behind ResNets\n",
    "\n",
    "### Residual Learning\n",
    "\n",
    "ResNets aim to learn the residual function \\( \\mathcal{F}(x) = H(x) - x \\), where \\( H(x) \\) is the desired mapping and \\( x \\) is the input. The original mapping becomes \\( H(x) = \\mathcal{F}(x) + x \\).\n",
    "\n",
    "This formulation helps in training deep networks by allowing the gradients to flow directly through the skip connections.\n",
    "\n",
    "### Identity Mapping\n",
    "\n",
    "The identity mapping in ResNets allows the network to preserve information and mitigate the vanishing gradient problem. The skip connection adds the input \\( x \\) to the output of the residual function \\( \\mathcal{F}(x) \\):\n",
    "\n",
    "\\[\n",
    "\\text{Output} = \\mathcal{F}(x, \\{W_i\\}) + x\n",
    "\\]\n",
    "\n",
    "Where \\( \\{W_i\\} \\) are the weights of the residual block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "![ResNet Block](https://pytorch.org/assets/images/resnet.png)\n",
    "\n",
    "*Figure: A residual block with a skip connection.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "# 5. Latest Developments in PyTorch\n",
    "\n",
    "PyTorch continues to evolve, introducing new features and improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.1\"></a>\n",
    "## 5.1 PyTorch Lightning\n",
    "\n",
    "PyTorch Lightning is a lightweight wrapper for PyTorch that helps organize code and reduce boilerplate. It abstracts away much of the training loop, enabling researchers to focus on the model and training logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch Lightning\n",
    "# !pip install pytorch-lightning\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(LitModel, self).__init__()\n",
    "        self.model = NeuralNet(input_size, hidden_size, num_classes)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        images = images.reshape(-1, 28*28)\n",
    "        outputs = self.model(images)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "# DataModule for data handling\n",
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def prepare_data(self):\n",
    "        torchvision.datasets.MNIST(root='./data', train=True, download=True)\n",
    "        torchvision.datasets.MNIST(root='./data', train=False, download=True)\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        transform = transforms.ToTensor()\n",
    "        self.train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform)\n",
    "        self.val_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.train_dataset, batch_size=batch_size)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Training\n",
    "mnist_dm = MNISTDataModule()\n",
    "model = LitModel()\n",
    "trainer = pl.Trainer(max_epochs=5)\n",
    "trainer.fit(model, mnist_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- `LitModel` defines the model and training logic.\n",
    "- `MNISTDataModule` handles data preparation and loading.\n",
    "- `trainer.fit` runs the training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.2\"></a>\n",
    "## 5.2 TorchScript and JIT Compilation\n",
    "\n",
    "TorchScript allows you to serialize and optimize models written in PyTorch, enabling them to run independently from Python. This is useful for deploying models in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of TorchScript\n",
    "\n",
    "# Define a simple model\n",
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.linear = torch.nn.Linear(10, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = MyModule()\n",
    "\n",
    "# Convert to TorchScript\n",
    "scripted_model = torch.jit.script(model)\n",
    "\n",
    "# Save the model\n",
    "scripted_model.save('model.pt')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = torch.jit.load('model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- `torch.jit.script` compiles the model to TorchScript.\n",
    "- The scripted model can be saved and loaded independently of Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "# 6. Conclusion\n",
    "\n",
    "PyTorch is a powerful and flexible deep learning framework that enables rapid experimentation and development of complex models. Its dynamic computation graphs, intuitive API, and extensive community support make it a preferred choice for researchers and practitioners. By understanding how to implement neural networks, create custom layers, and leverage advanced features like TorchScript, you can build efficient and scalable deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "# 7. References\n",
    "\n",
    "1. He, K., Zhang, X., Ren, S., & Sun, J. (2016). *Deep Residual Learning for Image Recognition*. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n",
    "2. Paszke, A., et al. (2019). *PyTorch: An Imperative Style, High-Performance Deep Learning Library*. [arXiv:1912.01703](https://arxiv.org/abs/1912.01703)\n",
    "3. PyTorch Official Documentation: [https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html)\n",
    "4. PyTorch Lightning Documentation: [https://www.pytorchlightning.ai/](https://www.pytorchlightning.ai/)\n",
    "\n",
    "---\n",
    "\n",
    "This notebook provides a comprehensive guide to implementing advanced neural network models using PyTorch. You can run the code cells to see how models are built, trained, and evaluated. Feel free to modify and extend the examples to suit your specific needs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
