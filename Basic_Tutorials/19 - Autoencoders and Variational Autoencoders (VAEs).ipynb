{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders and Variational Autoencoders (VAEs)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Autoencoders are a class of unsupervised learning algorithms used for representation learning, dimensionality reduction, and feature extraction. They work by compressing the input data into a lower-dimensional latent space and then reconstructing the original data from this representation. Variational Autoencoders (VAEs) [[1]](#ref1) extend this concept by introducing probabilistic generative models, allowing for more efficient data generation and interpolation.\n",
    "\n",
    "In this tutorial, we'll explore the architecture of autoencoders and VAEs, delve into their mathematical foundations, and implement them using TensorFlow and Keras. We'll also discuss some of the latest developments in this field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Understanding Autoencoders](#1)\n",
    "   - [Architecture](#1.1)\n",
    "   - [Mathematical Foundations](#1.2)\n",
    "2. [Implementing an Autoencoder](#2)\n",
    "   - [Dataset Preparation](#2.1)\n",
    "   - [Building the Autoencoder](#2.2)\n",
    "   - [Training the Autoencoder](#2.3)\n",
    "   - [Visualizing the Results](#2.4)\n",
    "3. [Variational Autoencoders (VAEs)](#3)\n",
    "   - [VAE Architecture](#3.1)\n",
    "   - [Mathematical Foundations](#3.2)\n",
    "4. [Implementing a Variational Autoencoder](#4)\n",
    "   - [Building the VAE](#4.1)\n",
    "   - [Training the VAE](#4.2)\n",
    "   - [Generating New Data](#4.3)\n",
    "5. [Latest Developments](#5)\n",
    "   - [Beta-VAE](#5.1)\n",
    "   - [Conditional VAEs](#5.2)\n",
    "   - [VQ-VAE (Vector Quantized VAE)](#5.3)\n",
    "6. [Conclusion](#6)\n",
    "7. [References](#7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. Understanding Autoencoders\n",
    "\n",
    "Autoencoders consist of two main components:\n",
    "\n",
    "- **Encoder**: Compresses the input data into a latent-space representation.\n",
    "- **Decoder**: Reconstructs the input data from the latent representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.1\"></a>\n",
    "### Architecture\n",
    "\n",
    "![Autoencoder Architecture](https://miro.medium.com/max/1400/1*BRBEthVexgTzI6OdSuR3VQ.png)\n",
    "\n",
    "*Image Source: [Medium](https://medium.com/)*\n",
    "\n",
    "- **Input Layer**: Receives the original data.\n",
    "- **Hidden Layers**: Maps the input to a latent representation.\n",
    "- **Latent Space**: The compressed representation of the input.\n",
    "- **Output Layer**: Attempts to reconstruct the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.2\"></a>\n",
    "### Mathematical Foundations\n",
    "\n",
    "Given input data $( \\mathbf{x} )$, the autoencoder aims to learn functions $( f )$ and $( g )$ such that:\n",
    "\n",
    "$[\n",
    "\\mathbf{z} = f(\\mathbf{x}) \\quad \\text{(Encoding)}\n",
    "]$\n",
    "$[\n",
    "\\hat{\\mathbf{x}} = g(\\mathbf{z}) \\quad \\text{(Decoding)}\n",
    "]$\n",
    "\n",
    "The objective is to minimize the reconstruction loss:\n",
    "\n",
    "$[\n",
    "\\mathcal{L}(\\mathbf{x}, \\hat{\\mathbf{x}}) = \\| \\mathbf{x} - \\hat{\\mathbf{x}} \\|^2\n",
    "]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. Implementing an Autoencoder\n",
    "\n",
    "We'll implement a simple autoencoder using the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.1\"></a>\n",
    "### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize and reshape the data\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
    "\n",
    "print('Training data shape:', x_train.shape)\n",
    "print('Test data shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.2\"></a>\n",
    "### Building the Autoencoder\n",
    "\n",
    "We'll use a convolutional autoencoder for better performance on image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "def build_encoder():\n",
    "    input_img = layers.Input(shape=(28, 28, 1))\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    return models.Model(input_img, x)\n",
    "\n",
    "# Decoder\n",
    "def build_decoder():\n",
    "    encoded_input = layers.Input(shape=(7, 7, 16))\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(encoded_input)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "    return models.Model(encoded_input, x)\n",
    "\n",
    "# Build the autoencoder\n",
    "encoder = build_encoder()\n",
    "decoder = build_decoder()\n",
    "autoencoder_input = encoder.input\n",
    "autoencoder_output = decoder(encoder.output)\n",
    "autoencoder = models.Model(autoencoder_input, autoencoder_output)\n",
    "\n",
    "# Compile the autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Model summary\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder compresses the input images to a latent representation of shape (7, 7, 16). The decoder reconstructs the images from this representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.3\"></a>\n",
    "### Training the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the autoencoder\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=10,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the autoencoder to minimize the reconstruction loss between the input and the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.4\"></a>\n",
    "### Visualizing the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode and decode some images\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)\n",
    "\n",
    "# Display original and reconstructed images\n",
    "n = 10  # Number of images to display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title('Original')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Reconstructed\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title('Reconstructed')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reconstructed images should resemble the original inputs, demonstrating that the autoencoder has learned an effective compression of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. Variational Autoencoders (VAEs)\n",
    "\n",
    "VAEs are generative models that learn a probabilistic mapping from a latent space to the data space. They are capable of generating new data samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.1\"></a>\n",
    "### VAE Architecture\n",
    "\n",
    "![VAE Architecture](https://miro.medium.com/max/1400/1*VZEHKY1RyqdW5bkj9IyHng.png)\n",
    "\n",
    "*Image Source: [Medium](https://medium.com/)*\n",
    "\n",
    "- **Encoder**: Maps input to a distribution over the latent space.\n",
    "- **Sampling Layer**: Samples a point from the latent distribution.\n",
    "- **Decoder**: Generates data from the sampled latent point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.2\"></a>\n",
    "### Mathematical Foundations\n",
    "\n",
    "VAEs aim to maximize the evidence lower bound (ELBO) on the data likelihood:\n",
    "\n",
    "$[\n",
    "\\mathcal{L}(\\theta, \\phi; \\mathbf{x}) = -\\text{KL}(q_{\\phi}(\\mathbf{z}|\\mathbf{x}) \\| p_{\\theta}(\\mathbf{z})) + \\mathbb{E}_{q_{\\phi}(\\mathbf{z}|\\mathbf{x})}[\\log p_{\\theta}(\\mathbf{x}|\\mathbf{z})]\n",
    "]$\n",
    "\n",
    "- **Encoder Distribution**: $( q_{\\phi}(\\mathbf{z}|\\mathbf{x}) )$\n",
    "- **Prior Distribution**: $( p_{\\theta}(\\mathbf{z}) )$\n",
    "- **Decoder Distribution**: $( p_{\\theta}(\\mathbf{x}|\\mathbf{z}) )$\n",
    "- **KL Divergence**: Measures the difference between the encoder's distribution and the prior.\n",
    "\n",
    "**Reparameterization Trick**: Allows backpropagation through stochastic nodes by expressing $( \\mathbf{z} )$ as:\n",
    "\n",
    "$[\n",
    "\\mathbf{z} = \\mu + \\sigma \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)\n",
    "]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4. Implementing a Variational Autoencoder\n",
    "\n",
    "We'll implement a VAE using the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.1\"></a>\n",
    "### Building the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE Parameters\n",
    "latent_dim = 2  # Dimensionality of the latent space\n",
    "\n",
    "# Encoder\n",
    "def build_vae_encoder():\n",
    "    encoder_inputs = layers.Input(shape=(28, 28, 1))\n",
    "    x = layers.Conv2D(32, 3, activation='relu', strides=2, padding='same')(encoder_inputs)\n",
    "    x = layers.Conv2D(64, 3, activation='relu', strides=2, padding='same')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(16, activation='relu')(x)\n",
    "    z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "    return models.Model(encoder_inputs, [z_mean, z_log_var], name='encoder')\n",
    "\n",
    "# Sampling Layer\n",
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# Decoder\n",
    "def build_vae_decoder():\n",
    "    latent_inputs = layers.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(7 * 7 * 64, activation='relu')(latent_inputs)\n",
    "    x = layers.Reshape((7, 7, 64))(x)\n",
    "    x = layers.Conv2DTranspose(64, 3, strides=2, padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2DTranspose(32, 3, strides=2, padding='same', activation='relu')(x)\n",
    "    decoder_outputs = layers.Conv2DTranspose(1, 3, padding='same', activation='sigmoid')(x)\n",
    "    return models.Model(latent_inputs, decoder_outputs, name='decoder')\n",
    "\n",
    "# Build the VAE\n",
    "encoder = build_vae_encoder()\n",
    "z_mean, z_log_var = encoder.output\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "decoder = build_vae_decoder()\n",
    "vae_outputs = decoder(z)\n",
    "vae = models.Model(encoder.input, vae_outputs, name='vae')\n",
    "\n",
    "# Define the VAE loss\n",
    "reconstruction_loss = tf.keras.losses.binary_crossentropy(encoder.input, vae_outputs)\n",
    "reconstruction_loss *= 28 * 28\n",
    "kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "kl_loss = -0.5 * tf.reduce_sum(kl_loss, axis=-1)\n",
    "vae_loss = tf.reduce_mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "\n",
    "# Compile the VAE\n",
    "vae.compile(optimizer='adam')\n",
    "\n",
    "# Model summary\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder maps the input to the parameters of a Gaussian distribution in the latent space. The decoder reconstructs the data from a sample drawn from this distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.2\"></a>\n",
    "### Training the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the VAE\n",
    "vae.fit(x_train, epochs=10, batch_size=128, validation_data=(x_test, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VAE is trained to minimize the reconstruction loss and the KL divergence between the learned latent distribution and the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.3\"></a>\n",
    "### Generating New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a 2D manifold of the digits\n",
    "import numpy as np\n",
    "\n",
    "n = 15  # Figure with 15x15 digits\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "\n",
    "# Linearly spaced coordinates corresponding to the 2D latent space\n",
    "grid_x = np.linspace(-4, 4, n)\n",
    "grid_y = np.linspace(-4, 4, n)[::-1]\n",
    "\n",
    "for i, yi in enumerate(grid_y):\n",
    "    for j, xi in enumerate(grid_x):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        x_decoded = decoder.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure, cmap='Greys_r')\n",
    "plt.axis('Off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting image shows a manifold of digits generated from the latent space, demonstrating the generative capabilities of the VAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## 5. Latest Developments\n",
    "\n",
    "Several extensions and variants of autoencoders and VAEs have been proposed to improve their performance and applicability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.1\"></a>\n",
    "### 5.1 Beta-VAE\n",
    "\n",
    "**Beta-VAE** [[2]](#ref2) introduces a weighting factor $( \\beta )$ to the KL divergence term in the VAE loss:\n",
    "\n",
    "$[\n",
    "\\mathcal{L}_{\\beta\\text{-VAE}} = \\mathbb{E}_{q_{\\phi}(\\mathbf{z}|\\mathbf{x})}[\\log p_{\\theta}(\\mathbf{x}|\\mathbf{z})] - \\beta \\text{KL}(q_{\\phi}(\\mathbf{z}|\\mathbf{x}) \\| p_{\\theta}(\\mathbf{z}))\n",
    "]$\n",
    "\n",
    "- Encourages disentangled latent representations.\n",
    "- Higher $( \\beta )$ values place more emphasis on the KL divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.2\"></a>\n",
    "### 5.2 Conditional VAEs\n",
    "\n",
    "**Conditional VAEs (CVAEs)** [[3]](#ref3) incorporate conditional information (e.g., class labels) into the VAE framework.\n",
    "\n",
    "- The encoder and decoder are conditioned on additional variables $( \\mathbf{y} )$.\n",
    "- Useful for tasks like controlled data generation and semi-supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.3\"></a>\n",
    "### 5.3 Vector Quantized VAE (VQ-VAE)\n",
    "\n",
    "**VQ-VAE** [[4]](#ref4) introduces discrete latent variables using vector quantization.\n",
    "\n",
    "- Combines VAEs with ideas from discrete representation learning.\n",
    "- Enables modeling of complex data distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "## 6. Conclusion\n",
    "\n",
    "Autoencoders and Variational Autoencoders are powerful tools for unsupervised learning, enabling feature extraction, dimensionality reduction, and generative modeling. By understanding their architectures and mathematical foundations, you can apply them to a wide range of tasks, from data compression to anomaly detection and synthetic data generation. Ongoing research continues to enhance their capabilities and extend their applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "## 7. References\n",
    "\n",
    "1. <a id=\"ref1\"></a>Kingma, D. P., & Welling, M. (2013). *Auto-Encoding Variational Bayes*. [arXiv:1312.6114](https://arxiv.org/abs/1312.6114)\n",
    "2. <a id=\"ref2\"></a>Higgins, I., et al. (2017). *beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework*. [ICLR 2017](https://openreview.net/forum?id=Sy2fzU9gl)\n",
    "3. <a id=\"ref3\"></a>Sohn, K., Lee, H., & Yan, X. (2015). *Learning Structured Output Representation using Deep Conditional Generative Models*. [NeurIPS 2015](https://papers.nips.cc/paper/5775-learning-structured-output-representation-using-deep-conditional-generative-models)\n",
    "4. <a id=\"ref4\"></a>van den Oord, A., Vinyals, O., & Kavukcuoglu, K. (2017). *Neural Discrete Representation Learning*. [NeurIPS 2017](https://arxiv.org/abs/1711.00937)\n",
    "\n",
    "---\n",
    "\n",
    "This notebook provides an in-depth exploration of autoencoders and variational autoencoders. You can run the code cells to see how they are implemented and experiment with different architectures and datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
